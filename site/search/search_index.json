{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RLM-Python","text":"<p> Secure LLM-driven code execution with Docker sandboxing </p> <p>RLM-Python (Recursive Language Model) is a Python library for safely executing LLM-generated code in isolated Docker containers. It provides enterprise-grade security through OS-level isolation, egress filtering, and memory-efficient context handling.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li>\ud83d\udc33 Docker Sandbox Execution - Execute untrusted code in isolated containers with gVisor support</li> <li>\ud83d\udd10 OS-Level Security - Network isolation, memory limits, process limits, privilege restrictions</li> <li>\ud83d\udee1\ufe0f Egress Filtering - Prevent data exfiltration via entropy detection and pattern matching</li> <li>\ud83d\udcda Memory-Efficient Context - Handle gigabyte-scale files with mmap-based <code>ContextHandle</code></li> <li>\ud83e\udd16 Multi-Provider LLM - Support for OpenAI, Anthropic, and Google Gemini</li> <li>\ud83d\udcb0 Budget Management - Track API costs and enforce spending limits</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code>pip install rlm-python\n</code></pre> <pre><code>from rlm import Orchestrator\n\norchestrator = Orchestrator()\nresult = orchestrator.run(\"Calculate the first 10 prime numbers\")\nprint(result.final_answer)\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>Configuration Reference</li> <li>API Documentation</li> </ul>"},{"location":"#security","title":"\ud83d\udd12 Security","text":"<p>RLM v2.0 implements defense in depth with multiple security layers:</p> Layer Protection Runtime gVisor (runsc) kernel isolation Network Complete network isolation Resources Memory, CPU, PID limits Egress Entropy and pattern-based filtering Privileges No privilege escalation <p>Learn more in the Security Architecture guide.</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for RLM-Python.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":"Class Description <code>Orchestrator</code> Main LLM agent loop <code>DockerSandbox</code> Secure code execution <code>ContextHandle</code> Memory-efficient file access <code>EgressFilter</code> Output sanitization"},{"location":"api/#llm-clients","title":"LLM Clients","text":"Class Description <code>BaseLLMClient</code> Abstract base class <code>OpenAIClient</code> OpenAI GPT models <code>AnthropicClient</code> Anthropic Claude <code>GoogleClient</code> Google Gemini"},{"location":"api/#exceptions","title":"Exceptions","text":"Exception Description <code>RLMError</code> Base exception <code>SecurityViolationError</code> Security boundary violation <code>DataLeakageError</code> Egress filter triggered <code>BudgetExceededError</code> Cost limit reached <code>SandboxError</code> Container execution failure"},{"location":"api/#utilities","title":"Utilities","text":"Class/Function Description <code>BudgetManager</code> Cost tracking <code>RLMSettings</code> Configuration"},{"location":"api/#quick-example","title":"Quick Example","text":"<pre><code>from rlm import Orchestrator, DockerSandbox, ContextHandle\n\n# Full orchestration\norchestrator = Orchestrator()\nresult = orchestrator.run(\"Calculate fibonacci(20)\")\nprint(result.final_answer)\n\n# Direct sandbox\nsandbox = DockerSandbox()\nexec_result = sandbox.execute(\"print(sum(range(100)))\")\nprint(exec_result.stdout)\n\n# Large file handling\nwith ContextHandle(\"/path/to/file.txt\") as ctx:\n    matches = ctx.search(r\"pattern\")\n</code></pre>"},{"location":"api/context-handle/","title":"ContextHandle API","text":""},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle","title":"<code>rlm.core.memory.handle.ContextHandle</code>","text":"<p>Memory-efficient handle for accessing large context files.</p> <p>This class is designed to be injected into the sandbox environment, allowing the LLM to search and read large files without loading everything into memory.</p> <p>The API is designed to encourage efficient patterns: - search() to find relevant sections - read_window() to read specific chunks - iterate_lines() for streaming access</p> Example <p>ctx = ContextHandle(\"/path/to/large_file.txt\") print(f\"File size: {ctx.size} bytes\") matches = ctx.search(r\"important.*pattern\") for offset, match in matches: ...     print(ctx.snippet(offset))</p> Source code in <code>src\\rlm\\core\\memory\\handle.py</code> <pre><code>class ContextHandle:\n    \"\"\"\n    Memory-efficient handle for accessing large context files.\n\n    This class is designed to be injected into the sandbox environment,\n    allowing the LLM to search and read large files without loading\n    everything into memory.\n\n    The API is designed to encourage efficient patterns:\n    - search() to find relevant sections\n    - read_window() to read specific chunks\n    - iterate_lines() for streaming access\n\n    Example:\n        &gt;&gt;&gt; ctx = ContextHandle(\"/path/to/large_file.txt\")\n        &gt;&gt;&gt; print(f\"File size: {ctx.size} bytes\")\n        &gt;&gt;&gt; matches = ctx.search(r\"important.*pattern\")\n        &gt;&gt;&gt; for offset, match in matches:\n        ...     print(ctx.snippet(offset))\n    \"\"\"\n\n    DEFAULT_WINDOW_SIZE = 500\n    MAX_SEARCH_RESULTS = 10\n\n    def __init__(self, path: str | Path) -&gt; None:\n        \"\"\"\n        Initialize the context handle.\n\n        Args:\n            path: Path to the context file\n\n        Raises:\n            ContextError: If the file doesn't exist or can't be accessed\n        \"\"\"\n        self.path = Path(path)\n\n        if not self.path.exists():\n            raise ContextError(\n                message=f\"Context file not found: {path}\",\n                path=str(path),\n            )\n\n        if not self.path.is_file():\n            raise ContextError(\n                message=f\"Context path is not a file: {path}\",\n                path=str(path),\n            )\n\n        try:\n            self._size = self.path.stat().st_size\n        except OSError as e:\n            raise ContextError(\n                message=f\"Cannot access context file: {e}\",\n                path=str(path),\n            ) from e\n\n        self._mmap: Optional[mmap.mmap] = None\n        self._file = None\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Return the total size of the context file in bytes.\"\"\"\n        return self._size\n\n    @property\n    def size_mb(self) -&gt; float:\n        \"\"\"Return the size in megabytes.\"\"\"\n        return self._size / (1024 * 1024)\n\n    def _get_mmap(self) -&gt; mmap.mmap:\n        \"\"\"Get or create the memory-mapped file.\"\"\"\n        if self._mmap is None:\n            try:\n                self._file = open(self.path, \"r+b\")\n                self._mmap = mmap.mmap(\n                    self._file.fileno(),\n                    0,\n                    access=mmap.ACCESS_READ,\n                )\n            except Exception as e:\n                if self._file:\n                    self._file.close()\n                raise ContextError(\n                    message=f\"Failed to memory-map context file: {e}\",\n                    path=str(self.path),\n                ) from e\n        return self._mmap\n\n    def read(self, start: int, length: int) -&gt; str:\n        \"\"\"\n        Read a specific chunk of the file.\n\n        Args:\n            start: Starting byte offset\n            length: Number of bytes to read\n\n        Returns:\n            The decoded text content\n        \"\"\"\n        if start &lt; 0:\n            start = 0\n        if start &gt;= self._size:\n            return \"\"\n\n        # Clamp length to file size\n        end = min(start + length, self._size)\n        actual_length = end - start\n\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            f.seek(start)\n            return f.read(actual_length)\n\n    def read_window(self, offset: int, radius: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n        \"\"\"\n        Read a window of text centered around an offset.\n\n        Args:\n            offset: Center point (byte offset)\n            radius: Number of bytes on each side of the offset\n\n        Returns:\n            The text content in the window\n        \"\"\"\n        start = max(0, offset - radius)\n        length = radius * 2\n        return self.read(start, length)\n\n    def snippet(self, offset: int, window: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n        \"\"\"\n        Get a snippet of text around an offset.\n\n        This is an alias for read_window with different semantics -\n        window is the total size, not the radius.\n\n        Args:\n            offset: Center point (byte offset)\n            window: Total window size in bytes\n\n        Returns:\n            The text snippet\n        \"\"\"\n        return self.read_window(offset, window // 2)\n\n    def search(\n        self,\n        pattern: str,\n        max_results: int = MAX_SEARCH_RESULTS,\n        ignore_case: bool = True,\n    ) -&gt; list[tuple[int, str]]:\n        \"\"\"\n        Search for a regex pattern in the file using memory mapping.\n\n        This is memory-efficient as it doesn't load the entire file.\n\n        Args:\n            pattern: Regular expression pattern\n            max_results: Maximum number of results to return\n            ignore_case: Whether to ignore case\n\n        Returns:\n            List of (byte_offset, matched_text) tuples\n        \"\"\"\n        matches: list[tuple[int, str]] = []\n\n        try:\n            mm = self._get_mmap()\n\n            # Compile pattern for bytes\n            flags = re.IGNORECASE if ignore_case else 0\n            try:\n                bytes_pattern = pattern.encode(\"utf-8\")\n                compiled = re.compile(bytes_pattern, flags)\n            except re.error as e:\n                raise ContextError(\n                    message=f\"Invalid regex pattern: {e}\",\n                    details={\"pattern\": pattern},\n                ) from e\n\n            for match in compiled.finditer(mm):\n                if len(matches) &gt;= max_results:\n                    break\n\n                try:\n                    match_text = match.group().decode(\"utf-8\", errors=\"replace\")\n                    matches.append((match.start(), match_text))\n                except UnicodeDecodeError:\n                    # Skip matches that can't be decoded\n                    continue\n\n        except Exception as e:\n            if not isinstance(e, ContextError):\n                raise ContextError(\n                    message=f\"Search failed: {e}\",\n                    path=str(self.path),\n                ) from e\n            raise\n\n        return matches\n\n    def search_lines(\n        self,\n        pattern: str,\n        max_results: int = MAX_SEARCH_RESULTS,\n        context_lines: int = 0,\n    ) -&gt; list[tuple[int, str, str]]:\n        \"\"\"\n        Search for a pattern and return matching lines.\n\n        Args:\n            pattern: Regex pattern to search for\n            max_results: Maximum number of results\n            context_lines: Number of lines before/after to include\n\n        Returns:\n            List of (line_number, matching_line, context) tuples\n        \"\"\"\n        matches: list[tuple[int, str, str]] = []\n        compiled = re.compile(pattern, re.IGNORECASE)\n\n        lines_buffer: list[str] = []\n\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            for line_no, line in enumerate(f, start=1):\n                lines_buffer.append(line)\n                if len(lines_buffer) &gt; context_lines * 2 + 1:\n                    lines_buffer.pop(0)\n\n                if compiled.search(line):\n                    context = \"\".join(lines_buffer)\n                    matches.append((line_no, line.strip(), context))\n\n                    if len(matches) &gt;= max_results:\n                        break\n\n        return matches\n\n    def iterate_lines(self, start_line: int = 1) -&gt; Iterator[tuple[int, str]]:\n        \"\"\"\n        Iterate over lines in the file.\n\n        This is memory-efficient as it reads line by line.\n\n        Args:\n            start_line: Line number to start from (1-indexed)\n\n        Yields:\n            Tuples of (line_number, line_content)\n        \"\"\"\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            for line_no, line in enumerate(f, start=1):\n                if line_no &gt;= start_line:\n                    yield line_no, line\n\n    def head(self, n_bytes: int = 1000) -&gt; str:\n        \"\"\"Read the first n bytes of the file.\"\"\"\n        return self.read(0, n_bytes)\n\n    def tail(self, n_bytes: int = 1000) -&gt; str:\n        \"\"\"Read the last n bytes of the file.\"\"\"\n        start = max(0, self._size - n_bytes)\n        return self.read(start, n_bytes)\n\n    def close(self) -&gt; None:\n        \"\"\"Close the memory-mapped file.\"\"\"\n        if self._mmap:\n            self._mmap.close()\n            self._mmap = None\n        if self._file:\n            self._file.close()\n            self._file = None\n\n    def __enter__(self) -&gt; \"ContextHandle\":\n        return self\n\n    def __exit__(self, *args) -&gt; None:\n        self.close()\n\n    def __repr__(self) -&gt; str:\n        return f\"ContextHandle(path='{self.path}', size={self.size_mb:.2f}MB)\"\n\n    def __del__(self) -&gt; None:\n        self.close()\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.size","title":"<code>size</code>  <code>property</code>","text":"<p>Return the total size of the context file in bytes.</p>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.size_mb","title":"<code>size_mb</code>  <code>property</code>","text":"<p>Return the size in megabytes.</p>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.read","title":"<code>read(start, length)</code>","text":"<p>Read a specific chunk of the file.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Starting byte offset</p> required <code>length</code> <code>int</code> <p>Number of bytes to read</p> required <p>Returns:</p> Type Description <code>str</code> <p>The decoded text content</p> Source code in <code>src\\rlm\\core\\memory\\handle.py</code> <pre><code>def read(self, start: int, length: int) -&gt; str:\n    \"\"\"\n    Read a specific chunk of the file.\n\n    Args:\n        start: Starting byte offset\n        length: Number of bytes to read\n\n    Returns:\n        The decoded text content\n    \"\"\"\n    if start &lt; 0:\n        start = 0\n    if start &gt;= self._size:\n        return \"\"\n\n    # Clamp length to file size\n    end = min(start + length, self._size)\n    actual_length = end - start\n\n    with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n        f.seek(start)\n        return f.read(actual_length)\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.read_window","title":"<code>read_window(offset, radius=DEFAULT_WINDOW_SIZE)</code>","text":"<p>Read a window of text centered around an offset.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Center point (byte offset)</p> required <code>radius</code> <code>int</code> <p>Number of bytes on each side of the offset</p> <code>DEFAULT_WINDOW_SIZE</code> <p>Returns:</p> Type Description <code>str</code> <p>The text content in the window</p> Source code in <code>src\\rlm\\core\\memory\\handle.py</code> <pre><code>def read_window(self, offset: int, radius: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n    \"\"\"\n    Read a window of text centered around an offset.\n\n    Args:\n        offset: Center point (byte offset)\n        radius: Number of bytes on each side of the offset\n\n    Returns:\n        The text content in the window\n    \"\"\"\n    start = max(0, offset - radius)\n    length = radius * 2\n    return self.read(start, length)\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.snippet","title":"<code>snippet(offset, window=DEFAULT_WINDOW_SIZE)</code>","text":"<p>Get a snippet of text around an offset.</p> <p>This is an alias for read_window with different semantics - window is the total size, not the radius.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Center point (byte offset)</p> required <code>window</code> <code>int</code> <p>Total window size in bytes</p> <code>DEFAULT_WINDOW_SIZE</code> <p>Returns:</p> Type Description <code>str</code> <p>The text snippet</p> Source code in <code>src\\rlm\\core\\memory\\handle.py</code> <pre><code>def snippet(self, offset: int, window: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n    \"\"\"\n    Get a snippet of text around an offset.\n\n    This is an alias for read_window with different semantics -\n    window is the total size, not the radius.\n\n    Args:\n        offset: Center point (byte offset)\n        window: Total window size in bytes\n\n    Returns:\n        The text snippet\n    \"\"\"\n    return self.read_window(offset, window // 2)\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.head","title":"<code>head(n_bytes=1000)</code>","text":"<p>Read the first n bytes of the file.</p> Source code in <code>src\\rlm\\core\\memory\\handle.py</code> <pre><code>def head(self, n_bytes: int = 1000) -&gt; str:\n    \"\"\"Read the first n bytes of the file.\"\"\"\n    return self.read(0, n_bytes)\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.tail","title":"<code>tail(n_bytes=1000)</code>","text":"<p>Read the last n bytes of the file.</p> Source code in <code>src\\rlm\\core\\memory\\handle.py</code> <pre><code>def tail(self, n_bytes: int = 1000) -&gt; str:\n    \"\"\"Read the last n bytes of the file.\"\"\"\n    start = max(0, self._size - n_bytes)\n    return self.read(start, n_bytes)\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.search","title":"<code>search(pattern, max_results=MAX_SEARCH_RESULTS, ignore_case=True)</code>","text":"<p>Search for a regex pattern in the file using memory mapping.</p> <p>This is memory-efficient as it doesn't load the entire file.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression pattern</p> required <code>max_results</code> <code>int</code> <p>Maximum number of results to return</p> <code>MAX_SEARCH_RESULTS</code> <code>ignore_case</code> <code>bool</code> <p>Whether to ignore case</p> <code>True</code> <p>Returns:</p> Type Description <code>list[tuple[int, str]]</code> <p>List of (byte_offset, matched_text) tuples</p> Source code in <code>src\\rlm\\core\\memory\\handle.py</code> <pre><code>def search(\n    self,\n    pattern: str,\n    max_results: int = MAX_SEARCH_RESULTS,\n    ignore_case: bool = True,\n) -&gt; list[tuple[int, str]]:\n    \"\"\"\n    Search for a regex pattern in the file using memory mapping.\n\n    This is memory-efficient as it doesn't load the entire file.\n\n    Args:\n        pattern: Regular expression pattern\n        max_results: Maximum number of results to return\n        ignore_case: Whether to ignore case\n\n    Returns:\n        List of (byte_offset, matched_text) tuples\n    \"\"\"\n    matches: list[tuple[int, str]] = []\n\n    try:\n        mm = self._get_mmap()\n\n        # Compile pattern for bytes\n        flags = re.IGNORECASE if ignore_case else 0\n        try:\n            bytes_pattern = pattern.encode(\"utf-8\")\n            compiled = re.compile(bytes_pattern, flags)\n        except re.error as e:\n            raise ContextError(\n                message=f\"Invalid regex pattern: {e}\",\n                details={\"pattern\": pattern},\n            ) from e\n\n        for match in compiled.finditer(mm):\n            if len(matches) &gt;= max_results:\n                break\n\n            try:\n                match_text = match.group().decode(\"utf-8\", errors=\"replace\")\n                matches.append((match.start(), match_text))\n            except UnicodeDecodeError:\n                # Skip matches that can't be decoded\n                continue\n\n    except Exception as e:\n        if not isinstance(e, ContextError):\n            raise ContextError(\n                message=f\"Search failed: {e}\",\n                path=str(self.path),\n            ) from e\n        raise\n\n    return matches\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.search_lines","title":"<code>search_lines(pattern, max_results=MAX_SEARCH_RESULTS, context_lines=0)</code>","text":"<p>Search for a pattern and return matching lines.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regex pattern to search for</p> required <code>max_results</code> <code>int</code> <p>Maximum number of results</p> <code>MAX_SEARCH_RESULTS</code> <code>context_lines</code> <code>int</code> <p>Number of lines before/after to include</p> <code>0</code> <p>Returns:</p> Type Description <code>list[tuple[int, str, str]]</code> <p>List of (line_number, matching_line, context) tuples</p> Source code in <code>src\\rlm\\core\\memory\\handle.py</code> <pre><code>def search_lines(\n    self,\n    pattern: str,\n    max_results: int = MAX_SEARCH_RESULTS,\n    context_lines: int = 0,\n) -&gt; list[tuple[int, str, str]]:\n    \"\"\"\n    Search for a pattern and return matching lines.\n\n    Args:\n        pattern: Regex pattern to search for\n        max_results: Maximum number of results\n        context_lines: Number of lines before/after to include\n\n    Returns:\n        List of (line_number, matching_line, context) tuples\n    \"\"\"\n    matches: list[tuple[int, str, str]] = []\n    compiled = re.compile(pattern, re.IGNORECASE)\n\n    lines_buffer: list[str] = []\n\n    with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n        for line_no, line in enumerate(f, start=1):\n            lines_buffer.append(line)\n            if len(lines_buffer) &gt; context_lines * 2 + 1:\n                lines_buffer.pop(0)\n\n            if compiled.search(line):\n                context = \"\".join(lines_buffer)\n                matches.append((line_no, line.strip(), context))\n\n                if len(matches) &gt;= max_results:\n                    break\n\n    return matches\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.iterate_lines","title":"<code>iterate_lines(start_line=1)</code>","text":"<p>Iterate over lines in the file.</p> <p>This is memory-efficient as it reads line by line.</p> <p>Parameters:</p> Name Type Description Default <code>start_line</code> <code>int</code> <p>Line number to start from (1-indexed)</p> <code>1</code> <p>Yields:</p> Type Description <code>tuple[int, str]</code> <p>Tuples of (line_number, line_content)</p> Source code in <code>src\\rlm\\core\\memory\\handle.py</code> <pre><code>def iterate_lines(self, start_line: int = 1) -&gt; Iterator[tuple[int, str]]:\n    \"\"\"\n    Iterate over lines in the file.\n\n    This is memory-efficient as it reads line by line.\n\n    Args:\n        start_line: Line number to start from (1-indexed)\n\n    Yields:\n        Tuples of (line_number, line_content)\n    \"\"\"\n    with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n        for line_no, line in enumerate(f, start=1):\n            if line_no &gt;= start_line:\n                yield line_no, line\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.close","title":"<code>close()</code>","text":"<p>Close the memory-mapped file.</p> Source code in <code>src\\rlm\\core\\memory\\handle.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the memory-mapped file.\"\"\"\n    if self._mmap:\n        self._mmap.close()\n        self._mmap = None\n    if self._file:\n        self._file.close()\n        self._file = None\n</code></pre>"},{"location":"api/egress/","title":"EgressFilter API","text":""},{"location":"api/egress/#rlm.security.egress.EgressFilter","title":"<code>rlm.security.egress.EgressFilter</code>  <code>dataclass</code>","text":"<p>Filter for sanitizing code execution output.</p> <p>Implements multiple layers of protection: 1. Entropy detection for secrets 2. Pattern matching for known secret formats 3. Context similarity detection 4. Output size limiting</p> Source code in <code>src\\rlm\\security\\egress.py</code> <pre><code>@dataclass\nclass EgressFilter:\n    \"\"\"\n    Filter for sanitizing code execution output.\n\n    Implements multiple layers of protection:\n    1. Entropy detection for secrets\n    2. Pattern matching for known secret formats\n    3. Context similarity detection\n    4. Output size limiting\n    \"\"\"\n\n    entropy_threshold: float = 4.5\n    min_entropy_length: int = 256\n    similarity_threshold: float = 0.8\n    max_output_bytes: int = 4000\n    context_fingerprint: Optional[str] = None\n    context_sample: Optional[str] = None\n\n    def __init__(\n        self,\n        context: Optional[str] = None,\n        entropy_threshold: Optional[float] = None,\n        similarity_threshold: Optional[float] = None,\n        max_output_bytes: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the egress filter.\n\n        Args:\n            context: The context/input data to protect from leakage\n            entropy_threshold: Override default entropy threshold\n            similarity_threshold: Override default similarity threshold\n            max_output_bytes: Override default output size limit\n        \"\"\"\n        self.entropy_threshold = entropy_threshold or settings.entropy_threshold\n        self.min_entropy_length = settings.min_entropy_length\n        self.similarity_threshold = similarity_threshold or settings.similarity_threshold\n        self.max_output_bytes = max_output_bytes or settings.max_stdout_bytes\n\n        if context:\n            self.context_fingerprint = hashlib.sha256(context.encode()).hexdigest()\n            # Store sample chunks for similarity comparison\n            self.context_sample = context[:2000] if len(context) &gt; 2000 else context\n\n    def check_entropy(self, text: str) -&gt; tuple[bool, float]:\n        \"\"\"\n        Check if text has suspiciously high entropy.\n\n        Args:\n            text: Text to check\n\n        Returns:\n            Tuple of (is_suspicious, entropy_value)\n        \"\"\"\n        if len(text) &lt; self.min_entropy_length:\n            return False, 0.0\n\n        entropy = calculate_shannon_entropy(text)\n        is_high = entropy &gt; self.entropy_threshold\n\n        if is_high:\n            logger.warning(f\"High entropy detected: {entropy:.2f} (threshold: {self.entropy_threshold})\")\n\n        return is_high, entropy\n\n    def check_context_echo(self, output: str) -&gt; tuple[bool, float]:\n        \"\"\"\n        Check if output is echoing the context (data exfiltration).\n\n        Args:\n            output: Output to check\n\n        Returns:\n            Tuple of (is_echo, similarity_score)\n        \"\"\"\n        if not self.context_sample:\n            return False, 0.0\n\n        similarity = calculate_similarity(output, self.context_sample)\n        is_echo = similarity &gt; self.similarity_threshold\n\n        if is_echo:\n            logger.warning(\n                f\"Context echo detected: similarity {similarity:.2f} \"\n                f\"(threshold: {self.similarity_threshold})\"\n            )\n\n        return is_echo, similarity\n\n    def check_secrets(self, text: str) -&gt; list[tuple[str, str]]:\n        \"\"\"\n        Check for known secret patterns.\n\n        Args:\n            text: Text to check\n\n        Returns:\n            List of detected secrets (pattern_name, redacted_match)\n        \"\"\"\n        return detect_secrets(text)\n\n    def truncate_output(self, text: str) -&gt; str:\n        \"\"\"\n        Truncate output while preserving head and tail (most useful parts).\n\n        Args:\n            text: Text to truncate\n\n        Returns:\n            Truncated text with indicator\n        \"\"\"\n        if len(text) &lt;= self.max_output_bytes:\n            return text\n\n        # Preserve first 1KB and last 3KB (errors usually at the end)\n        head_size = 1000\n        tail_size = 3000\n        truncated = len(text) - head_size - tail_size\n\n        head = text[:head_size]\n        tail = text[-tail_size:]\n\n        return f\"{head}\\n\\n... [TRUNCATED {truncated} bytes] ...\\n\\n{tail}\"\n\n    def filter(\n        self,\n        output: str,\n        raise_on_leak: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Apply all egress filters to output.\n\n        Args:\n            output: Raw output to filter\n            raise_on_leak: Whether to raise DataLeakageError on detection\n\n        Returns:\n            Sanitized output\n\n        Raises:\n            DataLeakageError: If raise_on_leak is True and leakage is detected\n        \"\"\"\n        if not output:\n            return output\n\n        redactions = []\n\n        # Check for secrets\n        secrets = self.check_secrets(output)\n        if secrets:\n            if raise_on_leak:\n                raise DataLeakageError(\n                    message=\"Secrets detected in output\",\n                    leak_type=\"secret_pattern\",\n                    details={\"patterns\": [s[0] for s in secrets]},\n                )\n            for pattern_name, matched in secrets:\n                redactions.append(f\"[REDACTED: {pattern_name}]\")\n                # Remove the matched content\n                output = re.sub(re.escape(matched), f\"[REDACTED: {pattern_name}]\", output)\n\n        # Check for high entropy\n        is_high_entropy, entropy = self.check_entropy(output)\n        if is_high_entropy:\n            if raise_on_leak:\n                raise DataLeakageError(\n                    message=\"High entropy data detected - potential secret leak\",\n                    leak_type=\"high_entropy\",\n                    details={\"entropy\": entropy},\n                )\n            output = f\"[SECURITY REDACTION: High Entropy Data Detected - Potential Secret Leak]\\n(Entropy: {entropy:.2f})\"\n\n        # Check for context echo\n        is_echo, similarity = self.check_context_echo(output)\n        if is_echo:\n            if raise_on_leak:\n                raise DataLeakageError(\n                    message=\"Do not print raw context. Summarize it.\",\n                    leak_type=\"context_echo\",\n                    details={\"similarity\": similarity},\n                )\n            output = f\"[SECURITY REDACTION: Context Echo Detected]\\nDo not print raw context data. Use ctx.search() and ctx.snippet() to extract specific information.\"\n\n        # Truncate if needed\n        output = self.truncate_output(output)\n\n        if redactions:\n            logger.info(f\"Egress filter applied {len(redactions)} redactions\")\n\n        return output\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.EgressFilter.filter","title":"<code>filter(output, raise_on_leak=False)</code>","text":"<p>Apply all egress filters to output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>Raw output to filter</p> required <code>raise_on_leak</code> <code>bool</code> <p>Whether to raise DataLeakageError on detection</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Sanitized output</p> <p>Raises:</p> Type Description <code>DataLeakageError</code> <p>If raise_on_leak is True and leakage is detected</p> Source code in <code>src\\rlm\\security\\egress.py</code> <pre><code>def filter(\n    self,\n    output: str,\n    raise_on_leak: bool = False,\n) -&gt; str:\n    \"\"\"\n    Apply all egress filters to output.\n\n    Args:\n        output: Raw output to filter\n        raise_on_leak: Whether to raise DataLeakageError on detection\n\n    Returns:\n        Sanitized output\n\n    Raises:\n        DataLeakageError: If raise_on_leak is True and leakage is detected\n    \"\"\"\n    if not output:\n        return output\n\n    redactions = []\n\n    # Check for secrets\n    secrets = self.check_secrets(output)\n    if secrets:\n        if raise_on_leak:\n            raise DataLeakageError(\n                message=\"Secrets detected in output\",\n                leak_type=\"secret_pattern\",\n                details={\"patterns\": [s[0] for s in secrets]},\n            )\n        for pattern_name, matched in secrets:\n            redactions.append(f\"[REDACTED: {pattern_name}]\")\n            # Remove the matched content\n            output = re.sub(re.escape(matched), f\"[REDACTED: {pattern_name}]\", output)\n\n    # Check for high entropy\n    is_high_entropy, entropy = self.check_entropy(output)\n    if is_high_entropy:\n        if raise_on_leak:\n            raise DataLeakageError(\n                message=\"High entropy data detected - potential secret leak\",\n                leak_type=\"high_entropy\",\n                details={\"entropy\": entropy},\n            )\n        output = f\"[SECURITY REDACTION: High Entropy Data Detected - Potential Secret Leak]\\n(Entropy: {entropy:.2f})\"\n\n    # Check for context echo\n    is_echo, similarity = self.check_context_echo(output)\n    if is_echo:\n        if raise_on_leak:\n            raise DataLeakageError(\n                message=\"Do not print raw context. Summarize it.\",\n                leak_type=\"context_echo\",\n                details={\"similarity\": similarity},\n            )\n        output = f\"[SECURITY REDACTION: Context Echo Detected]\\nDo not print raw context data. Use ctx.search() and ctx.snippet() to extract specific information.\"\n\n    # Truncate if needed\n    output = self.truncate_output(output)\n\n    if redactions:\n        logger.info(f\"Egress filter applied {len(redactions)} redactions\")\n\n    return output\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.EgressFilter.check_entropy","title":"<code>check_entropy(text)</code>","text":"<p>Check if text has suspiciously high entropy.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to check</p> required <p>Returns:</p> Type Description <code>tuple[bool, float]</code> <p>Tuple of (is_suspicious, entropy_value)</p> Source code in <code>src\\rlm\\security\\egress.py</code> <pre><code>def check_entropy(self, text: str) -&gt; tuple[bool, float]:\n    \"\"\"\n    Check if text has suspiciously high entropy.\n\n    Args:\n        text: Text to check\n\n    Returns:\n        Tuple of (is_suspicious, entropy_value)\n    \"\"\"\n    if len(text) &lt; self.min_entropy_length:\n        return False, 0.0\n\n    entropy = calculate_shannon_entropy(text)\n    is_high = entropy &gt; self.entropy_threshold\n\n    if is_high:\n        logger.warning(f\"High entropy detected: {entropy:.2f} (threshold: {self.entropy_threshold})\")\n\n    return is_high, entropy\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.EgressFilter.check_context_echo","title":"<code>check_context_echo(output)</code>","text":"<p>Check if output is echoing the context (data exfiltration).</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>Output to check</p> required <p>Returns:</p> Type Description <code>tuple[bool, float]</code> <p>Tuple of (is_echo, similarity_score)</p> Source code in <code>src\\rlm\\security\\egress.py</code> <pre><code>def check_context_echo(self, output: str) -&gt; tuple[bool, float]:\n    \"\"\"\n    Check if output is echoing the context (data exfiltration).\n\n    Args:\n        output: Output to check\n\n    Returns:\n        Tuple of (is_echo, similarity_score)\n    \"\"\"\n    if not self.context_sample:\n        return False, 0.0\n\n    similarity = calculate_similarity(output, self.context_sample)\n    is_echo = similarity &gt; self.similarity_threshold\n\n    if is_echo:\n        logger.warning(\n            f\"Context echo detected: similarity {similarity:.2f} \"\n            f\"(threshold: {self.similarity_threshold})\"\n        )\n\n    return is_echo, similarity\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.EgressFilter.check_secrets","title":"<code>check_secrets(text)</code>","text":"<p>Check for known secret patterns.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to check</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>List of detected secrets (pattern_name, redacted_match)</p> Source code in <code>src\\rlm\\security\\egress.py</code> <pre><code>def check_secrets(self, text: str) -&gt; list[tuple[str, str]]:\n    \"\"\"\n    Check for known secret patterns.\n\n    Args:\n        text: Text to check\n\n    Returns:\n        List of detected secrets (pattern_name, redacted_match)\n    \"\"\"\n    return detect_secrets(text)\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.EgressFilter.truncate_output","title":"<code>truncate_output(text)</code>","text":"<p>Truncate output while preserving head and tail (most useful parts).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to truncate</p> required <p>Returns:</p> Type Description <code>str</code> <p>Truncated text with indicator</p> Source code in <code>src\\rlm\\security\\egress.py</code> <pre><code>def truncate_output(self, text: str) -&gt; str:\n    \"\"\"\n    Truncate output while preserving head and tail (most useful parts).\n\n    Args:\n        text: Text to truncate\n\n    Returns:\n        Truncated text with indicator\n    \"\"\"\n    if len(text) &lt;= self.max_output_bytes:\n        return text\n\n    # Preserve first 1KB and last 3KB (errors usually at the end)\n    head_size = 1000\n    tail_size = 3000\n    truncated = len(text) - head_size - tail_size\n\n    head = text[:head_size]\n    tail = text[-tail_size:]\n\n    return f\"{head}\\n\\n... [TRUNCATED {truncated} bytes] ...\\n\\n{tail}\"\n</code></pre>"},{"location":"api/egress/#utility-functions","title":"Utility Functions","text":""},{"location":"api/egress/#rlm.security.egress.calculate_shannon_entropy","title":"<code>rlm.security.egress.calculate_shannon_entropy(data)</code>","text":"<p>Calculate Shannon entropy of a string.</p> <p>Higher entropy indicates more randomness - potential secrets or encoded binary data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>String to analyze</p> required <p>Returns:</p> Type Description <code>float</code> <p>Shannon entropy value (typically 0-8 for ASCII text)</p> <code>float</code> <ul> <li>0-2: Very low entropy (repetitive)</li> </ul> <code>float</code> <ul> <li>2-4: Low entropy (natural language)</li> </ul> <code>float</code> <ul> <li>4-5: Medium entropy (code, mixed content)</li> </ul> <code>float</code> <ul> <li>5-8: High entropy (secrets, binary, compressed)</li> </ul> Source code in <code>src\\rlm\\security\\egress.py</code> <pre><code>def calculate_shannon_entropy(data: str) -&gt; float:\n    \"\"\"\n    Calculate Shannon entropy of a string.\n\n    Higher entropy indicates more randomness - potential secrets or\n    encoded binary data.\n\n    Args:\n        data: String to analyze\n\n    Returns:\n        Shannon entropy value (typically 0-8 for ASCII text)\n        - 0-2: Very low entropy (repetitive)\n        - 2-4: Low entropy (natural language)\n        - 4-5: Medium entropy (code, mixed content)\n        - 5-8: High entropy (secrets, binary, compressed)\n    \"\"\"\n    if not data:\n        return 0.0\n\n    entropy = 0.0\n    counter = Counter(data)\n    length = len(data)\n\n    for count in counter.values():\n        if count &gt; 0:\n            probability = count / length\n            entropy -= probability * math.log2(probability)\n\n    return entropy\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.calculate_similarity","title":"<code>rlm.security.egress.calculate_similarity(s1, s2, ngram_size=3)</code>","text":"<p>Calculate Jaccard similarity between two strings using n-grams.</p> <p>This is used to detect \"echo attacks\" where the LLM prints the raw context back.</p> <p>Parameters:</p> Name Type Description Default <code>s1</code> <code>str</code> <p>First string</p> required <code>s2</code> <code>str</code> <p>Second string</p> required <code>ngram_size</code> <code>int</code> <p>Size of n-grams to use</p> <code>3</code> <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> Source code in <code>src\\rlm\\security\\egress.py</code> <pre><code>def calculate_similarity(s1: str, s2: str, ngram_size: int = 3) -&gt; float:\n    \"\"\"\n    Calculate Jaccard similarity between two strings using n-grams.\n\n    This is used to detect \"echo attacks\" where the LLM prints\n    the raw context back.\n\n    Args:\n        s1: First string\n        s2: Second string\n        ngram_size: Size of n-grams to use\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n    \"\"\"\n    if not s1 or not s2:\n        return 0.0\n\n    # Generate n-grams\n    def get_ngrams(s: str) -&gt; set:\n        s = s.lower()\n        return {s[i : i + ngram_size] for i in range(len(s) - ngram_size + 1)}\n\n    ngrams1 = get_ngrams(s1)\n    ngrams2 = get_ngrams(s2)\n\n    if not ngrams1 or not ngrams2:\n        return 0.0\n\n    intersection = len(ngrams1 &amp; ngrams2)\n    union = len(ngrams1 | ngrams2)\n\n    return intersection / union if union &gt; 0 else 0.0\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.detect_secrets","title":"<code>rlm.security.egress.detect_secrets(text)</code>","text":"<p>Detect potential secrets in text using pattern matching.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to scan</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>List of (pattern_name, matched_text) tuples</p> Source code in <code>src\\rlm\\security\\egress.py</code> <pre><code>def detect_secrets(text: str) -&gt; list[tuple[str, str]]:\n    \"\"\"\n    Detect potential secrets in text using pattern matching.\n\n    Args:\n        text: Text to scan\n\n    Returns:\n        List of (pattern_name, matched_text) tuples\n    \"\"\"\n    matches = []\n    pattern_names = [\n        \"api_key\",\n        \"aws_access_key\",\n        \"aws_secret\",\n        \"private_key\",\n        \"jwt\",\n        \"generic_secret\",\n        \"bearer_token\",\n    ]\n\n    for pattern, name in zip(SECRET_PATTERNS, pattern_names):\n        for match in re.finditer(pattern, text):\n            # Truncate match for logging\n            matched = match.group()[:50] + \"...\" if len(match.group()) &gt; 50 else match.group()\n            matches.append((name, matched))\n\n    return matches\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.sanitize_output","title":"<code>rlm.security.egress.sanitize_output(output, context=None, raise_on_leak=False)</code>","text":"<p>Convenience function to sanitize code execution output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>Raw output from code execution</p> required <code>context</code> <code>Optional[str]</code> <p>Optional context data to protect from leakage</p> <code>None</code> <code>raise_on_leak</code> <code>bool</code> <p>Whether to raise exception on detected leakage</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Sanitized output</p> Source code in <code>src\\rlm\\security\\egress.py</code> <pre><code>def sanitize_output(\n    output: str,\n    context: Optional[str] = None,\n    raise_on_leak: bool = False,\n) -&gt; str:\n    \"\"\"\n    Convenience function to sanitize code execution output.\n\n    Args:\n        output: Raw output from code execution\n        context: Optional context data to protect from leakage\n        raise_on_leak: Whether to raise exception on detected leakage\n\n    Returns:\n        Sanitized output\n    \"\"\"\n    filter_instance = EgressFilter(context=context)\n    return filter_instance.filter(output, raise_on_leak=raise_on_leak)\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions API","text":"<p>All RLM exceptions inherit from <code>RLMError</code>.</p>"},{"location":"api/exceptions/#base-exception","title":"Base Exception","text":""},{"location":"api/exceptions/#rlm.core.exceptions.RLMError","title":"<code>rlm.core.exceptions.RLMError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all RLM errors.</p> Source code in <code>src\\rlm\\core\\exceptions.py</code> <pre><code>class RLMError(Exception):\n    \"\"\"Base exception for all RLM errors.\"\"\"\n\n    def __init__(self, message: str, details: Optional[dict] = None) -&gt; None:\n        super().__init__(message)\n        self.message = message\n        self.details = details or {}\n\n    def __str__(self) -&gt; str:\n        if self.details:\n            return f\"{self.message} | Details: {self.details}\"\n        return self.message\n</code></pre>"},{"location":"api/exceptions/#security-exceptions","title":"Security Exceptions","text":""},{"location":"api/exceptions/#rlm.core.exceptions.SecurityViolationError","title":"<code>rlm.core.exceptions.SecurityViolationError</code>","text":"<p>               Bases: <code>RLMError</code></p> <p>Raised when a security boundary is violated.</p> <p>This includes attempts to escape the sandbox, access restricted resources, or perform unauthorized operations.</p> Source code in <code>src\\rlm\\core\\exceptions.py</code> <pre><code>class SecurityViolationError(RLMError):\n    \"\"\"\n    Raised when a security boundary is violated.\n\n    This includes attempts to escape the sandbox, access restricted\n    resources, or perform unauthorized operations.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Security violation detected\",\n        violation_type: Optional[str] = None,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        super().__init__(message, details)\n        self.violation_type = violation_type\n</code></pre>"},{"location":"api/exceptions/#rlm.core.exceptions.DataLeakageError","title":"<code>rlm.core.exceptions.DataLeakageError</code>","text":"<p>               Bases: <code>SecurityViolationError</code></p> <p>Raised when potential data leakage is detected.</p> <p>This is triggered by the egress filter when: - High entropy data (potential secrets) is detected in output - Output closely matches the input context (echo attack)</p> Source code in <code>src\\rlm\\core\\exceptions.py</code> <pre><code>class DataLeakageError(SecurityViolationError):\n    \"\"\"\n    Raised when potential data leakage is detected.\n\n    This is triggered by the egress filter when:\n    - High entropy data (potential secrets) is detected in output\n    - Output closely matches the input context (echo attack)\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Potential data leakage detected\",\n        leak_type: Optional[str] = None,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        super().__init__(message, violation_type=\"data_leakage\", details=details)\n        self.leak_type = leak_type\n</code></pre>"},{"location":"api/exceptions/#execution-exceptions","title":"Execution Exceptions","text":""},{"location":"api/exceptions/#rlm.core.exceptions.SandboxError","title":"<code>rlm.core.exceptions.SandboxError</code>","text":"<p>               Bases: <code>RLMError</code></p> <p>Raised when sandbox execution fails.</p> <p>This includes container startup failures, timeout issues, and resource limit violations (OOM, CPU quota, etc.).</p> Source code in <code>src\\rlm\\core\\exceptions.py</code> <pre><code>class SandboxError(RLMError):\n    \"\"\"\n    Raised when sandbox execution fails.\n\n    This includes container startup failures, timeout issues,\n    and resource limit violations (OOM, CPU quota, etc.).\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Sandbox execution failed\",\n        exit_code: Optional[int] = None,\n        stderr: Optional[str] = None,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        details = details or {}\n        if exit_code is not None:\n            details[\"exit_code\"] = exit_code\n        if stderr:\n            details[\"stderr\"] = stderr[:500]  # Truncate for safety\n        super().__init__(message, details)\n        self.exit_code = exit_code\n        self.stderr = stderr\n\n    @property\n    def is_oom_killed(self) -&gt; bool:\n        \"\"\"Check if the container was killed due to OOM.\"\"\"\n        return self.exit_code == 137\n\n    @property\n    def is_timeout(self) -&gt; bool:\n        \"\"\"Check if the execution timed out.\"\"\"\n        return self.exit_code == 124\n</code></pre>"},{"location":"api/exceptions/#rlm.core.exceptions.SandboxError.is_oom_killed","title":"<code>is_oom_killed</code>  <code>property</code>","text":"<p>Check if the container was killed due to OOM.</p>"},{"location":"api/exceptions/#rlm.core.exceptions.SandboxError.is_timeout","title":"<code>is_timeout</code>  <code>property</code>","text":"<p>Check if the execution timed out.</p>"},{"location":"api/exceptions/#rlm.core.exceptions.BudgetExceededError","title":"<code>rlm.core.exceptions.BudgetExceededError</code>","text":"<p>               Bases: <code>RLMError</code></p> <p>Raised when the cost limit is exceeded.</p> <p>This prevents runaway API costs by stopping execution when the configured spending limit is reached.</p> Source code in <code>src\\rlm\\core\\exceptions.py</code> <pre><code>class BudgetExceededError(RLMError):\n    \"\"\"\n    Raised when the cost limit is exceeded.\n\n    This prevents runaway API costs by stopping execution when\n    the configured spending limit is reached.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Budget limit exceeded\",\n        spent: float = 0.0,\n        limit: float = 0.0,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        details = details or {}\n        details.update({\"spent_usd\": spent, \"limit_usd\": limit})\n        super().__init__(message, details)\n        self.spent = spent\n        self.limit = limit\n</code></pre>"},{"location":"api/exceptions/#configuration-exceptions","title":"Configuration Exceptions","text":""},{"location":"api/exceptions/#rlm.core.exceptions.ConfigurationError","title":"<code>rlm.core.exceptions.ConfigurationError</code>","text":"<p>               Bases: <code>RLMError</code></p> <p>Raised when there's a configuration issue.</p> <p>This includes missing API keys, invalid settings, or unavailable Docker runtime.</p> Source code in <code>src\\rlm\\core\\exceptions.py</code> <pre><code>class ConfigurationError(RLMError):\n    \"\"\"\n    Raised when there's a configuration issue.\n\n    This includes missing API keys, invalid settings, or\n    unavailable Docker runtime.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Configuration error\",\n        setting_name: Optional[str] = None,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        details = details or {}\n        if setting_name:\n            details[\"setting\"] = setting_name\n        super().__init__(message, details)\n        self.setting_name = setting_name\n</code></pre>"},{"location":"api/exceptions/#rlm.core.exceptions.ContextError","title":"<code>rlm.core.exceptions.ContextError</code>","text":"<p>               Bases: <code>RLMError</code></p> <p>Raised when there's an issue with context handling.</p> <p>This includes file not found, encoding issues, or memory mapping failures.</p> Source code in <code>src\\rlm\\core\\exceptions.py</code> <pre><code>class ContextError(RLMError):\n    \"\"\"\n    Raised when there's an issue with context handling.\n\n    This includes file not found, encoding issues, or memory\n    mapping failures.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Context handling error\",\n        path: Optional[str] = None,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        details = details or {}\n        if path:\n            details[\"path\"] = path\n        super().__init__(message, details)\n        self.path = path\n</code></pre>"},{"location":"api/exceptions/#llm-exceptions","title":"LLM Exceptions","text":""},{"location":"api/exceptions/#rlm.core.exceptions.LLMError","title":"<code>rlm.core.exceptions.LLMError</code>","text":"<p>               Bases: <code>RLMError</code></p> <p>Raised when there's an error communicating with the LLM.</p> <p>This includes API errors, rate limiting, and model-specific issues.</p> Source code in <code>src\\rlm\\core\\exceptions.py</code> <pre><code>class LLMError(RLMError):\n    \"\"\"\n    Raised when there's an error communicating with the LLM.\n\n    This includes API errors, rate limiting, and model-specific issues.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"LLM error\",\n        provider: Optional[str] = None,\n        status_code: Optional[int] = None,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        details = details or {}\n        if provider:\n            details[\"provider\"] = provider\n        if status_code:\n            details[\"status_code\"] = status_code\n        super().__init__(message, details)\n        self.provider = provider\n        self.status_code = status_code\n</code></pre>"},{"location":"api/llm/","title":"LLM Clients API","text":""},{"location":"api/llm/#base-client","title":"Base Client","text":""},{"location":"api/llm/#rlm.llm.base.BaseLLMClient","title":"<code>rlm.llm.base.BaseLLMClient</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for LLM provider clients.</p> <p>All provider implementations (OpenAI, Anthropic, Google) must inherit from this class and implement the required methods.</p> Source code in <code>src\\rlm\\llm\\base.py</code> <pre><code>class BaseLLMClient(ABC):\n    \"\"\"\n    Abstract base class for LLM provider clients.\n\n    All provider implementations (OpenAI, Anthropic, Google) must\n    inherit from this class and implement the required methods.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str,\n        temperature: float = 0.0,\n        max_tokens: int = 4096,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the LLM client.\n\n        Args:\n            api_key: API key for the provider\n            model: Model name to use\n            temperature: Sampling temperature (0.0-2.0)\n            max_tokens: Maximum tokens in response\n        \"\"\"\n        self.api_key = api_key\n        self.model = model\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n\n    @property\n    @abstractmethod\n    def provider_name(self) -&gt; str:\n        \"\"\"Return the provider name (e.g., 'openai', 'anthropic').\"\"\"\n        pass\n\n    @abstractmethod\n    def complete(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; LLMResponse:\n        \"\"\"\n        Generate a completion from the model.\n\n        Args:\n            messages: List of conversation messages\n            system_prompt: Optional system prompt\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            LLMResponse containing the generated content\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def stream(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"\n        Stream a completion from the model.\n\n        Args:\n            messages: List of conversation messages\n            system_prompt: Optional system prompt\n            **kwargs: Additional provider-specific arguments\n\n        Yields:\n            Chunks of the generated content\n        \"\"\"\n        pass\n\n    def validate_api_key(self) -&gt; bool:\n        \"\"\"\n        Validate that the API key is working.\n\n        Returns:\n            True if the API key is valid\n        \"\"\"\n        try:\n            # Try a minimal request\n            self.complete([Message(role=\"user\", content=\"test\")])\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.BaseLLMClient.provider_name","title":"<code>provider_name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the provider name (e.g., 'openai', 'anthropic').</p>"},{"location":"api/llm/#rlm.llm.base.BaseLLMClient.complete","title":"<code>complete(messages, system_prompt=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a completion from the model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse containing the generated content</p> Source code in <code>src\\rlm\\llm\\base.py</code> <pre><code>@abstractmethod\ndef complete(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; LLMResponse:\n    \"\"\"\n    Generate a completion from the model.\n\n    Args:\n        messages: List of conversation messages\n        system_prompt: Optional system prompt\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        LLMResponse containing the generated content\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.BaseLLMClient.stream","title":"<code>stream(messages, system_prompt=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Stream a completion from the model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of the generated content</p> Source code in <code>src\\rlm\\llm\\base.py</code> <pre><code>@abstractmethod\ndef stream(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Stream a completion from the model.\n\n    Args:\n        messages: List of conversation messages\n        system_prompt: Optional system prompt\n        **kwargs: Additional provider-specific arguments\n\n    Yields:\n        Chunks of the generated content\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.BaseLLMClient.validate_api_key","title":"<code>validate_api_key()</code>","text":"<p>Validate that the API key is working.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the API key is valid</p> Source code in <code>src\\rlm\\llm\\base.py</code> <pre><code>def validate_api_key(self) -&gt; bool:\n    \"\"\"\n    Validate that the API key is working.\n\n    Returns:\n        True if the API key is valid\n    \"\"\"\n    try:\n        # Try a minimal request\n        self.complete([Message(role=\"user\", content=\"test\")])\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.Message","title":"<code>rlm.llm.base.Message</code>  <code>dataclass</code>","text":"<p>A message in the conversation history.</p> Source code in <code>src\\rlm\\llm\\base.py</code> <pre><code>@dataclass\nclass Message:\n    \"\"\"A message in the conversation history.\"\"\"\n\n    role: Role | Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n    name: Optional[str] = None\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert to dictionary format for API calls.\"\"\"\n        d = {\"role\": str(self.role.value if isinstance(self.role, Role) else self.role), \"content\": self.content}\n        if self.name:\n            d[\"name\"] = self.name\n        return d\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.Message.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary format for API calls.</p> Source code in <code>src\\rlm\\llm\\base.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary format for API calls.\"\"\"\n    d = {\"role\": str(self.role.value if isinstance(self.role, Role) else self.role), \"content\": self.content}\n    if self.name:\n        d[\"name\"] = self.name\n    return d\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.LLMResponse","title":"<code>rlm.llm.base.LLMResponse</code>  <code>dataclass</code>","text":"<p>Response from an LLM API call.</p> Source code in <code>src\\rlm\\llm\\base.py</code> <pre><code>@dataclass\nclass LLMResponse:\n    \"\"\"Response from an LLM API call.\"\"\"\n\n    content: str\n    model: str\n    usage: TokenUsage = field(default_factory=TokenUsage)\n    finish_reason: Optional[str] = None\n    raw_response: Optional[dict] = None\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.TokenUsage","title":"<code>rlm.llm.base.TokenUsage</code>  <code>dataclass</code>","text":"<p>Token usage information from an API response.</p> Source code in <code>src\\rlm\\llm\\base.py</code> <pre><code>@dataclass\nclass TokenUsage:\n    \"\"\"Token usage information from an API response.\"\"\"\n\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n    total_tokens: int = 0\n</code></pre>"},{"location":"api/llm/#openai-client","title":"OpenAI Client","text":""},{"location":"api/llm/#rlm.llm.openai_client.OpenAIClient","title":"<code>rlm.llm.openai_client.OpenAIClient</code>","text":"<p>               Bases: <code>BaseLLMClient</code></p> <p>OpenAI API client for GPT models.</p> <p>Supports both regular and streaming completions.</p> Example <p>client = OpenAIClient(api_key=\"sk-...\", model=\"gpt-4o\") response = client.complete([Message(role=\"user\", content=\"Hello!\")]) print(response.content)</p> Source code in <code>src\\rlm\\llm\\openai_client.py</code> <pre><code>class OpenAIClient(BaseLLMClient):\n    \"\"\"\n    OpenAI API client for GPT models.\n\n    Supports both regular and streaming completions.\n\n    Example:\n        &gt;&gt;&gt; client = OpenAIClient(api_key=\"sk-...\", model=\"gpt-4o\")\n        &gt;&gt;&gt; response = client.complete([Message(role=\"user\", content=\"Hello!\")])\n        &gt;&gt;&gt; print(response.content)\n    \"\"\"\n\n    DEFAULT_MODEL = \"gpt-4o\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str = DEFAULT_MODEL,\n        temperature: float = 0.0,\n        max_tokens: int = 4096,\n        base_url: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the OpenAI client.\n\n        Args:\n            api_key: OpenAI API key\n            model: Model name (default: gpt-4o)\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens in response\n            base_url: Optional custom base URL (for Azure or proxies)\n        \"\"\"\n        super().__init__(api_key, model, temperature, max_tokens)\n        self._client = OpenAI(api_key=api_key, base_url=base_url)\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"openai\"\n\n    def complete(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; LLMResponse:\n        \"\"\"\n        Generate a completion using OpenAI's API.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments (passed to API)\n\n        Returns:\n            LLMResponse with the generated content\n        \"\"\"\n        all_messages = []\n\n        if system_prompt:\n            all_messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n        all_messages.extend([m.to_dict() for m in messages])\n\n        try:\n            response = self._client.chat.completions.create(\n                model=self.model,\n                messages=all_messages,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                **kwargs,\n            )\n\n            choice = response.choices[0]\n            usage = response.usage\n\n            return LLMResponse(\n                content=choice.message.content or \"\",\n                model=response.model,\n                usage=TokenUsage(\n                    prompt_tokens=usage.prompt_tokens if usage else 0,\n                    completion_tokens=usage.completion_tokens if usage else 0,\n                    total_tokens=usage.total_tokens if usage else 0,\n                ),\n                finish_reason=choice.finish_reason,\n                raw_response=response.model_dump() if hasattr(response, \"model_dump\") else None,\n            )\n\n        except Exception as e:\n            logger.error(f\"OpenAI API error: {e}\")\n            raise LLMError(\n                message=f\"OpenAI API request failed: {e}\",\n                provider=\"openai\",\n            ) from e\n\n    def stream(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"\n        Stream a completion from OpenAI.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments\n\n        Yields:\n            Chunks of the generated content\n        \"\"\"\n        all_messages = []\n\n        if system_prompt:\n            all_messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n        all_messages.extend([m.to_dict() for m in messages])\n\n        try:\n            stream = self._client.chat.completions.create(\n                model=self.model,\n                messages=all_messages,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                stream=True,\n                **kwargs,\n            )\n\n            for chunk in stream:\n                if chunk.choices and chunk.choices[0].delta.content:\n                    yield chunk.choices[0].delta.content\n\n        except Exception as e:\n            logger.error(f\"OpenAI streaming error: {e}\")\n            raise LLMError(\n                message=f\"OpenAI streaming failed: {e}\",\n                provider=\"openai\",\n            ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.openai_client.OpenAIClient.__init__","title":"<code>__init__(api_key, model=DEFAULT_MODEL, temperature=0.0, max_tokens=4096, base_url=None)</code>","text":"<p>Initialize the OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key</p> required <code>model</code> <code>str</code> <p>Model name (default: gpt-4o)</p> <code>DEFAULT_MODEL</code> <code>temperature</code> <code>float</code> <p>Sampling temperature</p> <code>0.0</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens in response</p> <code>4096</code> <code>base_url</code> <code>Optional[str]</code> <p>Optional custom base URL (for Azure or proxies)</p> <code>None</code> Source code in <code>src\\rlm\\llm\\openai_client.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.0,\n    max_tokens: int = 4096,\n    base_url: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the OpenAI client.\n\n    Args:\n        api_key: OpenAI API key\n        model: Model name (default: gpt-4o)\n        temperature: Sampling temperature\n        max_tokens: Maximum tokens in response\n        base_url: Optional custom base URL (for Azure or proxies)\n    \"\"\"\n    super().__init__(api_key, model, temperature, max_tokens)\n    self._client = OpenAI(api_key=api_key, base_url=base_url)\n</code></pre>"},{"location":"api/llm/#rlm.llm.openai_client.OpenAIClient.complete","title":"<code>complete(messages, system_prompt=None, **kwargs)</code>","text":"<p>Generate a completion using OpenAI's API.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments (passed to API)</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with the generated content</p> Source code in <code>src\\rlm\\llm\\openai_client.py</code> <pre><code>def complete(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; LLMResponse:\n    \"\"\"\n    Generate a completion using OpenAI's API.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments (passed to API)\n\n    Returns:\n        LLMResponse with the generated content\n    \"\"\"\n    all_messages = []\n\n    if system_prompt:\n        all_messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n    all_messages.extend([m.to_dict() for m in messages])\n\n    try:\n        response = self._client.chat.completions.create(\n            model=self.model,\n            messages=all_messages,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            **kwargs,\n        )\n\n        choice = response.choices[0]\n        usage = response.usage\n\n        return LLMResponse(\n            content=choice.message.content or \"\",\n            model=response.model,\n            usage=TokenUsage(\n                prompt_tokens=usage.prompt_tokens if usage else 0,\n                completion_tokens=usage.completion_tokens if usage else 0,\n                total_tokens=usage.total_tokens if usage else 0,\n            ),\n            finish_reason=choice.finish_reason,\n            raw_response=response.model_dump() if hasattr(response, \"model_dump\") else None,\n        )\n\n    except Exception as e:\n        logger.error(f\"OpenAI API error: {e}\")\n        raise LLMError(\n            message=f\"OpenAI API request failed: {e}\",\n            provider=\"openai\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.openai_client.OpenAIClient.stream","title":"<code>stream(messages, system_prompt=None, **kwargs)</code>","text":"<p>Stream a completion from OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of the generated content</p> Source code in <code>src\\rlm\\llm\\openai_client.py</code> <pre><code>def stream(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Stream a completion from OpenAI.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments\n\n    Yields:\n        Chunks of the generated content\n    \"\"\"\n    all_messages = []\n\n    if system_prompt:\n        all_messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n    all_messages.extend([m.to_dict() for m in messages])\n\n    try:\n        stream = self._client.chat.completions.create(\n            model=self.model,\n            messages=all_messages,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            stream=True,\n            **kwargs,\n        )\n\n        for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content:\n                yield chunk.choices[0].delta.content\n\n    except Exception as e:\n        logger.error(f\"OpenAI streaming error: {e}\")\n        raise LLMError(\n            message=f\"OpenAI streaming failed: {e}\",\n            provider=\"openai\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#anthropic-client","title":"Anthropic Client","text":""},{"location":"api/llm/#rlm.llm.anthropic_client.AnthropicClient","title":"<code>rlm.llm.anthropic_client.AnthropicClient</code>","text":"<p>               Bases: <code>BaseLLMClient</code></p> <p>Anthropic API client for Claude models.</p> <p>Supports both regular and streaming completions.</p> Example <p>client = AnthropicClient(api_key=\"sk-...\", model=\"claude-3-sonnet-20240229\") response = client.complete([Message(role=\"user\", content=\"Hello!\")]) print(response.content)</p> Source code in <code>src\\rlm\\llm\\anthropic_client.py</code> <pre><code>class AnthropicClient(BaseLLMClient):\n    \"\"\"\n    Anthropic API client for Claude models.\n\n    Supports both regular and streaming completions.\n\n    Example:\n        &gt;&gt;&gt; client = AnthropicClient(api_key=\"sk-...\", model=\"claude-3-sonnet-20240229\")\n        &gt;&gt;&gt; response = client.complete([Message(role=\"user\", content=\"Hello!\")])\n        &gt;&gt;&gt; print(response.content)\n    \"\"\"\n\n    DEFAULT_MODEL = \"claude-3-sonnet-20240229\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str = DEFAULT_MODEL,\n        temperature: float = 0.0,\n        max_tokens: int = 4096,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Anthropic client.\n\n        Args:\n            api_key: Anthropic API key\n            model: Model name (default: claude-3-sonnet)\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens in response\n        \"\"\"\n        super().__init__(api_key, model, temperature, max_tokens)\n        self._client = anthropic.Anthropic(api_key=api_key)\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"anthropic\"\n\n    def _convert_messages(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n    ) -&gt; tuple[Optional[str], list[dict]]:\n        \"\"\"\n        Convert messages to Anthropic's format.\n\n        Anthropic uses a separate system parameter instead of a message.\n\n        Returns:\n            Tuple of (system_prompt, messages)\n        \"\"\"\n        system = system_prompt\n        converted = []\n\n        for msg in messages:\n            role = msg.role.value if hasattr(msg.role, \"value\") else msg.role\n\n            # Extract system messages\n            if role == \"system\":\n                system = msg.content\n                continue\n\n            converted.append({\n                \"role\": role,\n                \"content\": msg.content,\n            })\n\n        return system, converted\n\n    def complete(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; LLMResponse:\n        \"\"\"\n        Generate a completion using Anthropic's API.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments\n\n        Returns:\n            LLMResponse with the generated content\n        \"\"\"\n        system, converted_messages = self._convert_messages(messages, system_prompt)\n\n        try:\n            create_kwargs = {\n                \"model\": self.model,\n                \"messages\": converted_messages,\n                \"temperature\": self.temperature,\n                \"max_tokens\": self.max_tokens,\n                **kwargs,\n            }\n\n            if system:\n                create_kwargs[\"system\"] = system\n\n            response = self._client.messages.create(**create_kwargs)\n\n            # Extract content from response\n            content = \"\"\n            if response.content:\n                content = response.content[0].text\n\n            return LLMResponse(\n                content=content,\n                model=response.model,\n                usage=TokenUsage(\n                    prompt_tokens=response.usage.input_tokens,\n                    completion_tokens=response.usage.output_tokens,\n                    total_tokens=response.usage.input_tokens + response.usage.output_tokens,\n                ),\n                finish_reason=response.stop_reason,\n                raw_response=response.model_dump() if hasattr(response, \"model_dump\") else None,\n            )\n\n        except Exception as e:\n            logger.error(f\"Anthropic API error: {e}\")\n            raise LLMError(\n                message=f\"Anthropic API request failed: {e}\",\n                provider=\"anthropic\",\n            ) from e\n\n    def stream(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"\n        Stream a completion from Anthropic.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments\n\n        Yields:\n            Chunks of the generated content\n        \"\"\"\n        system, converted_messages = self._convert_messages(messages, system_prompt)\n\n        try:\n            create_kwargs = {\n                \"model\": self.model,\n                \"messages\": converted_messages,\n                \"temperature\": self.temperature,\n                \"max_tokens\": self.max_tokens,\n                **kwargs,\n            }\n\n            if system:\n                create_kwargs[\"system\"] = system\n\n            with self._client.messages.stream(**create_kwargs) as stream:\n                for text in stream.text_stream:\n                    yield text\n\n        except Exception as e:\n            logger.error(f\"Anthropic streaming error: {e}\")\n            raise LLMError(\n                message=f\"Anthropic streaming failed: {e}\",\n                provider=\"anthropic\",\n            ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.anthropic_client.AnthropicClient.__init__","title":"<code>__init__(api_key, model=DEFAULT_MODEL, temperature=0.0, max_tokens=4096)</code>","text":"<p>Initialize the Anthropic client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Anthropic API key</p> required <code>model</code> <code>str</code> <p>Model name (default: claude-3-sonnet)</p> <code>DEFAULT_MODEL</code> <code>temperature</code> <code>float</code> <p>Sampling temperature</p> <code>0.0</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens in response</p> <code>4096</code> Source code in <code>src\\rlm\\llm\\anthropic_client.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.0,\n    max_tokens: int = 4096,\n) -&gt; None:\n    \"\"\"\n    Initialize the Anthropic client.\n\n    Args:\n        api_key: Anthropic API key\n        model: Model name (default: claude-3-sonnet)\n        temperature: Sampling temperature\n        max_tokens: Maximum tokens in response\n    \"\"\"\n    super().__init__(api_key, model, temperature, max_tokens)\n    self._client = anthropic.Anthropic(api_key=api_key)\n</code></pre>"},{"location":"api/llm/#rlm.llm.anthropic_client.AnthropicClient.complete","title":"<code>complete(messages, system_prompt=None, **kwargs)</code>","text":"<p>Generate a completion using Anthropic's API.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with the generated content</p> Source code in <code>src\\rlm\\llm\\anthropic_client.py</code> <pre><code>def complete(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; LLMResponse:\n    \"\"\"\n    Generate a completion using Anthropic's API.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments\n\n    Returns:\n        LLMResponse with the generated content\n    \"\"\"\n    system, converted_messages = self._convert_messages(messages, system_prompt)\n\n    try:\n        create_kwargs = {\n            \"model\": self.model,\n            \"messages\": converted_messages,\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            **kwargs,\n        }\n\n        if system:\n            create_kwargs[\"system\"] = system\n\n        response = self._client.messages.create(**create_kwargs)\n\n        # Extract content from response\n        content = \"\"\n        if response.content:\n            content = response.content[0].text\n\n        return LLMResponse(\n            content=content,\n            model=response.model,\n            usage=TokenUsage(\n                prompt_tokens=response.usage.input_tokens,\n                completion_tokens=response.usage.output_tokens,\n                total_tokens=response.usage.input_tokens + response.usage.output_tokens,\n            ),\n            finish_reason=response.stop_reason,\n            raw_response=response.model_dump() if hasattr(response, \"model_dump\") else None,\n        )\n\n    except Exception as e:\n        logger.error(f\"Anthropic API error: {e}\")\n        raise LLMError(\n            message=f\"Anthropic API request failed: {e}\",\n            provider=\"anthropic\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.anthropic_client.AnthropicClient.stream","title":"<code>stream(messages, system_prompt=None, **kwargs)</code>","text":"<p>Stream a completion from Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of the generated content</p> Source code in <code>src\\rlm\\llm\\anthropic_client.py</code> <pre><code>def stream(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Stream a completion from Anthropic.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments\n\n    Yields:\n        Chunks of the generated content\n    \"\"\"\n    system, converted_messages = self._convert_messages(messages, system_prompt)\n\n    try:\n        create_kwargs = {\n            \"model\": self.model,\n            \"messages\": converted_messages,\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            **kwargs,\n        }\n\n        if system:\n            create_kwargs[\"system\"] = system\n\n        with self._client.messages.stream(**create_kwargs) as stream:\n            for text in stream.text_stream:\n                yield text\n\n    except Exception as e:\n        logger.error(f\"Anthropic streaming error: {e}\")\n        raise LLMError(\n            message=f\"Anthropic streaming failed: {e}\",\n            provider=\"anthropic\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#google-client","title":"Google Client","text":""},{"location":"api/llm/#rlm.llm.google_client.GoogleClient","title":"<code>rlm.llm.google_client.GoogleClient</code>","text":"<p>               Bases: <code>BaseLLMClient</code></p> <p>Google Generative AI client for Gemini models.</p> <p>Supports both regular and streaming completions.</p> Example <p>client = GoogleClient(api_key=\"...\", model=\"gemini-1.5-pro\") response = client.complete([Message(role=\"user\", content=\"Hello!\")]) print(response.content)</p> Source code in <code>src\\rlm\\llm\\google_client.py</code> <pre><code>class GoogleClient(BaseLLMClient):\n    \"\"\"\n    Google Generative AI client for Gemini models.\n\n    Supports both regular and streaming completions.\n\n    Example:\n        &gt;&gt;&gt; client = GoogleClient(api_key=\"...\", model=\"gemini-1.5-pro\")\n        &gt;&gt;&gt; response = client.complete([Message(role=\"user\", content=\"Hello!\")])\n        &gt;&gt;&gt; print(response.content)\n    \"\"\"\n\n    DEFAULT_MODEL = \"gemini-1.5-pro\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str = DEFAULT_MODEL,\n        temperature: float = 0.0,\n        max_tokens: int = 4096,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Google client.\n\n        Args:\n            api_key: Google API key\n            model: Model name (default: gemini-1.5-pro)\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens in response\n        \"\"\"\n        super().__init__(api_key, model, temperature, max_tokens)\n\n        genai.configure(api_key=api_key)\n        self._model = genai.GenerativeModel(model)\n        self._generation_config = GenerationConfig(\n            temperature=temperature,\n            max_output_tokens=max_tokens,\n        )\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"google\"\n\n    def _convert_messages(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n    ) -&gt; tuple[Optional[str], list[dict]]:\n        \"\"\"\n        Convert messages to Google's format.\n\n        Google uses 'user' and 'model' roles.\n        \"\"\"\n        system = system_prompt\n        converted = []\n\n        for msg in messages:\n            role = msg.role.value if hasattr(msg.role, \"value\") else msg.role\n\n            if role == \"system\":\n                system = msg.content\n                continue\n\n            # Map assistant to model\n            google_role = \"model\" if role == \"assistant\" else \"user\"\n\n            converted.append({\n                \"role\": google_role,\n                \"parts\": [msg.content],\n            })\n\n        return system, converted\n\n    def complete(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; LLMResponse:\n        \"\"\"\n        Generate a completion using Google's API.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments\n\n        Returns:\n            LLMResponse with the generated content\n        \"\"\"\n        system, converted_messages = self._convert_messages(messages, system_prompt)\n\n        try:\n            # Rebuild model with system instruction if provided\n            if system:\n                model = genai.GenerativeModel(\n                    self.model,\n                    system_instruction=system,\n                )\n            else:\n                model = self._model\n\n            response = model.generate_content(\n                converted_messages,\n                generation_config=self._generation_config,\n                **kwargs,\n            )\n\n            # Extract usage information\n            usage = TokenUsage()\n            if hasattr(response, \"usage_metadata\") and response.usage_metadata:\n                usage = TokenUsage(\n                    prompt_tokens=getattr(response.usage_metadata, \"prompt_token_count\", 0),\n                    completion_tokens=getattr(response.usage_metadata, \"candidates_token_count\", 0),\n                    total_tokens=getattr(response.usage_metadata, \"total_token_count\", 0),\n                )\n\n            return LLMResponse(\n                content=response.text if response.text else \"\",\n                model=self.model,\n                usage=usage,\n                finish_reason=str(response.candidates[0].finish_reason) if response.candidates else None,\n                raw_response=None,  # Google's response doesn't have a simple dict export\n            )\n\n        except Exception as e:\n            logger.error(f\"Google API error: {e}\")\n            raise LLMError(\n                message=f\"Google API request failed: {e}\",\n                provider=\"google\",\n            ) from e\n\n    def stream(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"\n        Stream a completion from Google.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments\n\n        Yields:\n            Chunks of the generated content\n        \"\"\"\n        system, converted_messages = self._convert_messages(messages, system_prompt)\n\n        try:\n            if system:\n                model = genai.GenerativeModel(\n                    self.model,\n                    system_instruction=system,\n                )\n            else:\n                model = self._model\n\n            response = model.generate_content(\n                converted_messages,\n                generation_config=self._generation_config,\n                stream=True,\n                **kwargs,\n            )\n\n            for chunk in response:\n                if chunk.text:\n                    yield chunk.text\n\n        except Exception as e:\n            logger.error(f\"Google streaming error: {e}\")\n            raise LLMError(\n                message=f\"Google streaming failed: {e}\",\n                provider=\"google\",\n            ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.google_client.GoogleClient.__init__","title":"<code>__init__(api_key, model=DEFAULT_MODEL, temperature=0.0, max_tokens=4096)</code>","text":"<p>Initialize the Google client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Google API key</p> required <code>model</code> <code>str</code> <p>Model name (default: gemini-1.5-pro)</p> <code>DEFAULT_MODEL</code> <code>temperature</code> <code>float</code> <p>Sampling temperature</p> <code>0.0</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens in response</p> <code>4096</code> Source code in <code>src\\rlm\\llm\\google_client.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.0,\n    max_tokens: int = 4096,\n) -&gt; None:\n    \"\"\"\n    Initialize the Google client.\n\n    Args:\n        api_key: Google API key\n        model: Model name (default: gemini-1.5-pro)\n        temperature: Sampling temperature\n        max_tokens: Maximum tokens in response\n    \"\"\"\n    super().__init__(api_key, model, temperature, max_tokens)\n\n    genai.configure(api_key=api_key)\n    self._model = genai.GenerativeModel(model)\n    self._generation_config = GenerationConfig(\n        temperature=temperature,\n        max_output_tokens=max_tokens,\n    )\n</code></pre>"},{"location":"api/llm/#rlm.llm.google_client.GoogleClient.complete","title":"<code>complete(messages, system_prompt=None, **kwargs)</code>","text":"<p>Generate a completion using Google's API.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with the generated content</p> Source code in <code>src\\rlm\\llm\\google_client.py</code> <pre><code>def complete(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; LLMResponse:\n    \"\"\"\n    Generate a completion using Google's API.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments\n\n    Returns:\n        LLMResponse with the generated content\n    \"\"\"\n    system, converted_messages = self._convert_messages(messages, system_prompt)\n\n    try:\n        # Rebuild model with system instruction if provided\n        if system:\n            model = genai.GenerativeModel(\n                self.model,\n                system_instruction=system,\n            )\n        else:\n            model = self._model\n\n        response = model.generate_content(\n            converted_messages,\n            generation_config=self._generation_config,\n            **kwargs,\n        )\n\n        # Extract usage information\n        usage = TokenUsage()\n        if hasattr(response, \"usage_metadata\") and response.usage_metadata:\n            usage = TokenUsage(\n                prompt_tokens=getattr(response.usage_metadata, \"prompt_token_count\", 0),\n                completion_tokens=getattr(response.usage_metadata, \"candidates_token_count\", 0),\n                total_tokens=getattr(response.usage_metadata, \"total_token_count\", 0),\n            )\n\n        return LLMResponse(\n            content=response.text if response.text else \"\",\n            model=self.model,\n            usage=usage,\n            finish_reason=str(response.candidates[0].finish_reason) if response.candidates else None,\n            raw_response=None,  # Google's response doesn't have a simple dict export\n        )\n\n    except Exception as e:\n        logger.error(f\"Google API error: {e}\")\n        raise LLMError(\n            message=f\"Google API request failed: {e}\",\n            provider=\"google\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.google_client.GoogleClient.stream","title":"<code>stream(messages, system_prompt=None, **kwargs)</code>","text":"<p>Stream a completion from Google.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of the generated content</p> Source code in <code>src\\rlm\\llm\\google_client.py</code> <pre><code>def stream(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Stream a completion from Google.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments\n\n    Yields:\n        Chunks of the generated content\n    \"\"\"\n    system, converted_messages = self._convert_messages(messages, system_prompt)\n\n    try:\n        if system:\n            model = genai.GenerativeModel(\n                self.model,\n                system_instruction=system,\n            )\n        else:\n            model = self._model\n\n        response = model.generate_content(\n            converted_messages,\n            generation_config=self._generation_config,\n            stream=True,\n            **kwargs,\n        )\n\n        for chunk in response:\n            if chunk.text:\n                yield chunk.text\n\n    except Exception as e:\n        logger.error(f\"Google streaming error: {e}\")\n        raise LLMError(\n            message=f\"Google streaming failed: {e}\",\n            provider=\"google\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#factory-function","title":"Factory Function","text":""},{"location":"api/llm/#rlm.llm.factory.create_llm_client","title":"<code>rlm.llm.factory.create_llm_client(provider=None, api_key=None, model=None, **kwargs)</code>","text":"<p>Create an LLM client based on provider.</p> <p>Uses settings as defaults, but allows overrides.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[Literal['openai', 'anthropic', 'google']]</code> <p>LLM provider (default: from settings)</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key (default: from settings)</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model name (default: from settings)</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the client</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseLLMClient</code> <p>Configured LLM client</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If provider is unknown or API key is missing</p> Source code in <code>src\\rlm\\llm\\factory.py</code> <pre><code>def create_llm_client(\n    provider: Optional[Literal[\"openai\", \"anthropic\", \"google\"]] = None,\n    api_key: Optional[str] = None,\n    model: Optional[str] = None,\n    **kwargs,\n) -&gt; BaseLLMClient:\n    \"\"\"\n    Create an LLM client based on provider.\n\n    Uses settings as defaults, but allows overrides.\n\n    Args:\n        provider: LLM provider (default: from settings)\n        api_key: API key (default: from settings)\n        model: Model name (default: from settings)\n        **kwargs: Additional arguments passed to the client\n\n    Returns:\n        Configured LLM client\n\n    Raises:\n        ConfigurationError: If provider is unknown or API key is missing\n    \"\"\"\n    provider = provider or settings.api_provider\n    api_key = api_key or settings.api_key.get_secret_value()\n    model = model or settings.model_name\n\n    if not api_key:\n        raise ConfigurationError(\n            message=f\"API key not configured for provider: {provider}\",\n            setting_name=\"api_key\",\n        )\n\n    if provider == \"openai\":\n        from rlm.llm.openai_client import OpenAIClient\n        return OpenAIClient(api_key=api_key, model=model, **kwargs)\n\n    elif provider == \"anthropic\":\n        from rlm.llm.anthropic_client import AnthropicClient\n        return AnthropicClient(api_key=api_key, model=model, **kwargs)\n\n    elif provider == \"google\":\n        from rlm.llm.google_client import GoogleClient\n        return GoogleClient(api_key=api_key, model=model, **kwargs)\n\n    else:\n        raise ConfigurationError(\n            message=f\"Unknown LLM provider: {provider}\",\n            setting_name=\"api_provider\",\n            details={\"valid_providers\": [\"openai\", \"anthropic\", \"google\"]},\n        )\n</code></pre>"},{"location":"api/orchestrator/","title":"Orchestrator API","text":""},{"location":"api/orchestrator/#rlm.core.orchestrator.Orchestrator","title":"<code>rlm.core.orchestrator.Orchestrator</code>","text":"<p>Main orchestrator for RLM code execution.</p> <p>Coordinates the agent loop: 1. Send user query to LLM with system prompt 2. Parse LLM response for code blocks 3. Execute code in sandbox 4. Filter output and send back to LLM 5. Repeat until FINAL() is emitted or max iterations reached</p> Example <p>orchestrator = Orchestrator() result = orchestrator.run(\"What is 2+2?\") print(result.final_answer) 4</p> Source code in <code>src\\rlm\\core\\orchestrator.py</code> <pre><code>class Orchestrator:\n    \"\"\"\n    Main orchestrator for RLM code execution.\n\n    Coordinates the agent loop:\n    1. Send user query to LLM with system prompt\n    2. Parse LLM response for code blocks\n    3. Execute code in sandbox\n    4. Filter output and send back to LLM\n    5. Repeat until FINAL() is emitted or max iterations reached\n\n    Example:\n        &gt;&gt;&gt; orchestrator = Orchestrator()\n        &gt;&gt;&gt; result = orchestrator.run(\"What is 2+2?\")\n        &gt;&gt;&gt; print(result.final_answer)\n        4\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_client: Optional[BaseLLMClient] = None,\n        sandbox: Optional[DockerSandbox] = None,\n        config: Optional[OrchestratorConfig] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the orchestrator.\n\n        Args:\n            llm_client: LLM client (created from settings if not provided)\n            sandbox: Docker sandbox (created if not provided)\n            config: Orchestrator configuration\n        \"\"\"\n        self.config = config or OrchestratorConfig()\n        self._llm_client = llm_client\n        self._sandbox = sandbox\n        self.budget = BudgetManager()\n        self.egress_filter: Optional[EgressFilter] = None\n        self.history: list[Message] = []\n        self.steps: list[ExecutionStep] = []\n\n    @property\n    def llm(self) -&gt; BaseLLMClient:\n        \"\"\"Lazy-load LLM client.\"\"\"\n        if self._llm_client is None:\n            self._llm_client = create_llm_client()\n        return self._llm_client\n\n    @property\n    def sandbox(self) -&gt; DockerSandbox:\n        \"\"\"Lazy-load Docker sandbox.\"\"\"\n        if self._sandbox is None:\n            self._sandbox = DockerSandbox()\n        return self._sandbox\n\n    def _get_system_prompt(self) -&gt; str:\n        \"\"\"Build the system prompt.\"\"\"\n        context_available = self.config.context_path is not None\n        return get_system_prompt(\n            mode=self.config.system_prompt_mode,\n            context_available=context_available,\n            custom_instructions=self.config.custom_instructions,\n        )\n\n    def _extract_code_blocks(self, text: str) -&gt; list[str]:\n        \"\"\"Extract Python code blocks from LLM response.\"\"\"\n        blocks = CODE_BLOCK_PATTERN.findall(text)\n        return [block.strip() for block in blocks if block.strip()]\n\n    def _extract_final_answer(self, text: str) -&gt; Optional[str]:\n        \"\"\"Extract final answer from LLM response or code output.\"\"\"\n        match = FINAL_ANSWER_PATTERN.search(text)\n        if match:\n            return match.group(1).strip()\n        return None\n\n    def _call_llm(self, iteration: int) -&gt; LLMResponse:\n        \"\"\"Call the LLM and track the cost.\"\"\"\n        system_prompt = self._get_system_prompt()\n\n        response = self.llm.complete(\n            messages=self.history,\n            system_prompt=system_prompt,\n        )\n\n        # Track cost\n        self.budget.record_usage(\n            model=response.model,\n            input_tokens=response.usage.prompt_tokens,\n            output_tokens=response.usage.completion_tokens,\n        )\n\n        self.steps.append(ExecutionStep(\n            iteration=iteration,\n            action=\"llm_call\",\n            input_data=self.history[-1].content if self.history else \"\",\n            output_data=response.content,\n            success=True,\n        ))\n\n        return response\n\n    def _execute_code(self, code: str, iteration: int) -&gt; ExecutionResult:\n        \"\"\"Execute code in the sandbox.\"\"\"\n        context_mount = str(self.config.context_path) if self.config.context_path else None\n\n        try:\n            result = self.sandbox.execute(code, context_mount=context_mount)\n\n            # Apply egress filter\n            if self.egress_filter:\n                result.stdout = self.egress_filter.filter(\n                    result.stdout,\n                    raise_on_leak=self.config.raise_on_leak,\n                )\n\n            self.steps.append(ExecutionStep(\n                iteration=iteration,\n                action=\"code_execution\",\n                input_data=code,\n                output_data=result.stdout,\n                success=result.success,\n                error=result.stderr if not result.success else None,\n            ))\n\n            return result\n\n        except SandboxError as e:\n            self.steps.append(ExecutionStep(\n                iteration=iteration,\n                action=\"code_execution\",\n                input_data=code,\n                output_data=\"\",\n                success=False,\n                error=str(e),\n            ))\n            raise\n\n    def run(\n        self,\n        query: str,\n        context_path: Optional[str | Path] = None,\n    ) -&gt; OrchestratorResult:\n        \"\"\"\n        Run the orchestration loop for a user query.\n\n        Args:\n            query: User's question or task\n            context_path: Optional path to context file\n\n        Returns:\n            OrchestratorResult with the final answer and execution details\n        \"\"\"\n        # Reset state\n        self.history = []\n        self.steps = []\n\n        # Setup context if provided\n        if context_path:\n            self.config.context_path = Path(context_path)\n            # Create egress filter with context\n            context_sample = Path(context_path).read_text(encoding=\"utf-8\", errors=\"replace\")[:5000]\n            self.egress_filter = EgressFilter(context=context_sample)\n        else:\n            self.egress_filter = EgressFilter()\n\n        # Add initial user message\n        self.history.append(Message(role=\"user\", content=query))\n\n        try:\n            for iteration in range(self.config.max_iterations):\n                logger.info(f\"Iteration {iteration + 1}/{self.config.max_iterations}\")\n\n                # Call LLM\n                response = self._call_llm(iteration)\n                assistant_message = response.content\n\n                # Check for final answer in LLM response\n                final_answer = self._extract_final_answer(assistant_message)\n                if final_answer:\n                    self.steps.append(ExecutionStep(\n                        iteration=iteration,\n                        action=\"final_answer\",\n                        input_data=assistant_message,\n                        output_data=final_answer,\n                        success=True,\n                    ))\n                    return OrchestratorResult(\n                        final_answer=final_answer,\n                        success=True,\n                        iterations=iteration + 1,\n                        steps=self.steps,\n                        budget_summary=self.budget.summary(),\n                    )\n\n                # Add assistant response to history\n                self.history.append(Message(role=\"assistant\", content=assistant_message))\n\n                # Extract and execute code blocks\n                code_blocks = self._extract_code_blocks(assistant_message)\n\n                if not code_blocks:\n                    # No code to execute - LLM might be answering directly\n                    # Check if this looks like a final answer\n                    if iteration &gt; 0:  # After first iteration, treat as final\n                        return OrchestratorResult(\n                            final_answer=assistant_message,\n                            success=True,\n                            iterations=iteration + 1,\n                            steps=self.steps,\n                            budget_summary=self.budget.summary(),\n                        )\n                    continue\n\n                # Execute each code block\n                combined_output = []\n                for code in code_blocks:\n                    result = self._execute_code(code, iteration)\n\n                    if result.oom_killed:\n                        combined_output.append(\"Error: Memory Limit Exceeded (OOMKilled)\")\n                    elif result.timed_out:\n                        combined_output.append(\"Error: Execution Timeout\")\n                    elif not result.success:\n                        combined_output.append(f\"Error (exit {result.exit_code}):\\n{result.stderr}\")\n                    else:\n                        combined_output.append(result.stdout)\n\n                    # Check for final answer in output\n                    final_answer = self._extract_final_answer(result.stdout)\n                    if final_answer:\n                        self.steps.append(ExecutionStep(\n                            iteration=iteration,\n                            action=\"final_answer\",\n                            input_data=result.stdout,\n                            output_data=final_answer,\n                            success=True,\n                        ))\n                        return OrchestratorResult(\n                            final_answer=final_answer,\n                            success=True,\n                            iterations=iteration + 1,\n                            steps=self.steps,\n                            budget_summary=self.budget.summary(),\n                        )\n\n                # Add observation to history\n                observation = \"\\n---\\n\".join(combined_output)\n                self.history.append(Message(\n                    role=\"user\",\n                    content=f\"Observation:\\n{observation}\",\n                ))\n\n            # Max iterations reached\n            return OrchestratorResult(\n                final_answer=None,\n                success=False,\n                iterations=self.config.max_iterations,\n                steps=self.steps,\n                budget_summary=self.budget.summary(),\n                error=\"Max iterations reached without final answer\",\n            )\n\n        except BudgetExceededError as e:\n            return OrchestratorResult(\n                final_answer=None,\n                success=False,\n                iterations=len([s for s in self.steps if s.action == \"llm_call\"]),\n                steps=self.steps,\n                budget_summary=self.budget.summary(),\n                error=str(e),\n            )\n\n        except RLMError as e:\n            return OrchestratorResult(\n                final_answer=None,\n                success=False,\n                iterations=len([s for s in self.steps if s.action == \"llm_call\"]),\n                steps=self.steps,\n                budget_summary=self.budget.summary(),\n                error=str(e),\n            )\n\n    def chat(self, message: str) -&gt; str:\n        \"\"\"\n        Simple chat interface for one-off questions.\n\n        This is a simplified interface that runs a single iteration\n        and returns the response directly.\n\n        Args:\n            message: User message\n\n        Returns:\n            Assistant response\n        \"\"\"\n        result = self.run(message)\n        return result.final_answer or result.steps[-1].output_data if result.steps else \"\"\n</code></pre>"},{"location":"api/orchestrator/#rlm.core.orchestrator.Orchestrator.run","title":"<code>run(query, context_path=None)</code>","text":"<p>Run the orchestration loop for a user query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>User's question or task</p> required <code>context_path</code> <code>Optional[str | Path]</code> <p>Optional path to context file</p> <code>None</code> <p>Returns:</p> Type Description <code>OrchestratorResult</code> <p>OrchestratorResult with the final answer and execution details</p> Source code in <code>src\\rlm\\core\\orchestrator.py</code> <pre><code>def run(\n    self,\n    query: str,\n    context_path: Optional[str | Path] = None,\n) -&gt; OrchestratorResult:\n    \"\"\"\n    Run the orchestration loop for a user query.\n\n    Args:\n        query: User's question or task\n        context_path: Optional path to context file\n\n    Returns:\n        OrchestratorResult with the final answer and execution details\n    \"\"\"\n    # Reset state\n    self.history = []\n    self.steps = []\n\n    # Setup context if provided\n    if context_path:\n        self.config.context_path = Path(context_path)\n        # Create egress filter with context\n        context_sample = Path(context_path).read_text(encoding=\"utf-8\", errors=\"replace\")[:5000]\n        self.egress_filter = EgressFilter(context=context_sample)\n    else:\n        self.egress_filter = EgressFilter()\n\n    # Add initial user message\n    self.history.append(Message(role=\"user\", content=query))\n\n    try:\n        for iteration in range(self.config.max_iterations):\n            logger.info(f\"Iteration {iteration + 1}/{self.config.max_iterations}\")\n\n            # Call LLM\n            response = self._call_llm(iteration)\n            assistant_message = response.content\n\n            # Check for final answer in LLM response\n            final_answer = self._extract_final_answer(assistant_message)\n            if final_answer:\n                self.steps.append(ExecutionStep(\n                    iteration=iteration,\n                    action=\"final_answer\",\n                    input_data=assistant_message,\n                    output_data=final_answer,\n                    success=True,\n                ))\n                return OrchestratorResult(\n                    final_answer=final_answer,\n                    success=True,\n                    iterations=iteration + 1,\n                    steps=self.steps,\n                    budget_summary=self.budget.summary(),\n                )\n\n            # Add assistant response to history\n            self.history.append(Message(role=\"assistant\", content=assistant_message))\n\n            # Extract and execute code blocks\n            code_blocks = self._extract_code_blocks(assistant_message)\n\n            if not code_blocks:\n                # No code to execute - LLM might be answering directly\n                # Check if this looks like a final answer\n                if iteration &gt; 0:  # After first iteration, treat as final\n                    return OrchestratorResult(\n                        final_answer=assistant_message,\n                        success=True,\n                        iterations=iteration + 1,\n                        steps=self.steps,\n                        budget_summary=self.budget.summary(),\n                    )\n                continue\n\n            # Execute each code block\n            combined_output = []\n            for code in code_blocks:\n                result = self._execute_code(code, iteration)\n\n                if result.oom_killed:\n                    combined_output.append(\"Error: Memory Limit Exceeded (OOMKilled)\")\n                elif result.timed_out:\n                    combined_output.append(\"Error: Execution Timeout\")\n                elif not result.success:\n                    combined_output.append(f\"Error (exit {result.exit_code}):\\n{result.stderr}\")\n                else:\n                    combined_output.append(result.stdout)\n\n                # Check for final answer in output\n                final_answer = self._extract_final_answer(result.stdout)\n                if final_answer:\n                    self.steps.append(ExecutionStep(\n                        iteration=iteration,\n                        action=\"final_answer\",\n                        input_data=result.stdout,\n                        output_data=final_answer,\n                        success=True,\n                    ))\n                    return OrchestratorResult(\n                        final_answer=final_answer,\n                        success=True,\n                        iterations=iteration + 1,\n                        steps=self.steps,\n                        budget_summary=self.budget.summary(),\n                    )\n\n            # Add observation to history\n            observation = \"\\n---\\n\".join(combined_output)\n            self.history.append(Message(\n                role=\"user\",\n                content=f\"Observation:\\n{observation}\",\n            ))\n\n        # Max iterations reached\n        return OrchestratorResult(\n            final_answer=None,\n            success=False,\n            iterations=self.config.max_iterations,\n            steps=self.steps,\n            budget_summary=self.budget.summary(),\n            error=\"Max iterations reached without final answer\",\n        )\n\n    except BudgetExceededError as e:\n        return OrchestratorResult(\n            final_answer=None,\n            success=False,\n            iterations=len([s for s in self.steps if s.action == \"llm_call\"]),\n            steps=self.steps,\n            budget_summary=self.budget.summary(),\n            error=str(e),\n        )\n\n    except RLMError as e:\n        return OrchestratorResult(\n            final_answer=None,\n            success=False,\n            iterations=len([s for s in self.steps if s.action == \"llm_call\"]),\n            steps=self.steps,\n            budget_summary=self.budget.summary(),\n            error=str(e),\n        )\n</code></pre>"},{"location":"api/orchestrator/#rlm.core.orchestrator.Orchestrator.chat","title":"<code>chat(message)</code>","text":"<p>Simple chat interface for one-off questions.</p> <p>This is a simplified interface that runs a single iteration and returns the response directly.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>User message</p> required <p>Returns:</p> Type Description <code>str</code> <p>Assistant response</p> Source code in <code>src\\rlm\\core\\orchestrator.py</code> <pre><code>def chat(self, message: str) -&gt; str:\n    \"\"\"\n    Simple chat interface for one-off questions.\n\n    This is a simplified interface that runs a single iteration\n    and returns the response directly.\n\n    Args:\n        message: User message\n\n    Returns:\n        Assistant response\n    \"\"\"\n    result = self.run(message)\n    return result.final_answer or result.steps[-1].output_data if result.steps else \"\"\n</code></pre>"},{"location":"api/orchestrator/#rlm.core.orchestrator.OrchestratorConfig","title":"<code>rlm.core.orchestrator.OrchestratorConfig</code>  <code>dataclass</code>","text":"<p>Configuration for the orchestrator.</p> Source code in <code>src\\rlm\\core\\orchestrator.py</code> <pre><code>@dataclass\nclass OrchestratorConfig:\n    \"\"\"Configuration for the orchestrator.\"\"\"\n\n    max_iterations: int = field(default_factory=lambda: settings.max_recursion_depth)\n    context_path: Optional[Path] = None\n    system_prompt_mode: str = \"full\"\n    custom_instructions: Optional[str] = None\n    raise_on_leak: bool = False\n</code></pre>"},{"location":"api/orchestrator/#rlm.core.orchestrator.OrchestratorResult","title":"<code>rlm.core.orchestrator.OrchestratorResult</code>  <code>dataclass</code>","text":"<p>Result of an orchestration run.</p> Source code in <code>src\\rlm\\core\\orchestrator.py</code> <pre><code>@dataclass\nclass OrchestratorResult:\n    \"\"\"Result of an orchestration run.\"\"\"\n\n    final_answer: Optional[str]\n    success: bool\n    iterations: int\n    steps: list[ExecutionStep]\n    budget_summary: dict\n    error: Optional[str] = None\n</code></pre>"},{"location":"api/orchestrator/#rlm.core.orchestrator.ExecutionStep","title":"<code>rlm.core.orchestrator.ExecutionStep</code>  <code>dataclass</code>","text":"<p>Record of a single execution step.</p> Source code in <code>src\\rlm\\core\\orchestrator.py</code> <pre><code>@dataclass\nclass ExecutionStep:\n    \"\"\"Record of a single execution step.\"\"\"\n\n    iteration: int\n    action: str  # \"llm_call\", \"code_execution\", \"final_answer\"\n    input_data: str\n    output_data: str\n    success: bool\n    error: Optional[str] = None\n</code></pre>"},{"location":"api/sandbox/","title":"DockerSandbox API","text":""},{"location":"api/sandbox/#rlm.core.repl.docker.DockerSandbox","title":"<code>rlm.core.repl.docker.DockerSandbox</code>","text":"<p>Hardened Docker sandbox for executing untrusted Python code.</p> <p>Security features: - gVisor runtime (runsc) when available - Network isolation (network_mode=\"none\") - Memory limits to prevent OOM attacks - Process limits to prevent fork bombs - CPU quotas to prevent crypto mining - Privilege escalation prevention - Read-only context mounting</p> Example <p>sandbox = DockerSandbox() result = sandbox.execute(\"print('Hello, World!')\") print(result.stdout) Hello, World!</p> Source code in <code>src\\rlm\\core\\repl\\docker.py</code> <pre><code>class DockerSandbox:\n    \"\"\"\n    Hardened Docker sandbox for executing untrusted Python code.\n\n    Security features:\n    - gVisor runtime (runsc) when available\n    - Network isolation (network_mode=\"none\")\n    - Memory limits to prevent OOM attacks\n    - Process limits to prevent fork bombs\n    - CPU quotas to prevent crypto mining\n    - Privilege escalation prevention\n    - Read-only context mounting\n\n    Example:\n        &gt;&gt;&gt; sandbox = DockerSandbox()\n        &gt;&gt;&gt; result = sandbox.execute(\"print('Hello, World!')\")\n        &gt;&gt;&gt; print(result.stdout)\n        Hello, World!\n    \"\"\"\n\n    def __init__(\n        self,\n        image: Optional[str] = None,\n        timeout: Optional[int] = None,\n        config: Optional[SandboxConfig] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Docker sandbox.\n\n        Args:\n            image: Docker image to use (default: from settings)\n            timeout: Execution timeout in seconds (default: from settings)\n            config: Full sandbox configuration (overrides individual params)\n        \"\"\"\n        self.config = config or SandboxConfig()\n        if image:\n            self.config.image = image\n        if timeout:\n            self.config.timeout = timeout\n\n        self._client: Optional[docker.DockerClient] = None\n        self._runtime: Optional[str] = None\n\n    @property\n    def client(self) -&gt; docker.DockerClient:\n        \"\"\"Lazy-load Docker client.\"\"\"\n        if self._client is None:\n            try:\n                self._client = docker.from_env()\n                # Verify Docker is accessible\n                self._client.ping()\n            except DockerException as e:\n                raise SandboxError(\n                    message=\"Failed to connect to Docker daemon\",\n                    details={\"error\": str(e)},\n                ) from e\n        return self._client\n\n    @property\n    def runtime(self) -&gt; str:\n        \"\"\"Detect and cache the best available runtime.\"\"\"\n        if self._runtime is None:\n            self._runtime = self._detect_runtime()\n        return self._runtime\n\n    def _detect_runtime(self) -&gt; str:\n        \"\"\"\n        Detect the most secure available Docker runtime.\n\n        Preference order:\n        1. runsc (gVisor) - Best isolation\n        2. runc with seccomp - Standard isolation\n\n        Returns:\n            Runtime name to use.\n        \"\"\"\n        if self.config.runtime != \"auto\":\n            logger.info(f\"Using configured runtime: {self.config.runtime}\")\n            return self.config.runtime\n\n        try:\n            info = self.client.info()\n            runtimes = info.get(\"Runtimes\", {})\n\n            if \"runsc\" in runtimes:\n                logger.info(\"\u2713 Runtime seguro 'runsc' (gVisor) detectado e ativado.\")\n                return \"runsc\"\n            else:\n                logger.warning(\n                    \"\u26a0 AVISO DE SEGURAN\u00c7A: 'runsc' n\u00e3o encontrado. \"\n                    \"Usando isolamento padr\u00e3o 'runc'.\"\n                )\n                return \"runc\"\n\n        except Exception as e:\n            logger.error(f\"Falha ao detectar runtimes Docker: {e}\")\n            return \"runc\"\n\n    def _ensure_image(self) -&gt; None:\n        \"\"\"Pull the Docker image if not available locally.\"\"\"\n        try:\n            self.client.images.get(self.config.image)\n        except ImageNotFound:\n            logger.info(f\"Pulling Docker image: {self.config.image}\")\n            self.client.images.pull(self.config.image)\n\n    def _build_execution_script(self, code: str, context_path: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Build the Python script to execute inside the container.\n\n        This injects the ContextHandle class and any necessary setup code.\n        \"\"\"\n        setup_code = '''\nimport sys\nimport os\n\n# Disable dangerous imports\n_blocked_modules = {'subprocess', 'multiprocessing', 'ctypes', 'cffi'}\n\nclass ImportBlocker:\n    def find_module(self, name, path=None):\n        if name in _blocked_modules or any(name.startswith(m + '.') for m in _blocked_modules):\n            return self\n        return None\n\n    def load_module(self, name):\n        raise ImportError(f\"Module '{name}' is blocked for security reasons\")\n\nsys.meta_path.insert(0, ImportBlocker())\n'''\n\n        context_setup = \"\"\n        if context_path:\n            context_setup = f'''\n# Context Handle for memory-efficient file access\nclass ContextHandle:\n    def __init__(self, path=\"/mnt/context\"):\n        self.path = path\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"Context file not found at {{path}}\")\n        self._size = os.path.getsize(path)\n\n    @property\n    def size(self):\n        return self._size\n\n    def read_window(self, offset, radius=500):\n        start = max(0, offset - radius)\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            f.seek(start)\n            return f.read(radius * 2)\n\n    def snippet(self, offset, window=500):\n        return self.read_window(offset, window // 2)\n\n    def search(self, pattern, max_results=10):\n        import re\n        matches = []\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            for i, line in enumerate(f):\n                for m in re.finditer(pattern, line):\n                    matches.append((i, m.group()))\n                    if len(matches) &gt;= max_results:\n                        return matches\n        return matches\n\nctx = ContextHandle()\n'''\n\n        return f\"{setup_code}\\n{context_setup}\\n# User code starts here\\n{code}\"\n\n    def execute(\n        self,\n        code: str,\n        context_mount: Optional[str] = None,\n    ) -&gt; ExecutionResult:\n        \"\"\"\n        Execute Python code in a secure Docker container.\n\n        Args:\n            code: Python code to execute\n            context_mount: Optional path to context file to mount read-only\n\n        Returns:\n            ExecutionResult with stdout, stderr, and exit code\n\n        Raises:\n            SandboxError: If container execution fails\n            SecurityViolationError: If security configuration is invalid\n        \"\"\"\n        self._ensure_image()\n\n        # Build the full script with security setup\n        full_script = self._build_execution_script(code, context_mount)\n\n        # Create a temporary file with the script\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\",\n            suffix=\".py\",\n            delete=False,\n            encoding=\"utf-8\",\n        ) as f:\n            f.write(full_script)\n            script_path = f.name\n\n        try:\n            # Configure volumes\n            volumes = {\n                script_path: {\"bind\": \"/tmp/script.py\", \"mode\": \"ro\"},\n            }\n            if context_mount:\n                volumes[context_mount] = {\"bind\": \"/mnt/context\", \"mode\": \"ro\"}\n\n            # Configure network\n            network_mode = \"bridge\" if self.config.network_enabled else \"none\"\n            if self.config.network_enabled:\n                logger.warning(\"\u26a0 Network access enabled - this is a security risk!\")\n\n            # Security options\n            security_opt = [\"no-new-privileges:true\"]\n\n            # CPU configuration (nano_cpus = 10^9 * cores)\n            nano_cpus = int(self.config.cpu_limit * 1_000_000_000)\n\n            logger.debug(\n                f\"Executing code in sandbox (runtime={self.runtime}, \"\n                f\"network={network_mode}, mem={self.config.memory_limit})\"\n            )\n\n            # Run the container\n            container = self.client.containers.run(\n                image=self.config.image,\n                command=[\"python3\", \"/tmp/script.py\"],\n                detach=True,\n                # Security: Runtime\n                runtime=self.runtime,\n                # Security: Network isolation\n                network_mode=network_mode,\n                # Security: Resource limits\n                mem_limit=self.config.memory_limit,\n                memswap_limit=self.config.memory_limit,  # Disable swap\n                nano_cpus=nano_cpus,\n                pids_limit=self.config.pids_limit,\n                # Security: Privileges\n                security_opt=security_opt,\n                # Security: IPC isolation\n                ipc_mode=\"none\",\n                # IO: Volumes\n                volumes=volumes,\n                # Cleanup\n                remove=False,  # We need to inspect exit status first\n            )\n\n            try:\n                # Wait for completion with timeout\n                result = container.wait(timeout=self.config.timeout)\n                exit_code = result.get(\"StatusCode\", -1)\n                timed_out = False\n            except Exception:\n                # Timeout or other error\n                logger.warning(\"Container execution timed out, killing...\")\n                container.kill()\n                exit_code = 124  # Standard timeout exit code\n                timed_out = True\n\n            # Get logs\n            stdout = container.logs(stdout=True, stderr=False).decode(\"utf-8\", errors=\"replace\")\n            stderr = container.logs(stdout=False, stderr=True).decode(\"utf-8\", errors=\"replace\")\n\n            # Check for OOM\n            container.reload()\n            oom_killed = container.attrs.get(\"State\", {}).get(\"OOMKilled\", False)\n\n            # Cleanup\n            container.remove(force=True)\n\n            # Truncate output for safety\n            max_bytes = settings.max_stdout_bytes\n            if len(stdout) &gt; max_bytes:\n                head = stdout[:1000]\n                tail = stdout[-3000:]\n                truncated = len(stdout) - max_bytes\n                stdout = f\"{head}\\n... [TRUNCATED {truncated} bytes] ...\\n{tail}\"\n\n            return ExecutionResult(\n                stdout=stdout,\n                stderr=stderr,\n                exit_code=exit_code,\n                timed_out=timed_out,\n                oom_killed=oom_killed,\n            )\n\n        except ContainerError as e:\n            raise SandboxError(\n                message=\"Container execution failed\",\n                exit_code=e.exit_status,\n                stderr=str(e.stderr),\n            ) from e\n\n        except DockerException as e:\n            raise SandboxError(\n                message=\"Docker error during execution\",\n                details={\"error\": str(e)},\n            ) from e\n\n        finally:\n            # Cleanup temp file\n            Path(script_path).unlink(missing_ok=True)\n\n    def validate_security(self) -&gt; dict:\n        \"\"\"\n        Validate the security configuration.\n\n        Returns:\n            Dictionary with security checks and their status.\n        \"\"\"\n        checks = {\n            \"docker_available\": False,\n            \"gvisor_available\": False,\n            \"network_disabled\": not self.config.network_enabled,\n            \"memory_limited\": bool(self.config.memory_limit),\n            \"pids_limited\": self.config.pids_limit &lt; 100,\n        }\n\n        try:\n            self.client.ping()\n            checks[\"docker_available\"] = True\n            checks[\"gvisor_available\"] = self.runtime == \"runsc\"\n        except Exception:\n            pass\n\n        return checks\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.docker.DockerSandbox.runtime","title":"<code>runtime</code>  <code>property</code>","text":"<p>Detect and cache the best available runtime.</p>"},{"location":"api/sandbox/#rlm.core.repl.docker.DockerSandbox.client","title":"<code>client</code>  <code>property</code>","text":"<p>Lazy-load Docker client.</p>"},{"location":"api/sandbox/#rlm.core.repl.docker.DockerSandbox.execute","title":"<code>execute(code, context_mount=None)</code>","text":"<p>Execute Python code in a secure Docker container.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Python code to execute</p> required <code>context_mount</code> <code>Optional[str]</code> <p>Optional path to context file to mount read-only</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with stdout, stderr, and exit code</p> <p>Raises:</p> Type Description <code>SandboxError</code> <p>If container execution fails</p> <code>SecurityViolationError</code> <p>If security configuration is invalid</p> Source code in <code>src\\rlm\\core\\repl\\docker.py</code> <pre><code>def execute(\n    self,\n    code: str,\n    context_mount: Optional[str] = None,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute Python code in a secure Docker container.\n\n    Args:\n        code: Python code to execute\n        context_mount: Optional path to context file to mount read-only\n\n    Returns:\n        ExecutionResult with stdout, stderr, and exit code\n\n    Raises:\n        SandboxError: If container execution fails\n        SecurityViolationError: If security configuration is invalid\n    \"\"\"\n    self._ensure_image()\n\n    # Build the full script with security setup\n    full_script = self._build_execution_script(code, context_mount)\n\n    # Create a temporary file with the script\n    with tempfile.NamedTemporaryFile(\n        mode=\"w\",\n        suffix=\".py\",\n        delete=False,\n        encoding=\"utf-8\",\n    ) as f:\n        f.write(full_script)\n        script_path = f.name\n\n    try:\n        # Configure volumes\n        volumes = {\n            script_path: {\"bind\": \"/tmp/script.py\", \"mode\": \"ro\"},\n        }\n        if context_mount:\n            volumes[context_mount] = {\"bind\": \"/mnt/context\", \"mode\": \"ro\"}\n\n        # Configure network\n        network_mode = \"bridge\" if self.config.network_enabled else \"none\"\n        if self.config.network_enabled:\n            logger.warning(\"\u26a0 Network access enabled - this is a security risk!\")\n\n        # Security options\n        security_opt = [\"no-new-privileges:true\"]\n\n        # CPU configuration (nano_cpus = 10^9 * cores)\n        nano_cpus = int(self.config.cpu_limit * 1_000_000_000)\n\n        logger.debug(\n            f\"Executing code in sandbox (runtime={self.runtime}, \"\n            f\"network={network_mode}, mem={self.config.memory_limit})\"\n        )\n\n        # Run the container\n        container = self.client.containers.run(\n            image=self.config.image,\n            command=[\"python3\", \"/tmp/script.py\"],\n            detach=True,\n            # Security: Runtime\n            runtime=self.runtime,\n            # Security: Network isolation\n            network_mode=network_mode,\n            # Security: Resource limits\n            mem_limit=self.config.memory_limit,\n            memswap_limit=self.config.memory_limit,  # Disable swap\n            nano_cpus=nano_cpus,\n            pids_limit=self.config.pids_limit,\n            # Security: Privileges\n            security_opt=security_opt,\n            # Security: IPC isolation\n            ipc_mode=\"none\",\n            # IO: Volumes\n            volumes=volumes,\n            # Cleanup\n            remove=False,  # We need to inspect exit status first\n        )\n\n        try:\n            # Wait for completion with timeout\n            result = container.wait(timeout=self.config.timeout)\n            exit_code = result.get(\"StatusCode\", -1)\n            timed_out = False\n        except Exception:\n            # Timeout or other error\n            logger.warning(\"Container execution timed out, killing...\")\n            container.kill()\n            exit_code = 124  # Standard timeout exit code\n            timed_out = True\n\n        # Get logs\n        stdout = container.logs(stdout=True, stderr=False).decode(\"utf-8\", errors=\"replace\")\n        stderr = container.logs(stdout=False, stderr=True).decode(\"utf-8\", errors=\"replace\")\n\n        # Check for OOM\n        container.reload()\n        oom_killed = container.attrs.get(\"State\", {}).get(\"OOMKilled\", False)\n\n        # Cleanup\n        container.remove(force=True)\n\n        # Truncate output for safety\n        max_bytes = settings.max_stdout_bytes\n        if len(stdout) &gt; max_bytes:\n            head = stdout[:1000]\n            tail = stdout[-3000:]\n            truncated = len(stdout) - max_bytes\n            stdout = f\"{head}\\n... [TRUNCATED {truncated} bytes] ...\\n{tail}\"\n\n        return ExecutionResult(\n            stdout=stdout,\n            stderr=stderr,\n            exit_code=exit_code,\n            timed_out=timed_out,\n            oom_killed=oom_killed,\n        )\n\n    except ContainerError as e:\n        raise SandboxError(\n            message=\"Container execution failed\",\n            exit_code=e.exit_status,\n            stderr=str(e.stderr),\n        ) from e\n\n    except DockerException as e:\n        raise SandboxError(\n            message=\"Docker error during execution\",\n            details={\"error\": str(e)},\n        ) from e\n\n    finally:\n        # Cleanup temp file\n        Path(script_path).unlink(missing_ok=True)\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.docker.DockerSandbox.validate_security","title":"<code>validate_security()</code>","text":"<p>Validate the security configuration.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with security checks and their status.</p> Source code in <code>src\\rlm\\core\\repl\\docker.py</code> <pre><code>def validate_security(self) -&gt; dict:\n    \"\"\"\n    Validate the security configuration.\n\n    Returns:\n        Dictionary with security checks and their status.\n    \"\"\"\n    checks = {\n        \"docker_available\": False,\n        \"gvisor_available\": False,\n        \"network_disabled\": not self.config.network_enabled,\n        \"memory_limited\": bool(self.config.memory_limit),\n        \"pids_limited\": self.config.pids_limit &lt; 100,\n    }\n\n    try:\n        self.client.ping()\n        checks[\"docker_available\"] = True\n        checks[\"gvisor_available\"] = self.runtime == \"runsc\"\n    except Exception:\n        pass\n\n    return checks\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.docker.ExecutionResult","title":"<code>rlm.core.repl.docker.ExecutionResult</code>  <code>dataclass</code>","text":"<p>Result of code execution in the sandbox.</p> Source code in <code>src\\rlm\\core\\repl\\docker.py</code> <pre><code>@dataclass\nclass ExecutionResult:\n    \"\"\"Result of code execution in the sandbox.\"\"\"\n\n    stdout: str\n    stderr: str\n    exit_code: int\n    timed_out: bool = False\n    oom_killed: bool = False\n    execution_time_ms: int = 0\n\n    @property\n    def success(self) -&gt; bool:\n        \"\"\"Check if execution was successful.\"\"\"\n        return self.exit_code == 0 and not self.timed_out and not self.oom_killed\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.docker.ExecutionResult.success","title":"<code>success</code>  <code>property</code>","text":"<p>Check if execution was successful.</p>"},{"location":"api/sandbox/#rlm.core.repl.docker.SandboxConfig","title":"<code>rlm.core.repl.docker.SandboxConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Docker sandbox.</p> Source code in <code>src\\rlm\\core\\repl\\docker.py</code> <pre><code>@dataclass\nclass SandboxConfig:\n    \"\"\"Configuration for Docker sandbox.\"\"\"\n\n    image: str = field(default_factory=lambda: settings.docker_image)\n    timeout: int = field(default_factory=lambda: settings.execution_timeout)\n    memory_limit: str = field(default_factory=lambda: settings.memory_limit)\n    cpu_limit: float = field(default_factory=lambda: settings.cpu_limit)\n    pids_limit: int = field(default_factory=lambda: settings.pids_limit)\n    network_enabled: bool = field(default_factory=lambda: settings.network_enabled)\n    runtime: str = field(default_factory=lambda: settings.docker_runtime)\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>RLM is configured via environment variables with the <code>RLM_</code> prefix or a <code>.env</code> file.</p>"},{"location":"getting-started/configuration/#configuration-file","title":"Configuration File","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># API Configuration\nRLM_API_PROVIDER=openai\nRLM_API_KEY=sk-your-api-key\nRLM_MODEL_NAME=gpt-4o\n\n# Execution\nRLM_EXECUTION_MODE=docker\nRLM_DOCKER_RUNTIME=auto\n\n# Safety\nRLM_COST_LIMIT_USD=5.0\nRLM_MAX_RECURSION_DEPTH=5\n</code></pre>"},{"location":"getting-started/configuration/#all-configuration-options","title":"All Configuration Options","text":""},{"location":"getting-started/configuration/#api-settings","title":"API Settings","text":"Variable Type Default Description <code>RLM_API_PROVIDER</code> string <code>openai</code> LLM provider: <code>openai</code>, <code>anthropic</code>, <code>google</code> <code>RLM_API_KEY</code> string - API key for the provider <code>RLM_MODEL_NAME</code> string <code>gpt-4o</code> Model name to use"},{"location":"getting-started/configuration/#execution-settings","title":"Execution Settings","text":"Variable Type Default Description <code>RLM_EXECUTION_MODE</code> string <code>docker</code> <code>docker</code> or <code>local</code> (dev only) <code>RLM_DOCKER_RUNTIME</code> string <code>auto</code> <code>auto</code>, <code>runsc</code>, or <code>runc</code> <code>RLM_DOCKER_IMAGE</code> string <code>python:3.11-slim</code> Docker image for sandbox <code>RLM_EXECUTION_TIMEOUT</code> int <code>30</code> Timeout in seconds (5-300)"},{"location":"getting-started/configuration/#safety-limits","title":"Safety Limits","text":"Variable Type Default Description <code>RLM_COST_LIMIT_USD</code> float <code>5.0</code> Max spending per session <code>RLM_MAX_RECURSION_DEPTH</code> int <code>5</code> Max code execution iterations <code>RLM_MAX_STDOUT_BYTES</code> int <code>4000</code> Max output bytes captured"},{"location":"getting-started/configuration/#security-settings","title":"Security Settings","text":"Variable Type Default Description <code>RLM_MEMORY_LIMIT</code> string <code>512m</code> Container memory limit <code>RLM_CPU_LIMIT</code> float <code>1.0</code> CPU cores limit (0.1-4.0) <code>RLM_PIDS_LIMIT</code> int <code>50</code> Max processes (10-200) <code>RLM_NETWORK_ENABLED</code> bool <code>false</code> Enable network (DANGEROUS)"},{"location":"getting-started/configuration/#egress-filtering","title":"Egress Filtering","text":"Variable Type Default Description <code>RLM_ENTROPY_THRESHOLD</code> float <code>4.5</code> Secret detection threshold (3.0-6.0) <code>RLM_MIN_ENTROPY_LENGTH</code> int <code>256</code> Min length for entropy check <code>RLM_SIMILARITY_THRESHOLD</code> float <code>0.8</code> Context echo threshold (0.5-1.0)"},{"location":"getting-started/configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>Override settings in code:</p> <pre><code>from rlm.config import RLMSettings\n\n# Create custom settings\ncustom_settings = RLMSettings(\n    api_provider=\"anthropic\",\n    api_key=\"sk-ant-...\",\n    model_name=\"claude-3-sonnet-20240229\",\n    cost_limit_usd=10.0,\n)\n</code></pre>"},{"location":"getting-started/configuration/#provider-specific-models","title":"Provider-Specific Models","text":""},{"location":"getting-started/configuration/#openai","title":"OpenAI","text":"<pre><code>RLM_API_PROVIDER=openai\nRLM_MODEL_NAME=gpt-4o          # Recommended\n# RLM_MODEL_NAME=gpt-4-turbo\n# RLM_MODEL_NAME=gpt-4o-mini\n</code></pre>"},{"location":"getting-started/configuration/#anthropic","title":"Anthropic","text":"<pre><code>RLM_API_PROVIDER=anthropic\nRLM_MODEL_NAME=claude-3-sonnet-20240229   # Recommended\n# RLM_MODEL_NAME=claude-3-opus-20240229\n</code></pre>"},{"location":"getting-started/configuration/#google","title":"Google","text":"<pre><code>RLM_API_PROVIDER=google\nRLM_MODEL_NAME=gemini-1.5-pro   # Recommended\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>Docker (for sandbox execution)</li> <li>Optional: gVisor (<code>runsc</code>) for enhanced isolation</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<pre><code>pip install rlm-python\n</code></pre>"},{"location":"getting-started/installation/#with-development-dependencies","title":"With Development Dependencies","text":"<pre><code>pip install \"rlm-python[dev]\"\n</code></pre> <p>This includes: - <code>pytest</code> - Testing framework - <code>pytest-cov</code> - Coverage reporting - <code>mypy</code> - Type checking - <code>ruff</code> - Linting</p>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/rlm-python/rlm.git\ncd rlm\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#docker-setup","title":"Docker Setup","text":"<p>RLM requires Docker for sandboxed code execution.</p>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<ol> <li>Install Docker Desktop</li> <li>Start Docker Desktop</li> <li>Verify installation:    <pre><code>docker run hello-world\n</code></pre></li> </ol>"},{"location":"getting-started/installation/#linux","title":"Linux","text":"<pre><code># Install Docker\ncurl -fsSL https://get.docker.com | sh\n\n# Add user to docker group\nsudo usermod -aG docker $USER\n\n# Verify\ndocker run hello-world\n</code></pre>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<ol> <li>Install Docker Desktop for Mac</li> <li>Start Docker from Applications</li> <li>Verify with <code>docker run hello-world</code></li> </ol>"},{"location":"getting-started/installation/#optional-gvisor-installation","title":"Optional: gVisor Installation","text":"<p>gVisor provides an additional security layer by intercepting syscalls in userspace.</p>"},{"location":"getting-started/installation/#linux-only","title":"Linux Only","text":"<pre><code># Download runsc\ncurl -fsSL https://gvisor.dev/archive.key | sudo gpg --dearmor -o /usr/share/keyrings/gvisor-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/gvisor-archive-keyring.gpg] https://storage.googleapis.com/gvisor/releases release main\" | sudo tee /etc/apt/sources.list.d/gvisor.list &gt; /dev/null\nsudo apt-get update &amp;&amp; sudo apt-get install -y runsc\n\n# Configure Docker\nsudo runsc install\nsudo systemctl restart docker\n</code></pre> <p>Windows/macOS</p> <p>gVisor is only available on Linux. On Windows/macOS, RLM automatically falls back to standard Docker isolation with enhanced security options.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>from rlm import DockerSandbox, settings\n\n# Check configuration\nprint(f\"Provider: {settings.api_provider}\")\nprint(f\"Execution Mode: {settings.execution_mode}\")\n\n# Check Docker sandbox\nsandbox = DockerSandbox()\nsecurity = sandbox.validate_security()\nprint(f\"Docker Available: {security['docker_available']}\")\nprint(f\"gVisor Available: {security['gvisor_available']}\")\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will get you running RLM in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#step-1-configure-api-key","title":"Step 1: Configure API Key","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># .env\nRLM_API_KEY=sk-your-openai-key-here\nRLM_API_PROVIDER=openai\n</code></pre> <p>Or set environment variables:</p> WindowsLinux/macOS <pre><code>$env:RLM_API_KEY = \"sk-your-key\"\n$env:RLM_API_PROVIDER = \"openai\"\n</code></pre> <pre><code>export RLM_API_KEY=\"sk-your-key\"\nexport RLM_API_PROVIDER=\"openai\"\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-basic-usage","title":"Step 2: Basic Usage","text":""},{"location":"getting-started/quickstart/#simple-query-with-code-execution","title":"Simple Query with Code Execution","text":"<pre><code>from rlm import Orchestrator\n\n# Create orchestrator\norchestrator = Orchestrator()\n\n# Run a query that requires computation\nresult = orchestrator.run(\"Calculate the factorial of 20\")\n\nprint(result.final_answer)\n# 2432902008176640000\n\nprint(f\"Iterations: {result.iterations}\")\nprint(f\"Cost: ${result.budget_summary['total_spent_usd']:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#direct-sandbox-usage","title":"Direct Sandbox Usage","text":"<p>For direct code execution without LLM:</p> <pre><code>from rlm import DockerSandbox\n\nsandbox = DockerSandbox()\n\nresult = sandbox.execute(\"\"\"\nimport math\n\n# Calculate some values\npi_value = math.pi\nfactorial_10 = math.factorial(10)\n\nprint(f\"Pi: {pi_value}\")\nprint(f\"10!: {factorial_10}\")\n\"\"\")\n\nprint(result.stdout)\n# Pi: 3.141592653589793\n# 10!: 3628800\n</code></pre>"},{"location":"getting-started/quickstart/#working-with-large-files","title":"Working with Large Files","text":"<pre><code>from rlm import ContextHandle\n\n# Open a large file efficiently\nwith ContextHandle(\"/path/to/large_data.txt\") as ctx:\n    print(f\"File size: {ctx.size_mb:.2f} MB\")\n\n    # Search without loading entire file\n    matches = ctx.search(r\"ERROR.*\")\n\n    for offset, match in matches[:5]:\n        snippet = ctx.snippet(offset, window=200)\n        print(f\"Found at {offset}: {snippet[:100]}...\")\n</code></pre>"},{"location":"getting-started/quickstart/#query-with-context","title":"Query with Context","text":"<pre><code>from rlm import Orchestrator\n\norchestrator = Orchestrator()\n\n# Query against a document\nresult = orchestrator.run(\n    query=\"What are the main topics discussed in this document?\",\n    context_path=\"/path/to/document.txt\"\n)\n\nprint(result.final_answer)\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-check-your-setup","title":"Step 3: Check Your Setup","text":"<p>Verify everything is working:</p> <pre><code>from rlm import DockerSandbox, settings\n\nprint(\"=== RLM Configuration ===\")\nprint(f\"Provider: {settings.api_provider}\")\nprint(f\"Model: {settings.model_name}\")\nprint(f\"Has API Key: {settings.has_api_key}\")\n\nprint(\"\\n=== Security Check ===\")\nsandbox = DockerSandbox()\nsecurity = sandbox.validate_security()\nfor check, status in security.items():\n    emoji = \"\u2705\" if status else \"\u26a0\ufe0f\"\n    print(f\"{emoji} {check}: {status}\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Reference - All available options</li> <li>Orchestrator Guide - Advanced agent usage</li> <li>Security Best Practices - Production deployment</li> </ul>"},{"location":"guide/budget/","title":"Budget Management","text":"<p>The <code>BudgetManager</code> tracks API costs and enforces spending limits.</p>"},{"location":"guide/budget/#why-budget-management","title":"Why Budget Management?","text":"<p>LLM APIs charge per token. Without limits, a runaway loop could:</p> <ul> <li>Consume thousands of tokens</li> <li>Cost hundreds of dollars</li> <li>Drain your API budget</li> </ul>"},{"location":"guide/budget/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm.utils import BudgetManager\n\nbudget = BudgetManager(limit_usd=5.0)\n\n# Record usage\ncost = budget.record_usage(\n    model=\"gpt-4o\",\n    input_tokens=1000,\n    output_tokens=500\n)\n\nprint(f\"This call cost: ${cost:.4f}\")\nprint(f\"Total spent: ${budget.total_spent:.4f}\")\nprint(f\"Remaining: ${budget.remaining_budget:.4f}\")\n</code></pre>"},{"location":"guide/budget/#budget-enforcement","title":"Budget Enforcement","text":"<p>When the limit is exceeded:</p> <pre><code>from rlm.core.exceptions import BudgetExceededError\n\ntry:\n    budget.record_usage(\"gpt-4o\", 100000, 50000)\nexcept BudgetExceededError as e:\n    print(f\"Budget exceeded: ${e.spent:.2f} / ${e.limit:.2f}\")\n</code></pre>"},{"location":"guide/budget/#usage-summary","title":"Usage Summary","text":"<pre><code>summary = budget.summary()\nprint(summary)\n# {\n#     'total_spent_usd': 0.0125,\n#     'limit_usd': 5.0,\n#     'remaining_usd': 4.9875,\n#     'usage_percentage': 0.25,\n#     'total_requests': 3,\n#     'total_input_tokens': 5000,\n#     'total_output_tokens': 2000\n# }\n</code></pre>"},{"location":"guide/budget/#pricing-data","title":"Pricing Data","text":"<p>Pricing is loaded from <code>pricing.json</code>:</p> <pre><code>{\n  \"models\": {\n    \"gpt-4o\": {\n      \"input_cost_per_m\": 5.00,\n      \"output_cost_per_m\": 15.00\n    }\n  }\n}\n</code></pre> <p>Costs are in USD per million tokens.</p>"},{"location":"guide/budget/#custom-pricing","title":"Custom Pricing","text":"<pre><code>RLM_PRICING_PATH=/path/to/custom/pricing.json\n</code></pre>"},{"location":"guide/budget/#configuration","title":"Configuration","text":"<pre><code>RLM_COST_LIMIT_USD=5.0   # Default budget limit\n</code></pre>"},{"location":"guide/budget/#with-orchestrator","title":"With Orchestrator","text":"<p>The Orchestrator automatically tracks costs:</p> <pre><code>from rlm import Orchestrator\n\norchestrator = Orchestrator()\nresult = orchestrator.run(\"Complex query...\")\n\nprint(f\"Cost: ${result.budget_summary['total_spent_usd']:.4f}\")\nprint(f\"Tokens: {result.budget_summary['total_input_tokens'] + result.budget_summary['total_output_tokens']}\")\n</code></pre>"},{"location":"guide/budget/#resetting-budget","title":"Resetting Budget","text":"<pre><code>budget.reset()\nprint(budget.total_spent)  # 0.0\n</code></pre>"},{"location":"guide/context-handle/","title":"Context Handling","text":"<p>The <code>ContextHandle</code> class provides memory-efficient access to large files.</p>"},{"location":"guide/context-handle/#why-contexthandle","title":"Why ContextHandle?","text":"<p>When working with large context files (100MB, 1GB, or more), loading the entire file into memory is:</p> <ul> <li>Slow - Takes time to read</li> <li>Expensive - Uses lots of RAM</li> <li>Risky - Can crash with OOM</li> </ul> <p><code>ContextHandle</code> uses memory-mapping (<code>mmap</code>) for O(1) random access without loading the entire file.</p>"},{"location":"guide/context-handle/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm import ContextHandle\n\nwith ContextHandle(\"/path/to/large_file.txt\") as ctx:\n    print(f\"File size: {ctx.size_mb:.2f} MB\")\n\n    # Read first 1000 bytes\n    header = ctx.head(1000)\n\n    # Read last 1000 bytes\n    footer = ctx.tail(1000)\n</code></pre>"},{"location":"guide/context-handle/#search-api","title":"Search API","text":"<p>Find patterns without reading the entire file:</p> <pre><code>ctx = ContextHandle(\"/path/to/logs.txt\")\n\n# Search with regex\nmatches = ctx.search(r\"ERROR.*timeout\", max_results=10)\n\nfor offset, match_text in matches:\n    print(f\"Found at byte {offset}: {match_text}\")\n\n    # Get surrounding context\n    snippet = ctx.snippet(offset, window=500)\n    print(snippet)\n</code></pre>"},{"location":"guide/context-handle/#line-based-search","title":"Line-Based Search","text":"<pre><code># Search and get line numbers\nmatches = ctx.search_lines(r\"CRITICAL\", max_results=5)\n\nfor line_num, line, context in matches:\n    print(f\"Line {line_num}: {line}\")\n</code></pre>"},{"location":"guide/context-handle/#windowed-reading","title":"Windowed Reading","text":"<p>Read specific portions:</p> <pre><code># Read around a specific offset\ntext = ctx.read_window(offset=50000, radius=500)\n\n# Or using snippet\ntext = ctx.snippet(offset=50000, window=1000)\n\n# Read exact range\ntext = ctx.read(start=1000, length=500)\n</code></pre>"},{"location":"guide/context-handle/#iterating-lines","title":"Iterating Lines","text":"<p>For streaming access:</p> <pre><code>for line_num, line in ctx.iterate_lines(start_line=100):\n    if line_num &gt; 200:\n        break\n    process(line)\n</code></pre>"},{"location":"guide/context-handle/#api-reference","title":"API Reference","text":"<pre><code>class ContextHandle:\n    size: int           # Total size in bytes\n    size_mb: float      # Size in megabytes\n    path: Path          # File path\n\n    def read(start: int, length: int) -&gt; str\n    def read_window(offset: int, radius: int = 500) -&gt; str\n    def snippet(offset: int, window: int = 500) -&gt; str\n    def head(n_bytes: int = 1000) -&gt; str\n    def tail(n_bytes: int = 1000) -&gt; str\n\n    def search(pattern: str, max_results: int = 10) -&gt; List[Tuple[int, str]]\n    def search_lines(pattern: str, max_results: int = 10) -&gt; List[Tuple[int, str, str]]\n\n    def iterate_lines(start_line: int = 1) -&gt; Iterator[Tuple[int, str]]\n\n    def close() -&gt; None\n</code></pre>"},{"location":"guide/context-handle/#in-the-sandbox","title":"In the Sandbox","text":"<p>When mounted in the Docker sandbox, use the <code>ctx</code> global variable:</p> <pre><code># Inside sandbox execution\nmatches = ctx.search(r\"important pattern\")\nfor offset, match in matches:\n    print(ctx.snippet(offset))\n</code></pre>"},{"location":"guide/egress-filtering/","title":"Egress Filtering","text":"<p>Egress filtering prevents sensitive data from leaking through code execution output.</p>"},{"location":"guide/egress-filtering/#how-it-works","title":"How It Works","text":"<p>The <code>EgressFilter</code> applies multiple detection layers:</p> <ol> <li>Entropy Detection - High-entropy strings suggest secrets</li> <li>Pattern Matching - Known secret formats (API keys, JWTs)</li> <li>Context Echo - Prevents printing raw context back</li> <li>Size Limiting - Truncates excessive output</li> </ol>"},{"location":"guide/egress-filtering/#detection-methods","title":"Detection Methods","text":""},{"location":"guide/egress-filtering/#shannon-entropy","title":"Shannon Entropy","text":"<p>Secrets have high entropy (randomness). Normal text has lower entropy.</p> Content Type Typical Entropy Repetitive text 0-2 Natural language 2-4 Code 4-4.5 Secrets/keys 4.5-6+ <pre><code>from rlm.security.egress import calculate_shannon_entropy\n\n# Normal text\nentropy = calculate_shannon_entropy(\"Hello, how are you?\")\nprint(f\"Text entropy: {entropy:.2f}\")  # ~3.5\n\n# API key\nentropy = calculate_shannon_entropy(\"sk-a1b2c3d4e5f6g7h8i9j0\")\nprint(f\"Key entropy: {entropy:.2f}\")  # ~4.5+\n</code></pre>"},{"location":"guide/egress-filtering/#pattern-detection","title":"Pattern Detection","text":"<p>Known secret patterns are detected:</p> <ul> <li>AWS Access Keys (<code>AKIA...</code>)</li> <li>API Keys (<code>api_key=...</code>)</li> <li>Private Keys (<code>-----BEGIN...</code>)</li> <li>JWT Tokens (<code>eyJ...</code>)</li> <li>Bearer Tokens</li> </ul> <pre><code>from rlm.security.egress import detect_secrets\n\ntext = \"My key is sk-abc123def456ghi789\"\nsecrets = detect_secrets(text)\nprint(secrets)  # [('api_key', 'sk-abc123def...')]\n</code></pre>"},{"location":"guide/egress-filtering/#context-echo-prevention","title":"Context Echo Prevention","text":"<p>Prevents the LLM from printing the raw context:</p> <pre><code>from rlm.security.egress import EgressFilter\n\ncontext = \"Sensitive company data here...\"\nfilter = EgressFilter(context=context)\n\n# Try to echo the context\noutput = \"Sensitive company data here...\"\nis_echo, similarity = filter.check_context_echo(output)\nprint(f\"Echo detected: {is_echo}\")  # True\n</code></pre>"},{"location":"guide/egress-filtering/#using-egressfilter","title":"Using EgressFilter","text":""},{"location":"guide/egress-filtering/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm.security.egress import sanitize_output\n\nraw_output = \"Result: AKIAIOSFODNN7EXAMPLE\"\nsafe_output = sanitize_output(raw_output)\nprint(safe_output)  # Result: [REDACTED: aws_access_key]\n</code></pre>"},{"location":"guide/egress-filtering/#with-context-protection","title":"With Context Protection","text":"<pre><code>from rlm.security.egress import EgressFilter\n\nfilter = EgressFilter(\n    context=\"My secret document...\",\n    entropy_threshold=4.5,\n    similarity_threshold=0.8,\n)\n\noutput = filter.filter(raw_output)\n</code></pre>"},{"location":"guide/egress-filtering/#raise-on-leak","title":"Raise on Leak","text":"<pre><code>from rlm.security.egress import sanitize_output\nfrom rlm.core.exceptions import DataLeakageError\n\ntry:\n    sanitize_output(\n        \"-----BEGIN RSA PRIVATE KEY-----\",\n        raise_on_leak=True\n    )\nexcept DataLeakageError as e:\n    print(f\"Leak prevented: {e}\")\n</code></pre>"},{"location":"guide/egress-filtering/#configuration","title":"Configuration","text":"<p>Adjust thresholds via environment variables:</p> <pre><code>RLM_ENTROPY_THRESHOLD=4.5      # Higher = less sensitive\nRLM_SIMILARITY_THRESHOLD=0.8   # Higher = exact matches only\nRLM_MIN_ENTROPY_LENGTH=256     # Minimum string length to check\nRLM_MAX_STDOUT_BYTES=4000      # Truncation limit\n</code></pre>"},{"location":"guide/llm-providers/","title":"LLM Providers","text":"<p>RLM supports multiple LLM providers with a unified interface.</p>"},{"location":"guide/llm-providers/#supported-providers","title":"Supported Providers","text":"Provider Models Streaming OpenAI GPT-4, GPT-4o, GPT-4o-mini \u2705 Anthropic Claude 3 Opus, Sonnet, Haiku \u2705 Google Gemini 1.5 Pro \u2705"},{"location":"guide/llm-providers/#configuration","title":"Configuration","text":""},{"location":"guide/llm-providers/#openai","title":"OpenAI","text":"<pre><code>RLM_API_PROVIDER=openai\nRLM_API_KEY=sk-...\nRLM_MODEL_NAME=gpt-4o\n</code></pre>"},{"location":"guide/llm-providers/#anthropic","title":"Anthropic","text":"<pre><code>RLM_API_PROVIDER=anthropic\nRLM_API_KEY=sk-ant-api03-...\nRLM_MODEL_NAME=claude-3-sonnet-20240229\n</code></pre>"},{"location":"guide/llm-providers/#google","title":"Google","text":"<pre><code>RLM_API_PROVIDER=google\nRLM_API_KEY=AIza...\nRLM_MODEL_NAME=gemini-1.5-pro\n</code></pre>"},{"location":"guide/llm-providers/#programmatic-usage","title":"Programmatic Usage","text":""},{"location":"guide/llm-providers/#using-the-factory","title":"Using the Factory","text":"<pre><code>from rlm.llm import create_llm_client\n\nclient = create_llm_client(\n    provider=\"openai\",\n    api_key=\"sk-...\",\n    model=\"gpt-4o\",\n    temperature=0.0,\n)\n</code></pre>"},{"location":"guide/llm-providers/#direct-client-usage","title":"Direct Client Usage","text":"<pre><code>from rlm.llm.openai_client import OpenAIClient\nfrom rlm.llm.base import Message\n\nclient = OpenAIClient(\n    api_key=\"sk-...\",\n    model=\"gpt-4o\",\n)\n\nresponse = client.complete([\n    Message(role=\"user\", content=\"What is 2+2?\")\n])\n\nprint(response.content)  # \"4\"\nprint(response.usage.total_tokens)  # Token count\n</code></pre>"},{"location":"guide/llm-providers/#streaming","title":"Streaming","text":"<pre><code>for chunk in client.stream([\n    Message(role=\"user\", content=\"Write a haiku\")\n]):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"guide/llm-providers/#custom-system-prompts","title":"Custom System Prompts","text":"<pre><code>response = client.complete(\n    messages=[Message(role=\"user\", content=\"Hello\")],\n    system_prompt=\"You are a helpful assistant.\",\n)\n</code></pre>"},{"location":"guide/llm-providers/#response-object","title":"Response Object","text":"<pre><code>@dataclass\nclass LLMResponse:\n    content: str              # Generated text\n    model: str                # Model used\n    usage: TokenUsage         # Token counts\n    finish_reason: str        # Why generation stopped\n\n@dataclass\nclass TokenUsage:\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n</code></pre>"},{"location":"guide/llm-providers/#error-handling","title":"Error Handling","text":"<pre><code>from rlm.core.exceptions import LLMError\n\ntry:\n    response = client.complete([...])\nexcept LLMError as e:\n    print(f\"Provider: {e.provider}\")\n    print(f\"Error: {e.message}\")\n</code></pre>"},{"location":"guide/orchestrator/","title":"Orchestrator","text":"<p>The <code>Orchestrator</code> is the main class for running LLM-powered code execution loops.</p>"},{"location":"guide/orchestrator/#overview","title":"Overview","text":"<p>The Orchestrator coordinates:</p> <ol> <li>Sending queries to the LLM</li> <li>Extracting code blocks from responses</li> <li>Executing code in the Docker sandbox</li> <li>Filtering output through egress controls</li> <li>Returning results to the LLM for iteration</li> </ol> <pre><code>graph LR\n    A[User Query] --&gt; B[Orchestrator]\n    B --&gt; C[LLM]\n    C --&gt; D[Code Extraction]\n    D --&gt; E[Docker Sandbox]\n    E --&gt; F[Egress Filter]\n    F --&gt; C\n    C --&gt; G[Final Answer]</code></pre>"},{"location":"guide/orchestrator/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm import Orchestrator\n\norchestrator = Orchestrator()\nresult = orchestrator.run(\"What is the square root of 144?\")\n\nprint(result.final_answer)  # 12.0\nprint(result.success)       # True\nprint(result.iterations)    # 1\n</code></pre>"},{"location":"guide/orchestrator/#configuration","title":"Configuration","text":"<pre><code>from rlm import Orchestrator\nfrom rlm.core.orchestrator import OrchestratorConfig\n\nconfig = OrchestratorConfig(\n    max_iterations=10,\n    system_prompt_mode=\"full\",  # or \"simple\"\n    custom_instructions=\"Always show your work.\",\n)\n\norchestrator = Orchestrator(config=config)\n</code></pre>"},{"location":"guide/orchestrator/#result-object","title":"Result Object","text":"<pre><code>@dataclass\nclass OrchestratorResult:\n    final_answer: Optional[str]  # The extracted answer\n    success: bool                # Whether execution succeeded\n    iterations: int              # Number of LLM calls\n    steps: list[ExecutionStep]   # Detailed execution log\n    budget_summary: dict         # Cost tracking\n    error: Optional[str]         # Error message if failed\n</code></pre>"},{"location":"guide/orchestrator/#custom-llm-client","title":"Custom LLM Client","text":"<pre><code>from rlm import Orchestrator\nfrom rlm.llm import create_llm_client\n\n# Use a different provider\nclient = create_llm_client(\n    provider=\"anthropic\",\n    api_key=\"sk-ant-...\",\n    model=\"claude-3-sonnet-20240229\"\n)\n\norchestrator = Orchestrator(llm_client=client)\n</code></pre>"},{"location":"guide/orchestrator/#with-context-file","title":"With Context File","text":"<pre><code>result = orchestrator.run(\n    query=\"Summarize the key findings\",\n    context_path=\"/data/research_paper.txt\"\n)\n</code></pre>"},{"location":"guide/orchestrator/#simple-chat-interface","title":"Simple Chat Interface","text":"<p>For one-off questions without the full result object:</p> <pre><code>answer = orchestrator.chat(\"What is 2+2?\")\nprint(answer)  # 4\n</code></pre>"},{"location":"guide/sandbox/","title":"Docker Sandbox","text":"<p>The <code>DockerSandbox</code> provides secure, isolated code execution in Docker containers.</p>"},{"location":"guide/sandbox/#security-features","title":"Security Features","text":"Feature Default Description gVisor Runtime Auto-detect Intercepts syscalls in userspace Network Isolation Enabled <code>network_mode=\"none\"</code> Memory Limit 512MB Prevents OOM attacks PID Limit 50 Prevents fork bombs CPU Quota 1 core Prevents crypto mining No New Privileges Enabled Blocks privilege escalation"},{"location":"guide/sandbox/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm import DockerSandbox\n\nsandbox = DockerSandbox()\n\nresult = sandbox.execute(\"\"\"\nprint(\"Hello from sandbox!\")\nimport sys\nprint(f\"Python version: {sys.version}\")\n\"\"\")\n\nprint(result.stdout)\nprint(f\"Exit code: {result.exit_code}\")\nprint(f\"Success: {result.success}\")\n</code></pre>"},{"location":"guide/sandbox/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from rlm.core.repl.docker import DockerSandbox, SandboxConfig\n\nconfig = SandboxConfig(\n    image=\"python:3.11-slim\",\n    timeout=60,\n    memory_limit=\"1g\",\n    cpu_limit=2.0,\n    pids_limit=100,\n    network_enabled=False,\n)\n\nsandbox = DockerSandbox(config=config)\n</code></pre>"},{"location":"guide/sandbox/#execution-result","title":"Execution Result","text":"<pre><code>@dataclass\nclass ExecutionResult:\n    stdout: str           # Standard output\n    stderr: str           # Standard error\n    exit_code: int        # Process exit code\n    timed_out: bool       # True if execution timed out\n    oom_killed: bool      # True if killed by OOM\n\n    @property\n    def success(self) -&gt; bool:\n        return self.exit_code == 0 and not self.timed_out and not self.oom_killed\n</code></pre>"},{"location":"guide/sandbox/#with-context-mount","title":"With Context Mount","text":"<p>Mount a file as read-only for the code to access:</p> <pre><code>result = sandbox.execute(\n    code=\"\"\"\nwith open('/mnt/context', 'r') as f:\n    print(f.read()[:100])\n\"\"\",\n    context_mount=\"/path/to/data.txt\"\n)\n</code></pre>"},{"location":"guide/sandbox/#security-validation","title":"Security Validation","text":"<pre><code>security = sandbox.validate_security()\nprint(security)\n# {\n#     'docker_available': True,\n#     'gvisor_available': True,\n#     'network_disabled': True,\n#     'memory_limited': True,\n#     'pids_limited': True\n# }\n</code></pre>"},{"location":"guide/sandbox/#blocked-modules","title":"Blocked Modules","text":"<p>The sandbox automatically blocks dangerous modules:</p> <ul> <li><code>subprocess</code></li> <li><code>multiprocessing</code></li> <li><code>ctypes</code></li> <li><code>cffi</code></li> </ul> <pre><code># This will raise ImportError in the sandbox:\nimport subprocess  # Blocked!\n</code></pre>"},{"location":"security/architecture/","title":"Security Architecture","text":"<p>RLM v2.0 implements defense in depth with multiple security layers.</p>"},{"location":"security/architecture/#threat-model","title":"Threat Model","text":""},{"location":"security/architecture/#attack-vectors-we-defend-against","title":"Attack Vectors We Defend Against","text":"Attack Description Mitigation Network Exfiltration Send data to external server Network isolation File Access Read sensitive system files Container isolation, volumes Memory Bomb Exhaust host memory Memory limits (cgroups) Fork Bomb Exhaust host PIDs PID limits CPU Mining Cryptomining on host CPU CPU quotas Container Escape Break out of container gVisor, no privileges Secret Leakage Extract secrets via stdout Egress filtering"},{"location":"security/architecture/#security-layers","title":"Security Layers","text":""},{"location":"security/architecture/#layer-1-runtime-isolation","title":"Layer 1: Runtime Isolation","text":"<pre><code>graph TD\n    A[Host Kernel] --&gt; B[gVisor Sentry]\n    B --&gt; C[Container]\n    C --&gt; D[Untrusted Code]</code></pre> <p>gVisor (runsc) runs containers with a user-space kernel that intercepts all syscalls. Even critical kernel vulnerabilities cannot escape the sandbox.</p> <p>When gVisor is unavailable, we fall back to standard Docker with enhanced security options.</p>"},{"location":"security/architecture/#layer-2-network-isolation","title":"Layer 2: Network Isolation","text":"<pre><code># Docker configuration\nnetwork_mode = \"none\"\n</code></pre> <p>The container has zero network access:</p> <ul> <li>Cannot resolve DNS</li> <li>Cannot open sockets</li> <li>Cannot reach any IP address</li> </ul> <p>This prevents: - Data exfiltration via HTTP - Reverse shells - DNS tunneling</p>"},{"location":"security/architecture/#layer-3-resource-limits","title":"Layer 3: Resource Limits","text":"<pre><code>mem_limit = \"512m\"\nmemswap_limit = \"512m\"  # No swap\npids_limit = 50\ncpu_quota = 1 core\n</code></pre> <p>These limits prevent denial-of-service attacks against the host.</p>"},{"location":"security/architecture/#layer-4-privilege-restrictions","title":"Layer 4: Privilege Restrictions","text":"<pre><code>security_opt = [\"no-new-privileges:true\"]\nipc_mode = \"none\"\n</code></pre> <p>Prevents: - Privilege escalation via setuid - Shared memory attacks</p>"},{"location":"security/architecture/#layer-5-egress-filtering","title":"Layer 5: Egress Filtering","text":"<p>Even if code generates output, we filter it for:</p> <ul> <li>High entropy - Potential secrets/keys</li> <li>Known patterns - AWS keys, JWTs, private keys</li> <li>Context echo - Raw context printing</li> </ul>"},{"location":"security/architecture/#security-validation","title":"Security Validation","text":"<p>Run the security test suite:</p> <pre><code>pytest tests/security/ -v -m security\n</code></pre> <p>This tests:</p> Test Attack Expected Result <code>test_network_blocked</code> <code>socket.connect()</code> <code>OSError: Network unreachable</code> <code>test_memory_bomb_killed</code> <code>'a' * 10**9</code> OOMKilled, exit 137 <code>test_sensitive_file_access</code> <code>open('/etc/shadow')</code> <code>PermissionError</code> <code>test_subprocess_blocked</code> <code>import subprocess</code> <code>ImportError</code>"},{"location":"security/architecture/#configuration-security-levels","title":"Configuration Security Levels","text":""},{"location":"security/architecture/#maximum-security-recommended","title":"Maximum Security (Recommended)","text":"<pre><code>RLM_EXECUTION_MODE=docker\nRLM_DOCKER_RUNTIME=runsc  # Requires gVisor\nRLM_NETWORK_ENABLED=false\nRLM_MEMORY_LIMIT=512m\n</code></pre>"},{"location":"security/architecture/#standard-security","title":"Standard Security","text":"<pre><code>RLM_EXECUTION_MODE=docker\nRLM_DOCKER_RUNTIME=runc  # Standard Docker\nRLM_NETWORK_ENABLED=false\nRLM_MEMORY_LIMIT=512m\n</code></pre>"},{"location":"security/architecture/#development-only-unsafe","title":"Development Only (UNSAFE)","text":"<pre><code>RLM_EXECUTION_MODE=local  # \u26a0\ufe0f No isolation!\n</code></pre> <p>Never use <code>local</code> mode in production</p> <p>Local mode executes code directly on your system without any isolation.</p>"},{"location":"security/best-practices/","title":"Security Best Practices","text":"<p>Guidelines for deploying RLM securely in production.</p>"},{"location":"security/best-practices/#deployment-checklist","title":"Deployment Checklist","text":"<ul> <li>[ ] Docker is running with security updates</li> <li>[ ] gVisor (runsc) is installed if on Linux</li> <li>[ ] Network isolation is enabled (<code>RLM_NETWORK_ENABLED=false</code>)</li> <li>[ ] Memory limits are configured</li> <li>[ ] API keys are stored securely (not in code)</li> <li>[ ] Budget limits are set appropriately</li> <li>[ ] Egress filtering thresholds are reviewed</li> </ul>"},{"location":"security/best-practices/#api-key-security","title":"API Key Security","text":""},{"location":"security/best-practices/#dont","title":"\u274c Don't","text":"<pre><code># Never hardcode keys\nclient = OpenAIClient(api_key=\"sk-abc123...\")\n</code></pre>"},{"location":"security/best-practices/#do","title":"\u2705 Do","text":"<pre><code># Use environment variables\nfrom rlm import settings\n# RLM_API_KEY loaded from .env or environment\n</code></pre>"},{"location":"security/best-practices/#recommended-use-secrets-managers","title":"Recommended: Use Secrets Managers","text":"<pre><code>import os\nfrom azure.keyvault.secrets import SecretClient\n\n# Load from Azure Key Vault\nsecret = keyvault_client.get_secret(\"rlm-api-key\")\nos.environ[\"RLM_API_KEY\"] = secret.value\n</code></pre>"},{"location":"security/best-practices/#docker-hardening","title":"Docker Hardening","text":""},{"location":"security/best-practices/#use-minimal-base-images","title":"Use Minimal Base Images","text":"<pre><code>RLM_DOCKER_IMAGE=python:3.11-slim  # Good\n# Avoid: python:3.11  # Larger attack surface\n</code></pre>"},{"location":"security/best-practices/#pre-install-dependencies","title":"Pre-install Dependencies","text":"<p>Since network is disabled, all dependencies must be in the image:</p> <pre><code># Custom sandbox image\nFROM python:3.11-slim\n\nRUN pip install --no-cache-dir \\\n    numpy pandas scipy scikit-learn\n\n# No network access at runtime\n</code></pre>"},{"location":"security/best-practices/#keep-images-updated","title":"Keep Images Updated","text":"<pre><code>docker pull python:3.11-slim\n# Regularly update for security patches\n</code></pre>"},{"location":"security/best-practices/#network-considerations","title":"Network Considerations","text":""},{"location":"security/best-practices/#never-enable-network-in-production","title":"Never Enable Network in Production","text":"<pre><code>RLM_NETWORK_ENABLED=false  # Must be false\n</code></pre> <p>If you need external data: 1. Pre-fetch data before execution 2. Mount as read-only volume 3. Use <code>ContextHandle</code> for access</p>"},{"location":"security/best-practices/#logging-and-auditing","title":"Logging and Auditing","text":"<p>Log all orchestrator executions:</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"rlm\")\n\n# Logs will include:\n# - Code executed\n# - Execution results\n# - Egress filter actions\n# - Cost tracking\n</code></pre>"},{"location":"security/best-practices/#rate-limiting","title":"Rate Limiting","text":"<p>Protect against abuse:</p> <pre><code>from datetime import datetime, timedelta\n\nclass RateLimiter:\n    def __init__(self, max_requests=10, window=timedelta(minutes=1)):\n        self.requests = []\n        self.max = max_requests\n        self.window = window\n\n    def check(self):\n        now = datetime.now()\n        self.requests = [r for r in self.requests if now - r &lt; self.window]\n        if len(self.requests) &gt;= self.max:\n            raise RateLimitError(\"Too many requests\")\n        self.requests.append(now)\n</code></pre>"},{"location":"security/best-practices/#budget-protection","title":"Budget Protection","text":"<p>Set conservative limits:</p> <pre><code>RLM_COST_LIMIT_USD=5.0  # Per session\nRLM_MAX_RECURSION_DEPTH=5  # Prevent infinite loops\n</code></pre>"},{"location":"security/best-practices/#monitoring","title":"Monitoring","text":"<p>Track these metrics:</p> <ul> <li>Execution count per hour</li> <li>Average cost per execution</li> <li>Egress filter triggers</li> <li>Container OOM kills</li> <li>Security test failures</li> </ul>"},{"location":"security/best-practices/#incident-response","title":"Incident Response","text":"<p>If a security issue is detected:</p> <ol> <li>Immediately revoke API keys</li> <li>Review execution logs</li> <li>Check for data exfiltration attempts</li> <li>Update egress filter patterns</li> <li>Rotate all credentials</li> </ol>"}]}