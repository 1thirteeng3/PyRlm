{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"\ud83d\udee1\ufe0f RLM-Python <p> Secure Sandbox for AI Agents     Replace unsafe <code>exec()</code> with Docker isolation, gVisor protection, and real-time data leak prevention.   </p>        Get Started             View on GitHub"},{"location":"#see-it-in-action","title":"See It In Action","text":"<pre>\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 $ python demo.py                                                \u2502\n\u2502                                                                 \u2502\n\u2502 \ud83e\udd16 Agent: Attempting to read environment variables...           \u2502\n\u2502 \ud83d\udcdd Code: import os; print(os.environ.get('API_KEY'))            \u2502\n\u2502                                                                 \u2502\n\u2502 \ud83d\udee1\ufe0f [SECURITY REDACTION: Secret Pattern Detected]                \u2502\n\u2502    Egress filter blocked potential API key exfiltration.        \u2502\n\u2502                                                                 \u2502\n\u2502 \ud83e\udd16 Agent: Let me calculate fibonacci instead...                 \u2502\n\u2502 \ud83d\udcdd Code: def fib(n): return n if n &lt; 2 else fib(n-1) + fib(n-2) \u2502\n\u2502          print(f\"FINAL({fib(10)})\")                             \u2502\n\u2502                                                                 \u2502\n\u2502 \u2705 Result: 55                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre>"},{"location":"#why-rlm-python","title":"Why RLM-Python?","text":"\ud83d\udee1\ufe0f gVisor Security <p>Kernel-level syscall interception blocks dangerous operations before they reach your host.</p> \u26a1 AsyncIO Native <p>True non-blocking execution perfect for FastAPI, aiohttp, and concurrent workloads.</p> \ud83d\udd0d Egress Filtering <p>Shannon entropy detection catches API keys, JWTs, and secrets before they leave the sandbox.</p> \ud83d\udce6 Clean Boot <p>Ephemeral containers with TemporaryDirectory cleanup. No state, no leaks, no traces.</p>"},{"location":"#quick-comparison","title":"Quick Comparison","text":"Feature <code>exec()</code> / <code>eval()</code> LangChain REPL RLM v3.0 Isolation \u274c None (Host) \u26a0\ufe0f Limited \u2705 Docker + gVisor Network \ud83d\udd13 Open \ud83d\udd13 Open \ud83d\udd12 Blocked by Default Concurrency \u274c Blocking \u274c Blocking \u2705 Native AsyncIO Data Leak Prevention \u274c None \u274c None \u2705 Egress Filtering Resource Limits \u274c None \u274c None \u2705 Memory/CPU/PIDs"},{"location":"#get-started-in-60-seconds","title":"Get Started in 60 Seconds","text":"<pre><code>import asyncio\nfrom rlm import Orchestrator\n\nasync def main():\n    agent = Orchestrator()\n    result = await agent.arun(\"Calculate the first 10 prime numbers\")\n    print(result.final_answer)\n\nasyncio.run(main())\n</code></pre> <p> Quick Start Guide  Security Architecture</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#300-2026-01-22","title":"[3.0.0] - 2026-01-22","text":""},{"location":"changelog/#the-perfection-update","title":"The Perfection Update \ud83c\udfaf","text":"<p>v3.0 focuses on architectural perfection: DRY code, strict parsing, resource safety, and non-blocking I/O.</p>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Phase 1: DRY Unification</li> <li><code>_execute_cycle()</code> as Single Source of Truth</li> <li><code>run()</code> is now a pure sync wrapper (no duplicated logic)</li> <li> <p>Zero code duplication between sync/async paths</p> </li> <li> <p>Phase 4: CPU Offloading</p> </li> <li>Egress filtering runs in <code>ThreadPoolExecutor</code></li> <li><code>run_in_executor</code> for non-blocking Shannon entropy calculation</li> <li> <p>Large outputs no longer block the event loop</p> </li> <li> <p>Phase 5: Context Safety</p> </li> <li>Binary file detection on <code>ContextHandle</code> initialization</li> <li>Null bytes and control character detection</li> <li>Prevents LLM hallucination on garbage input</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Phase 2: Strict Parsing</li> <li>Direct <code>mistletoe</code> import (fail-fast if missing)</li> <li>Removed all regex fallback code</li> <li> <p>Pure AST parsing for deterministic behavior</p> </li> <li> <p>Phase 3: Resource Safety</p> </li> <li><code>TemporaryDirectory</code> replaces <code>NamedTemporaryFile</code></li> <li>Auto-cleanup even on SIGKILL</li> <li>No temp file leaks possible</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li><code>HAS_MISTLETOE</code> conditional import</li> <li><code>extract_code_blocks_regex()</code> fallback function</li> <li>Manual temp file cleanup in <code>finally</code> blocks</li> </ul>"},{"location":"changelog/#210-2026-01-21","title":"[2.1.0] - 2026-01-21","text":""},{"location":"changelog/#async-hardening-update","title":"Async &amp; Hardening Update","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Async Foundation</li> <li><code>aiodocker</code> for true async Docker operations</li> <li><code>AsyncDockerSandbox</code> class</li> <li><code>Orchestrator.arun()</code> for async agent loops</li> <li> <p><code>acomplete()</code> and <code>astream()</code> in LLM clients</p> </li> <li> <p>Robust Parsing</p> </li> <li>State machine parser (<code>parsing.py</code>)</li> <li><code>mistletoe</code> integration for edge cases</li> <li> <p><code>extract_python_code()</code> for reliable extraction</p> </li> <li> <p>Clean Boot Architecture</p> </li> <li><code>agent_lib/</code> module for in-container code</li> <li>Volume mounting instead of string injection</li> <li> <p><code>boot.py</code> clean entry point</p> </li> <li> <p>Security Hardening</p> </li> <li>Binary output detection (magic bytes)</li> <li>Entropy allowlist for hashes/UUIDs</li> <li>Fail-closed: gVisor required by default</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Docker sandbox uses volume mounting for agent code</li> <li>Orchestrator uses robust parsing instead of basic regex</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li><code>ImportBlocker</code> class (Python-level false security)</li> <li>String-based code injection</li> </ul>"},{"location":"changelog/#200-2026-01-21","title":"[2.0.0] - 2026-01-21","text":""},{"location":"changelog/#initial-release","title":"Initial Release \ud83d\ude80","text":"<ul> <li>Docker sandbox with gVisor support</li> <li>Network isolation (<code>network_mode=\"none\"</code>)</li> <li>Resource limits (memory, CPU, PIDs)</li> <li>Egress filtering (entropy, patterns, context echo)</li> <li>ContextHandle for memory-efficient file access</li> <li>Multi-provider LLM clients (OpenAI, Anthropic, Google)</li> <li>Budget management and cost tracking</li> <li>Comprehensive test suite</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Guidelines for contributing to RLM-Python.</p> <p>Security First</p> <p>Security improvements are our highest priority. If you discover a vulnerability, please report it privately.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Docker Engine</li> <li>gVisor (optional, for security tests)</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/1thirteeng3/PyRlm.git\ncd PyRlm\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n\n# Install in development mode\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Unit tests\npytest tests/unit/ -v\n\n# Integration tests (requires Docker)\npytest tests/integration/ -v\n\n# All tests with coverage\npytest --cov=rlm --cov-report=html\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use <code>ruff</code> for linting and formatting:</p> <pre><code>ruff format .\nruff check . --fix\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork the repository</li> <li>Create a branch: <code>git checkout -b feature/amazing-feature</code></li> <li>Write tests for new functionality</li> <li>Run the test suite</li> <li>Commit: <code>git commit -m \"feat(scope): description\"</code></li> <li>Push and open a Pull Request</li> </ol>"},{"location":"contributing/#commit-format","title":"Commit Format","text":"<pre><code>type(scope): description\n\nTypes: feat, fix, docs, style, refactor, test, chore\n</code></pre>"},{"location":"contributing/#v30-architecture-principles","title":"v3.0 Architecture Principles","text":"<ol> <li>DRY: All logic in <code>_execute_cycle()</code>, no duplication</li> <li>Fail-Fast: Direct imports, no conditional fallbacks</li> <li>Async-First: Use <code>async/await</code> for all I/O operations</li> <li>Defense in Depth: Security at every layer</li> </ol> <p>Thank you for contributing! \ud83d\ude4f</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for RLM-Python.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":"Class Description <code>Orchestrator</code> Main LLM agent loop <code>DockerSandbox</code> Secure code execution <code>ContextHandle</code> Memory-efficient file access <code>EgressFilter</code> Output sanitization"},{"location":"api/#llm-clients","title":"LLM Clients","text":"Class Description <code>BaseLLMClient</code> Abstract base class <code>OpenAIClient</code> OpenAI GPT models <code>AnthropicClient</code> Anthropic Claude <code>GoogleClient</code> Google Gemini"},{"location":"api/#exceptions","title":"Exceptions","text":"Exception Description <code>RLMError</code> Base exception <code>SecurityViolationError</code> Security boundary violation <code>DataLeakageError</code> Egress filter triggered <code>BudgetExceededError</code> Cost limit reached <code>SandboxError</code> Container execution failure"},{"location":"api/#utilities","title":"Utilities","text":"Class/Function Description <code>BudgetManager</code> Cost tracking <code>RLMSettings</code> Configuration"},{"location":"api/#quick-example","title":"Quick Example","text":"<pre><code>from rlm import Orchestrator, DockerSandbox, ContextHandle\n\n# Full orchestration\norchestrator = Orchestrator()\nresult = orchestrator.run(\"Calculate fibonacci(20)\")\nprint(result.final_answer)\n\n# Direct sandbox\nsandbox = DockerSandbox()\nexec_result = sandbox.execute(\"print(sum(range(100)))\")\nprint(exec_result.stdout)\n\n# Large file handling\nwith ContextHandle(\"/path/to/file.txt\") as ctx:\n    matches = ctx.search(r\"pattern\")\n</code></pre>"},{"location":"api/context-handle/","title":"ContextHandle API","text":""},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle","title":"rlm.core.memory.handle.ContextHandle","text":"<p>Memory-efficient handle for accessing large context files.</p> <p>v3.0: Detects binary files to prevent LLM hallucination. Raises ContextError if you try to load a PDF, image, or compiled binary.</p> <p>The API is designed to encourage efficient patterns: - search() to find relevant sections - read_window() to read specific chunks - iterate_lines() for streaming access</p> Example <p>ctx = ContextHandle(\"/path/to/large_file.txt\") print(f\"File size: {ctx.size} bytes\") matches = ctx.search(r\"important.*pattern\") for offset, match in matches: ...     print(ctx.snippet(offset))</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>class ContextHandle:\n    \"\"\"\n    Memory-efficient handle for accessing large context files.\n\n    v3.0: Detects binary files to prevent LLM hallucination.\n    Raises ContextError if you try to load a PDF, image, or compiled binary.\n\n    The API is designed to encourage efficient patterns:\n    - search() to find relevant sections\n    - read_window() to read specific chunks\n    - iterate_lines() for streaming access\n\n    Example:\n        &gt;&gt;&gt; ctx = ContextHandle(\"/path/to/large_file.txt\")\n        &gt;&gt;&gt; print(f\"File size: {ctx.size} bytes\")\n        &gt;&gt;&gt; matches = ctx.search(r\"important.*pattern\")\n        &gt;&gt;&gt; for offset, match in matches:\n        ...     print(ctx.snippet(offset))\n    \"\"\"\n\n    DEFAULT_WINDOW_SIZE = 500\n    MAX_SEARCH_RESULTS = 10\n\n    def __init__(self, path: str | Path) -&gt; None:\n        \"\"\"\n        Initialize the context handle.\n\n        Args:\n            path: Path to the context file\n\n        Raises:\n            ContextError: If the file doesn't exist, is binary, or can't be accessed\n        \"\"\"\n        self.path = Path(path)\n\n        if not self.path.exists():\n            raise ContextError(\n                message=f\"Context file not found: {path}\",\n                path=str(path),\n            )\n\n        if not self.path.is_file():\n            raise ContextError(\n                message=f\"Context path is not a file: {path}\",\n                path=str(path),\n            )\n\n        try:\n            self._size = self.path.stat().st_size\n        except OSError as e:\n            raise ContextError(\n                message=f\"Cannot access context file: {e}\",\n                path=str(path),\n            ) from e\n\n        self._mmap: Optional[mmap.mmap] = None\n        self._file = None\n\n        # v3.0: Validate text content on init\n        self._validate_not_binary()\n\n    def _validate_not_binary(self) -&gt; None:\n        \"\"\"\n        Validate that the file is text, not binary.\n\n        v3.0: Prevents loading PDFs, images, or binaries as context.\n        Checks first 8KB for null bytes and control characters.\n\n        Raises:\n            ContextError: If binary content is detected\n        \"\"\"\n        sample_size = min(8192, self._size)\n        if sample_size == 0:\n            return  # Empty file is OK\n\n        with open(self.path, \"rb\") as f:\n            sample = f.read(sample_size)\n\n        # Check for null bytes (definitive binary indicator)\n        if b'\\x00' in sample:\n            raise ContextError(\n                message=\"Binary file detected via null bytes. ContextHandle only supports text files.\",\n                path=str(self.path),\n                details={\"hint\": \"Cannot load PDFs, images, or compiled binaries as context.\"},\n            )\n\n        # Check for excessive control characters\n        control_count = sum(1 for b in sample if b in CONTROL_CHARS)\n        ratio = control_count / len(sample)\n\n        if ratio &gt; BINARY_THRESHOLD:\n            raise ContextError(\n                message=f\"Binary file detected ({ratio:.1%} control characters). ContextHandle only supports text.\",\n                path=str(self.path),\n                details={\"control_char_ratio\": ratio, \"threshold\": BINARY_THRESHOLD},\n            )\n\n    def _validate_text_content(self, chunk: str) -&gt; None:\n        \"\"\"\n        Runtime validation of text content.\n\n        v3.0: Additional check during reads to catch late-detected binary.\n\n        Raises:\n            ContextError: If binary content is found\n        \"\"\"\n        if '\\x00' in chunk:\n            raise ContextError(\n                message=\"Binary content detected via null bytes during read.\",\n                path=str(self.path),\n            )\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Return the total size of the context file in bytes.\"\"\"\n        return self._size\n\n    @property\n    def size_mb(self) -&gt; float:\n        \"\"\"Return the size in megabytes.\"\"\"\n        return self._size / (1024 * 1024)\n\n    def _get_mmap(self) -&gt; mmap.mmap:\n        \"\"\"Get or create the memory-mapped file.\"\"\"\n        if self._mmap is None:\n            try:\n                self._file = open(self.path, \"r+b\")\n                self._mmap = mmap.mmap(\n                    self._file.fileno(),\n                    0,\n                    access=mmap.ACCESS_READ,\n                )\n            except Exception as e:\n                if self._file:\n                    self._file.close()\n                raise ContextError(\n                    message=f\"Failed to memory-map context file: {e}\",\n                    path=str(self.path),\n                ) from e\n        return self._mmap\n\n    def read(self, start: int, length: int) -&gt; str:\n        \"\"\"\n        Read a specific chunk of the file.\n\n        v3.0: Validates content is text, not binary.\n\n        Args:\n            start: Starting byte offset\n            length: Number of bytes to read\n\n        Returns:\n            The decoded text content\n\n        Raises:\n            ContextError: If binary content is detected\n        \"\"\"\n        if start &lt; 0:\n            start = 0\n        if start &gt;= self._size:\n            return \"\"\n\n        # Clamp length to file size\n        end = min(start + length, self._size)\n        actual_length = end - start\n\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            f.seek(start)\n            content = f.read(actual_length)\n\n        # v3.0: Validate no binary garbage\n        self._validate_text_content(content)\n\n        return content\n\n    def read_window(self, offset: int, radius: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n        \"\"\"\n        Read a window of text centered around an offset.\n\n        Args:\n            offset: Center point (byte offset)\n            radius: Number of bytes on each side of the offset\n\n        Returns:\n            The text content in the window\n        \"\"\"\n        start = max(0, offset - radius)\n        length = radius * 2\n        return self.read(start, length)\n\n    def snippet(self, offset: int, window: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n        \"\"\"\n        Get a snippet of text around an offset.\n\n        This is an alias for read_window with different semantics -\n        window is the total size, not the radius.\n\n        Args:\n            offset: Center point (byte offset)\n            window: Total window size in bytes\n\n        Returns:\n            The text snippet\n        \"\"\"\n        return self.read_window(offset, window // 2)\n\n    def search(\n        self,\n        pattern: str,\n        max_results: int = MAX_SEARCH_RESULTS,\n        ignore_case: bool = True,\n    ) -&gt; list[tuple[int, str]]:\n        \"\"\"\n        Search for a regex pattern in the file using memory mapping.\n\n        This is memory-efficient as it doesn't load the entire file.\n\n        Args:\n            pattern: Regular expression pattern\n            max_results: Maximum number of results to return\n            ignore_case: Whether to ignore case\n\n        Returns:\n            List of (byte_offset, matched_text) tuples\n        \"\"\"\n        matches: list[tuple[int, str]] = []\n\n        try:\n            mm = self._get_mmap()\n\n            # Compile pattern for bytes\n            flags = re.IGNORECASE if ignore_case else 0\n            try:\n                bytes_pattern = pattern.encode(\"utf-8\")\n                compiled = re.compile(bytes_pattern, flags)\n            except re.error as e:\n                raise ContextError(\n                    message=f\"Invalid regex pattern: {e}\",\n                    details={\"pattern\": pattern},\n                ) from e\n\n            for match in compiled.finditer(mm):\n                if len(matches) &gt;= max_results:\n                    break\n\n                try:\n                    match_text = match.group().decode(\"utf-8\", errors=\"replace\")\n                    matches.append((match.start(), match_text))\n                except UnicodeDecodeError:\n                    # Skip matches that can't be decoded\n                    continue\n\n        except Exception as e:\n            if not isinstance(e, ContextError):\n                raise ContextError(\n                    message=f\"Search failed: {e}\",\n                    path=str(self.path),\n                ) from e\n            raise\n\n        return matches\n\n    def search_lines(\n        self,\n        pattern: str,\n        max_results: int = MAX_SEARCH_RESULTS,\n        context_lines: int = 0,\n    ) -&gt; list[tuple[int, str, str]]:\n        \"\"\"\n        Search for a pattern and return matching lines.\n\n        Args:\n            pattern: Regex pattern to search for\n            max_results: Maximum number of results\n            context_lines: Number of lines before/after to include\n\n        Returns:\n            List of (line_number, matching_line, context) tuples\n        \"\"\"\n        matches: list[tuple[int, str, str]] = []\n        compiled = re.compile(pattern, re.IGNORECASE)\n\n        lines_buffer: list[str] = []\n\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            for line_no, line in enumerate(f, start=1):\n                lines_buffer.append(line)\n                if len(lines_buffer) &gt; context_lines * 2 + 1:\n                    lines_buffer.pop(0)\n\n                if compiled.search(line):\n                    context = \"\".join(lines_buffer)\n                    matches.append((line_no, line.strip(), context))\n\n                    if len(matches) &gt;= max_results:\n                        break\n\n        return matches\n\n    def iterate_lines(self, start_line: int = 1) -&gt; Iterator[tuple[int, str]]:\n        \"\"\"\n        Iterate over lines in the file.\n\n        This is memory-efficient as it reads line by line.\n\n        Args:\n            start_line: Line number to start from (1-indexed)\n\n        Yields:\n            Tuples of (line_number, line_content)\n        \"\"\"\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            for line_no, line in enumerate(f, start=1):\n                if line_no &gt;= start_line:\n                    yield line_no, line\n\n    def head(self, n_bytes: int = 1000) -&gt; str:\n        \"\"\"Read the first n bytes of the file.\"\"\"\n        return self.read(0, n_bytes)\n\n    def tail(self, n_bytes: int = 1000) -&gt; str:\n        \"\"\"Read the last n bytes of the file.\"\"\"\n        start = max(0, self._size - n_bytes)\n        return self.read(start, n_bytes)\n\n    def close(self) -&gt; None:\n        \"\"\"Close the memory-mapped file.\"\"\"\n        # v3.0: Safe attribute access (handle partial init on validation failure)\n        if getattr(self, '_mmap', None):\n            self._mmap.close()\n            self._mmap = None\n        if getattr(self, '_file', None):\n            self._file.close()\n            self._file = None\n\n    def __enter__(self) -&gt; \"ContextHandle\":\n        return self\n\n    def __exit__(self, *args) -&gt; None:\n        self.close()\n\n    def __repr__(self) -&gt; str:\n        return f\"ContextHandle(path='{self.path}', size={self.size_mb:.2f}MB)\"\n\n    def __del__(self) -&gt; None:\n        self.close()\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.size","title":"size  <code>property</code>","text":"<pre><code>size: int\n</code></pre> <p>Return the total size of the context file in bytes.</p>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.size_mb","title":"size_mb  <code>property</code>","text":"<pre><code>size_mb: float\n</code></pre> <p>Return the size in megabytes.</p>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.read","title":"read","text":"<pre><code>read(start: int, length: int) -&gt; str\n</code></pre> <p>Read a specific chunk of the file.</p> <p>v3.0: Validates content is text, not binary.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Starting byte offset</p> required <code>length</code> <code>int</code> <p>Number of bytes to read</p> required <p>Returns:</p> Type Description <code>str</code> <p>The decoded text content</p> <p>Raises:</p> Type Description <code>ContextError</code> <p>If binary content is detected</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def read(self, start: int, length: int) -&gt; str:\n    \"\"\"\n    Read a specific chunk of the file.\n\n    v3.0: Validates content is text, not binary.\n\n    Args:\n        start: Starting byte offset\n        length: Number of bytes to read\n\n    Returns:\n        The decoded text content\n\n    Raises:\n        ContextError: If binary content is detected\n    \"\"\"\n    if start &lt; 0:\n        start = 0\n    if start &gt;= self._size:\n        return \"\"\n\n    # Clamp length to file size\n    end = min(start + length, self._size)\n    actual_length = end - start\n\n    with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n        f.seek(start)\n        content = f.read(actual_length)\n\n    # v3.0: Validate no binary garbage\n    self._validate_text_content(content)\n\n    return content\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.read_window","title":"read_window","text":"<pre><code>read_window(offset: int, radius: int = DEFAULT_WINDOW_SIZE) -&gt; str\n</code></pre> <p>Read a window of text centered around an offset.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Center point (byte offset)</p> required <code>radius</code> <code>int</code> <p>Number of bytes on each side of the offset</p> <code>DEFAULT_WINDOW_SIZE</code> <p>Returns:</p> Type Description <code>str</code> <p>The text content in the window</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def read_window(self, offset: int, radius: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n    \"\"\"\n    Read a window of text centered around an offset.\n\n    Args:\n        offset: Center point (byte offset)\n        radius: Number of bytes on each side of the offset\n\n    Returns:\n        The text content in the window\n    \"\"\"\n    start = max(0, offset - radius)\n    length = radius * 2\n    return self.read(start, length)\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.snippet","title":"snippet","text":"<pre><code>snippet(offset: int, window: int = DEFAULT_WINDOW_SIZE) -&gt; str\n</code></pre> <p>Get a snippet of text around an offset.</p> <p>This is an alias for read_window with different semantics - window is the total size, not the radius.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Center point (byte offset)</p> required <code>window</code> <code>int</code> <p>Total window size in bytes</p> <code>DEFAULT_WINDOW_SIZE</code> <p>Returns:</p> Type Description <code>str</code> <p>The text snippet</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def snippet(self, offset: int, window: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n    \"\"\"\n    Get a snippet of text around an offset.\n\n    This is an alias for read_window with different semantics -\n    window is the total size, not the radius.\n\n    Args:\n        offset: Center point (byte offset)\n        window: Total window size in bytes\n\n    Returns:\n        The text snippet\n    \"\"\"\n    return self.read_window(offset, window // 2)\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.head","title":"head","text":"<pre><code>head(n_bytes: int = 1000) -&gt; str\n</code></pre> <p>Read the first n bytes of the file.</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def head(self, n_bytes: int = 1000) -&gt; str:\n    \"\"\"Read the first n bytes of the file.\"\"\"\n    return self.read(0, n_bytes)\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.tail","title":"tail","text":"<pre><code>tail(n_bytes: int = 1000) -&gt; str\n</code></pre> <p>Read the last n bytes of the file.</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def tail(self, n_bytes: int = 1000) -&gt; str:\n    \"\"\"Read the last n bytes of the file.\"\"\"\n    start = max(0, self._size - n_bytes)\n    return self.read(start, n_bytes)\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.search","title":"search","text":"<pre><code>search(pattern: str, max_results: int = MAX_SEARCH_RESULTS, ignore_case: bool = True) -&gt; list[tuple[int, str]]\n</code></pre> <p>Search for a regex pattern in the file using memory mapping.</p> <p>This is memory-efficient as it doesn't load the entire file.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression pattern</p> required <code>max_results</code> <code>int</code> <p>Maximum number of results to return</p> <code>MAX_SEARCH_RESULTS</code> <code>ignore_case</code> <code>bool</code> <p>Whether to ignore case</p> <code>True</code> <p>Returns:</p> Type Description <code>list[tuple[int, str]]</code> <p>List of (byte_offset, matched_text) tuples</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def search(\n    self,\n    pattern: str,\n    max_results: int = MAX_SEARCH_RESULTS,\n    ignore_case: bool = True,\n) -&gt; list[tuple[int, str]]:\n    \"\"\"\n    Search for a regex pattern in the file using memory mapping.\n\n    This is memory-efficient as it doesn't load the entire file.\n\n    Args:\n        pattern: Regular expression pattern\n        max_results: Maximum number of results to return\n        ignore_case: Whether to ignore case\n\n    Returns:\n        List of (byte_offset, matched_text) tuples\n    \"\"\"\n    matches: list[tuple[int, str]] = []\n\n    try:\n        mm = self._get_mmap()\n\n        # Compile pattern for bytes\n        flags = re.IGNORECASE if ignore_case else 0\n        try:\n            bytes_pattern = pattern.encode(\"utf-8\")\n            compiled = re.compile(bytes_pattern, flags)\n        except re.error as e:\n            raise ContextError(\n                message=f\"Invalid regex pattern: {e}\",\n                details={\"pattern\": pattern},\n            ) from e\n\n        for match in compiled.finditer(mm):\n            if len(matches) &gt;= max_results:\n                break\n\n            try:\n                match_text = match.group().decode(\"utf-8\", errors=\"replace\")\n                matches.append((match.start(), match_text))\n            except UnicodeDecodeError:\n                # Skip matches that can't be decoded\n                continue\n\n    except Exception as e:\n        if not isinstance(e, ContextError):\n            raise ContextError(\n                message=f\"Search failed: {e}\",\n                path=str(self.path),\n            ) from e\n        raise\n\n    return matches\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.search_lines","title":"search_lines","text":"<pre><code>search_lines(pattern: str, max_results: int = MAX_SEARCH_RESULTS, context_lines: int = 0) -&gt; list[tuple[int, str, str]]\n</code></pre> <p>Search for a pattern and return matching lines.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regex pattern to search for</p> required <code>max_results</code> <code>int</code> <p>Maximum number of results</p> <code>MAX_SEARCH_RESULTS</code> <code>context_lines</code> <code>int</code> <p>Number of lines before/after to include</p> <code>0</code> <p>Returns:</p> Type Description <code>list[tuple[int, str, str]]</code> <p>List of (line_number, matching_line, context) tuples</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def search_lines(\n    self,\n    pattern: str,\n    max_results: int = MAX_SEARCH_RESULTS,\n    context_lines: int = 0,\n) -&gt; list[tuple[int, str, str]]:\n    \"\"\"\n    Search for a pattern and return matching lines.\n\n    Args:\n        pattern: Regex pattern to search for\n        max_results: Maximum number of results\n        context_lines: Number of lines before/after to include\n\n    Returns:\n        List of (line_number, matching_line, context) tuples\n    \"\"\"\n    matches: list[tuple[int, str, str]] = []\n    compiled = re.compile(pattern, re.IGNORECASE)\n\n    lines_buffer: list[str] = []\n\n    with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n        for line_no, line in enumerate(f, start=1):\n            lines_buffer.append(line)\n            if len(lines_buffer) &gt; context_lines * 2 + 1:\n                lines_buffer.pop(0)\n\n            if compiled.search(line):\n                context = \"\".join(lines_buffer)\n                matches.append((line_no, line.strip(), context))\n\n                if len(matches) &gt;= max_results:\n                    break\n\n    return matches\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.iterate_lines","title":"iterate_lines","text":"<pre><code>iterate_lines(start_line: int = 1) -&gt; Iterator[tuple[int, str]]\n</code></pre> <p>Iterate over lines in the file.</p> <p>This is memory-efficient as it reads line by line.</p> <p>Parameters:</p> Name Type Description Default <code>start_line</code> <code>int</code> <p>Line number to start from (1-indexed)</p> <code>1</code> <p>Yields:</p> Type Description <code>tuple[int, str]</code> <p>Tuples of (line_number, line_content)</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def iterate_lines(self, start_line: int = 1) -&gt; Iterator[tuple[int, str]]:\n    \"\"\"\n    Iterate over lines in the file.\n\n    This is memory-efficient as it reads line by line.\n\n    Args:\n        start_line: Line number to start from (1-indexed)\n\n    Yields:\n        Tuples of (line_number, line_content)\n    \"\"\"\n    with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n        for line_no, line in enumerate(f, start=1):\n            if line_no &gt;= start_line:\n                yield line_no, line\n</code></pre>"},{"location":"api/context-handle/#rlm.core.memory.handle.ContextHandle.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the memory-mapped file.</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the memory-mapped file.\"\"\"\n    # v3.0: Safe attribute access (handle partial init on validation failure)\n    if getattr(self, '_mmap', None):\n        self._mmap.close()\n        self._mmap = None\n    if getattr(self, '_file', None):\n        self._file.close()\n        self._file = None\n</code></pre>"},{"location":"api/context/","title":"Context Handle","text":""},{"location":"api/context/#rlm.core.memory.handle.ContextHandle","title":"ContextHandle","text":"<p>Memory-efficient handle for accessing large context files.</p> <p>v3.0: Detects binary files to prevent LLM hallucination. Raises ContextError if you try to load a PDF, image, or compiled binary.</p> <p>The API is designed to encourage efficient patterns: - search() to find relevant sections - read_window() to read specific chunks - iterate_lines() for streaming access</p> Example <p>ctx = ContextHandle(\"/path/to/large_file.txt\") print(f\"File size: {ctx.size} bytes\") matches = ctx.search(r\"important.*pattern\") for offset, match in matches: ...     print(ctx.snippet(offset))</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>class ContextHandle:\n    \"\"\"\n    Memory-efficient handle for accessing large context files.\n\n    v3.0: Detects binary files to prevent LLM hallucination.\n    Raises ContextError if you try to load a PDF, image, or compiled binary.\n\n    The API is designed to encourage efficient patterns:\n    - search() to find relevant sections\n    - read_window() to read specific chunks\n    - iterate_lines() for streaming access\n\n    Example:\n        &gt;&gt;&gt; ctx = ContextHandle(\"/path/to/large_file.txt\")\n        &gt;&gt;&gt; print(f\"File size: {ctx.size} bytes\")\n        &gt;&gt;&gt; matches = ctx.search(r\"important.*pattern\")\n        &gt;&gt;&gt; for offset, match in matches:\n        ...     print(ctx.snippet(offset))\n    \"\"\"\n\n    DEFAULT_WINDOW_SIZE = 500\n    MAX_SEARCH_RESULTS = 10\n\n    def __init__(self, path: str | Path) -&gt; None:\n        \"\"\"\n        Initialize the context handle.\n\n        Args:\n            path: Path to the context file\n\n        Raises:\n            ContextError: If the file doesn't exist, is binary, or can't be accessed\n        \"\"\"\n        self.path = Path(path)\n\n        if not self.path.exists():\n            raise ContextError(\n                message=f\"Context file not found: {path}\",\n                path=str(path),\n            )\n\n        if not self.path.is_file():\n            raise ContextError(\n                message=f\"Context path is not a file: {path}\",\n                path=str(path),\n            )\n\n        try:\n            self._size = self.path.stat().st_size\n        except OSError as e:\n            raise ContextError(\n                message=f\"Cannot access context file: {e}\",\n                path=str(path),\n            ) from e\n\n        self._mmap: Optional[mmap.mmap] = None\n        self._file = None\n\n        # v3.0: Validate text content on init\n        self._validate_not_binary()\n\n    def _validate_not_binary(self) -&gt; None:\n        \"\"\"\n        Validate that the file is text, not binary.\n\n        v3.0: Prevents loading PDFs, images, or binaries as context.\n        Checks first 8KB for null bytes and control characters.\n\n        Raises:\n            ContextError: If binary content is detected\n        \"\"\"\n        sample_size = min(8192, self._size)\n        if sample_size == 0:\n            return  # Empty file is OK\n\n        with open(self.path, \"rb\") as f:\n            sample = f.read(sample_size)\n\n        # Check for null bytes (definitive binary indicator)\n        if b'\\x00' in sample:\n            raise ContextError(\n                message=\"Binary file detected via null bytes. ContextHandle only supports text files.\",\n                path=str(self.path),\n                details={\"hint\": \"Cannot load PDFs, images, or compiled binaries as context.\"},\n            )\n\n        # Check for excessive control characters\n        control_count = sum(1 for b in sample if b in CONTROL_CHARS)\n        ratio = control_count / len(sample)\n\n        if ratio &gt; BINARY_THRESHOLD:\n            raise ContextError(\n                message=f\"Binary file detected ({ratio:.1%} control characters). ContextHandle only supports text.\",\n                path=str(self.path),\n                details={\"control_char_ratio\": ratio, \"threshold\": BINARY_THRESHOLD},\n            )\n\n    def _validate_text_content(self, chunk: str) -&gt; None:\n        \"\"\"\n        Runtime validation of text content.\n\n        v3.0: Additional check during reads to catch late-detected binary.\n\n        Raises:\n            ContextError: If binary content is found\n        \"\"\"\n        if '\\x00' in chunk:\n            raise ContextError(\n                message=\"Binary content detected via null bytes during read.\",\n                path=str(self.path),\n            )\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Return the total size of the context file in bytes.\"\"\"\n        return self._size\n\n    @property\n    def size_mb(self) -&gt; float:\n        \"\"\"Return the size in megabytes.\"\"\"\n        return self._size / (1024 * 1024)\n\n    def _get_mmap(self) -&gt; mmap.mmap:\n        \"\"\"Get or create the memory-mapped file.\"\"\"\n        if self._mmap is None:\n            try:\n                self._file = open(self.path, \"r+b\")\n                self._mmap = mmap.mmap(\n                    self._file.fileno(),\n                    0,\n                    access=mmap.ACCESS_READ,\n                )\n            except Exception as e:\n                if self._file:\n                    self._file.close()\n                raise ContextError(\n                    message=f\"Failed to memory-map context file: {e}\",\n                    path=str(self.path),\n                ) from e\n        return self._mmap\n\n    def read(self, start: int, length: int) -&gt; str:\n        \"\"\"\n        Read a specific chunk of the file.\n\n        v3.0: Validates content is text, not binary.\n\n        Args:\n            start: Starting byte offset\n            length: Number of bytes to read\n\n        Returns:\n            The decoded text content\n\n        Raises:\n            ContextError: If binary content is detected\n        \"\"\"\n        if start &lt; 0:\n            start = 0\n        if start &gt;= self._size:\n            return \"\"\n\n        # Clamp length to file size\n        end = min(start + length, self._size)\n        actual_length = end - start\n\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            f.seek(start)\n            content = f.read(actual_length)\n\n        # v3.0: Validate no binary garbage\n        self._validate_text_content(content)\n\n        return content\n\n    def read_window(self, offset: int, radius: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n        \"\"\"\n        Read a window of text centered around an offset.\n\n        Args:\n            offset: Center point (byte offset)\n            radius: Number of bytes on each side of the offset\n\n        Returns:\n            The text content in the window\n        \"\"\"\n        start = max(0, offset - radius)\n        length = radius * 2\n        return self.read(start, length)\n\n    def snippet(self, offset: int, window: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n        \"\"\"\n        Get a snippet of text around an offset.\n\n        This is an alias for read_window with different semantics -\n        window is the total size, not the radius.\n\n        Args:\n            offset: Center point (byte offset)\n            window: Total window size in bytes\n\n        Returns:\n            The text snippet\n        \"\"\"\n        return self.read_window(offset, window // 2)\n\n    def search(\n        self,\n        pattern: str,\n        max_results: int = MAX_SEARCH_RESULTS,\n        ignore_case: bool = True,\n    ) -&gt; list[tuple[int, str]]:\n        \"\"\"\n        Search for a regex pattern in the file using memory mapping.\n\n        This is memory-efficient as it doesn't load the entire file.\n\n        Args:\n            pattern: Regular expression pattern\n            max_results: Maximum number of results to return\n            ignore_case: Whether to ignore case\n\n        Returns:\n            List of (byte_offset, matched_text) tuples\n        \"\"\"\n        matches: list[tuple[int, str]] = []\n\n        try:\n            mm = self._get_mmap()\n\n            # Compile pattern for bytes\n            flags = re.IGNORECASE if ignore_case else 0\n            try:\n                bytes_pattern = pattern.encode(\"utf-8\")\n                compiled = re.compile(bytes_pattern, flags)\n            except re.error as e:\n                raise ContextError(\n                    message=f\"Invalid regex pattern: {e}\",\n                    details={\"pattern\": pattern},\n                ) from e\n\n            for match in compiled.finditer(mm):\n                if len(matches) &gt;= max_results:\n                    break\n\n                try:\n                    match_text = match.group().decode(\"utf-8\", errors=\"replace\")\n                    matches.append((match.start(), match_text))\n                except UnicodeDecodeError:\n                    # Skip matches that can't be decoded\n                    continue\n\n        except Exception as e:\n            if not isinstance(e, ContextError):\n                raise ContextError(\n                    message=f\"Search failed: {e}\",\n                    path=str(self.path),\n                ) from e\n            raise\n\n        return matches\n\n    def search_lines(\n        self,\n        pattern: str,\n        max_results: int = MAX_SEARCH_RESULTS,\n        context_lines: int = 0,\n    ) -&gt; list[tuple[int, str, str]]:\n        \"\"\"\n        Search for a pattern and return matching lines.\n\n        Args:\n            pattern: Regex pattern to search for\n            max_results: Maximum number of results\n            context_lines: Number of lines before/after to include\n\n        Returns:\n            List of (line_number, matching_line, context) tuples\n        \"\"\"\n        matches: list[tuple[int, str, str]] = []\n        compiled = re.compile(pattern, re.IGNORECASE)\n\n        lines_buffer: list[str] = []\n\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            for line_no, line in enumerate(f, start=1):\n                lines_buffer.append(line)\n                if len(lines_buffer) &gt; context_lines * 2 + 1:\n                    lines_buffer.pop(0)\n\n                if compiled.search(line):\n                    context = \"\".join(lines_buffer)\n                    matches.append((line_no, line.strip(), context))\n\n                    if len(matches) &gt;= max_results:\n                        break\n\n        return matches\n\n    def iterate_lines(self, start_line: int = 1) -&gt; Iterator[tuple[int, str]]:\n        \"\"\"\n        Iterate over lines in the file.\n\n        This is memory-efficient as it reads line by line.\n\n        Args:\n            start_line: Line number to start from (1-indexed)\n\n        Yields:\n            Tuples of (line_number, line_content)\n        \"\"\"\n        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            for line_no, line in enumerate(f, start=1):\n                if line_no &gt;= start_line:\n                    yield line_no, line\n\n    def head(self, n_bytes: int = 1000) -&gt; str:\n        \"\"\"Read the first n bytes of the file.\"\"\"\n        return self.read(0, n_bytes)\n\n    def tail(self, n_bytes: int = 1000) -&gt; str:\n        \"\"\"Read the last n bytes of the file.\"\"\"\n        start = max(0, self._size - n_bytes)\n        return self.read(start, n_bytes)\n\n    def close(self) -&gt; None:\n        \"\"\"Close the memory-mapped file.\"\"\"\n        # v3.0: Safe attribute access (handle partial init on validation failure)\n        if getattr(self, '_mmap', None):\n            self._mmap.close()\n            self._mmap = None\n        if getattr(self, '_file', None):\n            self._file.close()\n            self._file = None\n\n    def __enter__(self) -&gt; \"ContextHandle\":\n        return self\n\n    def __exit__(self, *args) -&gt; None:\n        self.close()\n\n    def __repr__(self) -&gt; str:\n        return f\"ContextHandle(path='{self.path}', size={self.size_mb:.2f}MB)\"\n\n    def __del__(self) -&gt; None:\n        self.close()\n</code></pre>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.size","title":"size  <code>property</code>","text":"<pre><code>size: int\n</code></pre> <p>Return the total size of the context file in bytes.</p>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.size_mb","title":"size_mb  <code>property</code>","text":"<pre><code>size_mb: float\n</code></pre> <p>Return the size in megabytes.</p>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.__init__","title":"__init__","text":"<pre><code>__init__(path: str | Path) -&gt; None\n</code></pre> <p>Initialize the context handle.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the context file</p> required <p>Raises:</p> Type Description <code>ContextError</code> <p>If the file doesn't exist, is binary, or can't be accessed</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def __init__(self, path: str | Path) -&gt; None:\n    \"\"\"\n    Initialize the context handle.\n\n    Args:\n        path: Path to the context file\n\n    Raises:\n        ContextError: If the file doesn't exist, is binary, or can't be accessed\n    \"\"\"\n    self.path = Path(path)\n\n    if not self.path.exists():\n        raise ContextError(\n            message=f\"Context file not found: {path}\",\n            path=str(path),\n        )\n\n    if not self.path.is_file():\n        raise ContextError(\n            message=f\"Context path is not a file: {path}\",\n            path=str(path),\n        )\n\n    try:\n        self._size = self.path.stat().st_size\n    except OSError as e:\n        raise ContextError(\n            message=f\"Cannot access context file: {e}\",\n            path=str(path),\n        ) from e\n\n    self._mmap: Optional[mmap.mmap] = None\n    self._file = None\n\n    # v3.0: Validate text content on init\n    self._validate_not_binary()\n</code></pre>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.read","title":"read","text":"<pre><code>read(start: int, length: int) -&gt; str\n</code></pre> <p>Read a specific chunk of the file.</p> <p>v3.0: Validates content is text, not binary.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Starting byte offset</p> required <code>length</code> <code>int</code> <p>Number of bytes to read</p> required <p>Returns:</p> Type Description <code>str</code> <p>The decoded text content</p> <p>Raises:</p> Type Description <code>ContextError</code> <p>If binary content is detected</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def read(self, start: int, length: int) -&gt; str:\n    \"\"\"\n    Read a specific chunk of the file.\n\n    v3.0: Validates content is text, not binary.\n\n    Args:\n        start: Starting byte offset\n        length: Number of bytes to read\n\n    Returns:\n        The decoded text content\n\n    Raises:\n        ContextError: If binary content is detected\n    \"\"\"\n    if start &lt; 0:\n        start = 0\n    if start &gt;= self._size:\n        return \"\"\n\n    # Clamp length to file size\n    end = min(start + length, self._size)\n    actual_length = end - start\n\n    with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n        f.seek(start)\n        content = f.read(actual_length)\n\n    # v3.0: Validate no binary garbage\n    self._validate_text_content(content)\n\n    return content\n</code></pre>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.read_window","title":"read_window","text":"<pre><code>read_window(offset: int, radius: int = DEFAULT_WINDOW_SIZE) -&gt; str\n</code></pre> <p>Read a window of text centered around an offset.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Center point (byte offset)</p> required <code>radius</code> <code>int</code> <p>Number of bytes on each side of the offset</p> <code>DEFAULT_WINDOW_SIZE</code> <p>Returns:</p> Type Description <code>str</code> <p>The text content in the window</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def read_window(self, offset: int, radius: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n    \"\"\"\n    Read a window of text centered around an offset.\n\n    Args:\n        offset: Center point (byte offset)\n        radius: Number of bytes on each side of the offset\n\n    Returns:\n        The text content in the window\n    \"\"\"\n    start = max(0, offset - radius)\n    length = radius * 2\n    return self.read(start, length)\n</code></pre>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.snippet","title":"snippet","text":"<pre><code>snippet(offset: int, window: int = DEFAULT_WINDOW_SIZE) -&gt; str\n</code></pre> <p>Get a snippet of text around an offset.</p> <p>This is an alias for read_window with different semantics - window is the total size, not the radius.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Center point (byte offset)</p> required <code>window</code> <code>int</code> <p>Total window size in bytes</p> <code>DEFAULT_WINDOW_SIZE</code> <p>Returns:</p> Type Description <code>str</code> <p>The text snippet</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def snippet(self, offset: int, window: int = DEFAULT_WINDOW_SIZE) -&gt; str:\n    \"\"\"\n    Get a snippet of text around an offset.\n\n    This is an alias for read_window with different semantics -\n    window is the total size, not the radius.\n\n    Args:\n        offset: Center point (byte offset)\n        window: Total window size in bytes\n\n    Returns:\n        The text snippet\n    \"\"\"\n    return self.read_window(offset, window // 2)\n</code></pre>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.search","title":"search","text":"<pre><code>search(pattern: str, max_results: int = MAX_SEARCH_RESULTS, ignore_case: bool = True) -&gt; list[tuple[int, str]]\n</code></pre> <p>Search for a regex pattern in the file using memory mapping.</p> <p>This is memory-efficient as it doesn't load the entire file.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression pattern</p> required <code>max_results</code> <code>int</code> <p>Maximum number of results to return</p> <code>MAX_SEARCH_RESULTS</code> <code>ignore_case</code> <code>bool</code> <p>Whether to ignore case</p> <code>True</code> <p>Returns:</p> Type Description <code>list[tuple[int, str]]</code> <p>List of (byte_offset, matched_text) tuples</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def search(\n    self,\n    pattern: str,\n    max_results: int = MAX_SEARCH_RESULTS,\n    ignore_case: bool = True,\n) -&gt; list[tuple[int, str]]:\n    \"\"\"\n    Search for a regex pattern in the file using memory mapping.\n\n    This is memory-efficient as it doesn't load the entire file.\n\n    Args:\n        pattern: Regular expression pattern\n        max_results: Maximum number of results to return\n        ignore_case: Whether to ignore case\n\n    Returns:\n        List of (byte_offset, matched_text) tuples\n    \"\"\"\n    matches: list[tuple[int, str]] = []\n\n    try:\n        mm = self._get_mmap()\n\n        # Compile pattern for bytes\n        flags = re.IGNORECASE if ignore_case else 0\n        try:\n            bytes_pattern = pattern.encode(\"utf-8\")\n            compiled = re.compile(bytes_pattern, flags)\n        except re.error as e:\n            raise ContextError(\n                message=f\"Invalid regex pattern: {e}\",\n                details={\"pattern\": pattern},\n            ) from e\n\n        for match in compiled.finditer(mm):\n            if len(matches) &gt;= max_results:\n                break\n\n            try:\n                match_text = match.group().decode(\"utf-8\", errors=\"replace\")\n                matches.append((match.start(), match_text))\n            except UnicodeDecodeError:\n                # Skip matches that can't be decoded\n                continue\n\n    except Exception as e:\n        if not isinstance(e, ContextError):\n            raise ContextError(\n                message=f\"Search failed: {e}\",\n                path=str(self.path),\n            ) from e\n        raise\n\n    return matches\n</code></pre>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.search_lines","title":"search_lines","text":"<pre><code>search_lines(pattern: str, max_results: int = MAX_SEARCH_RESULTS, context_lines: int = 0) -&gt; list[tuple[int, str, str]]\n</code></pre> <p>Search for a pattern and return matching lines.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regex pattern to search for</p> required <code>max_results</code> <code>int</code> <p>Maximum number of results</p> <code>MAX_SEARCH_RESULTS</code> <code>context_lines</code> <code>int</code> <p>Number of lines before/after to include</p> <code>0</code> <p>Returns:</p> Type Description <code>list[tuple[int, str, str]]</code> <p>List of (line_number, matching_line, context) tuples</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def search_lines(\n    self,\n    pattern: str,\n    max_results: int = MAX_SEARCH_RESULTS,\n    context_lines: int = 0,\n) -&gt; list[tuple[int, str, str]]:\n    \"\"\"\n    Search for a pattern and return matching lines.\n\n    Args:\n        pattern: Regex pattern to search for\n        max_results: Maximum number of results\n        context_lines: Number of lines before/after to include\n\n    Returns:\n        List of (line_number, matching_line, context) tuples\n    \"\"\"\n    matches: list[tuple[int, str, str]] = []\n    compiled = re.compile(pattern, re.IGNORECASE)\n\n    lines_buffer: list[str] = []\n\n    with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n        for line_no, line in enumerate(f, start=1):\n            lines_buffer.append(line)\n            if len(lines_buffer) &gt; context_lines * 2 + 1:\n                lines_buffer.pop(0)\n\n            if compiled.search(line):\n                context = \"\".join(lines_buffer)\n                matches.append((line_no, line.strip(), context))\n\n                if len(matches) &gt;= max_results:\n                    break\n\n    return matches\n</code></pre>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.iterate_lines","title":"iterate_lines","text":"<pre><code>iterate_lines(start_line: int = 1) -&gt; Iterator[tuple[int, str]]\n</code></pre> <p>Iterate over lines in the file.</p> <p>This is memory-efficient as it reads line by line.</p> <p>Parameters:</p> Name Type Description Default <code>start_line</code> <code>int</code> <p>Line number to start from (1-indexed)</p> <code>1</code> <p>Yields:</p> Type Description <code>tuple[int, str]</code> <p>Tuples of (line_number, line_content)</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def iterate_lines(self, start_line: int = 1) -&gt; Iterator[tuple[int, str]]:\n    \"\"\"\n    Iterate over lines in the file.\n\n    This is memory-efficient as it reads line by line.\n\n    Args:\n        start_line: Line number to start from (1-indexed)\n\n    Yields:\n        Tuples of (line_number, line_content)\n    \"\"\"\n    with open(self.path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n        for line_no, line in enumerate(f, start=1):\n            if line_no &gt;= start_line:\n                yield line_no, line\n</code></pre>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.head","title":"head","text":"<pre><code>head(n_bytes: int = 1000) -&gt; str\n</code></pre> <p>Read the first n bytes of the file.</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def head(self, n_bytes: int = 1000) -&gt; str:\n    \"\"\"Read the first n bytes of the file.\"\"\"\n    return self.read(0, n_bytes)\n</code></pre>"},{"location":"api/context/#rlm.core.memory.handle.ContextHandle.tail","title":"tail","text":"<pre><code>tail(n_bytes: int = 1000) -&gt; str\n</code></pre> <p>Read the last n bytes of the file.</p> Source code in <code>src/rlm/core/memory/handle.py</code> <pre><code>def tail(self, n_bytes: int = 1000) -&gt; str:\n    \"\"\"Read the last n bytes of the file.\"\"\"\n    start = max(0, self._size - n_bytes)\n    return self.read(start, n_bytes)\n</code></pre>"},{"location":"api/context/#overview","title":"Overview","text":"<p><code>ContextHandle</code> provides memory-efficient access to large files using <code>mmap</code>. The LLM never loads the entire file into memory.</p>"},{"location":"api/context/#usage","title":"Usage","text":""},{"location":"api/context/#creating-a-handle","title":"Creating a Handle","text":"<pre><code>from rlm.core.memory import ContextHandle\n\n# Open a large file\nctx = ContextHandle(\"/path/to/large_file.csv\")\n\nprint(f\"File size: {ctx.size_mb:.2f} MB\")\n</code></pre>"},{"location":"api/context/#searching","title":"Searching","text":"<pre><code># Find all occurrences of a pattern\nmatches = ctx.search(r\"ERROR.*timeout\", max_results=10)\n\nfor offset, matched_text in matches:\n    # Get context around the match\n    snippet = ctx.snippet(offset, window=500)\n    print(f\"Found at {offset}: {snippet}\")\n</code></pre>"},{"location":"api/context/#reading-sections","title":"Reading Sections","text":"<pre><code># Read first 1KB\nheader = ctx.head(n_bytes=1024)\n\n# Read last 1KB\nfooter = ctx.tail(n_bytes=1024)\n\n# Read specific range\nchunk = ctx.read(start=5000, length=2000)\n</code></pre>"},{"location":"api/context/#streaming-lines","title":"Streaming Lines","text":"<pre><code># Memory-efficient line iteration\nfor line_no, line in ctx.iterate_lines(start_line=100):\n    if \"important\" in line:\n        print(f\"Line {line_no}: {line}\")\n    if line_no &gt; 200:\n        break\n</code></pre>"},{"location":"api/context/#binary-detection","title":"Binary Detection","text":"<p>v3.0 detects binary files and raises <code>ContextError</code>:</p> <pre><code>from rlm.core.memory import ContextHandle\nfrom rlm.core.exceptions import ContextError\n\ntry:\n    ctx = ContextHandle(\"/path/to/image.png\")\nexcept ContextError as e:\n    print(e)  # \"Binary file detected via null bytes...\"\n</code></pre> <p>This prevents the LLM from receiving garbage input that would cause hallucination.</p>"},{"location":"api/context/#with-orchestrator","title":"With Orchestrator","text":"<pre><code>from rlm import Orchestrator\n\nagent = Orchestrator()\n\nresult = await agent.arun(\n    query=\"Find all customers with revenue &gt; $100,000\",\n    context_path=\"/path/to/customers.csv\"  # Uses ContextHandle internally\n)\n</code></pre>"},{"location":"api/egress/","title":"EgressFilter API","text":""},{"location":"api/egress/#rlm.security.egress.EgressFilter","title":"rlm.security.egress.EgressFilter  <code>dataclass</code>","text":"<p>Filter for sanitizing code execution output.</p> <p>Implements multiple layers of protection: 1. Entropy detection for secrets 2. Pattern matching for known secret formats 3. Context similarity detection 4. Output size limiting</p> Source code in <code>src/rlm/security/egress.py</code> <pre><code>@dataclass\nclass EgressFilter:\n    \"\"\"\n    Filter for sanitizing code execution output.\n\n    Implements multiple layers of protection:\n    1. Entropy detection for secrets\n    2. Pattern matching for known secret formats\n    3. Context similarity detection\n    4. Output size limiting\n    \"\"\"\n\n    entropy_threshold: float = 4.5\n    min_entropy_length: int = 256\n    similarity_threshold: float = 0.8\n    max_output_bytes: int = 4000\n    context_fingerprint: Optional[str] = None\n    context_sample: Optional[str] = None\n\n    def __init__(\n        self,\n        context: Optional[str] = None,\n        entropy_threshold: Optional[float] = None,\n        similarity_threshold: Optional[float] = None,\n        max_output_bytes: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the egress filter.\n\n        Args:\n            context: The context/input data to protect from leakage\n            entropy_threshold: Override default entropy threshold\n            similarity_threshold: Override default similarity threshold\n            max_output_bytes: Override default output size limit\n        \"\"\"\n        self.entropy_threshold = entropy_threshold or settings.entropy_threshold\n        self.min_entropy_length = settings.min_entropy_length\n        self.similarity_threshold = similarity_threshold or settings.similarity_threshold\n        self.max_output_bytes = max_output_bytes or settings.max_stdout_bytes\n\n        if context:\n            self.context_fingerprint = hashlib.sha256(context.encode()).hexdigest()\n            # Store sample chunks for similarity comparison\n            self.context_sample = context[:2000] if len(context) &gt; 2000 else context\n\n    def check_entropy(self, text: str) -&gt; tuple[bool, float]:\n        \"\"\"\n        Check if text has suspiciously high entropy.\n\n        Args:\n            text: Text to check\n\n        Returns:\n            Tuple of (is_suspicious, entropy_value)\n        \"\"\"\n        if len(text) &lt; self.min_entropy_length:\n            return False, 0.0\n\n        entropy = calculate_shannon_entropy(text)\n        is_high = entropy &gt; self.entropy_threshold\n\n        if is_high:\n            logger.warning(f\"High entropy detected: {entropy:.2f} (threshold: {self.entropy_threshold})\")\n\n        return is_high, entropy\n\n    def check_context_echo(self, output: str) -&gt; tuple[bool, float]:\n        \"\"\"\n        Check if output is echoing the context (data exfiltration).\n\n        Args:\n            output: Output to check\n\n        Returns:\n            Tuple of (is_echo, similarity_score)\n        \"\"\"\n        if not self.context_sample:\n            return False, 0.0\n\n        similarity = calculate_similarity(output, self.context_sample)\n        is_echo = similarity &gt; self.similarity_threshold\n\n        if is_echo:\n            logger.warning(\n                f\"Context echo detected: similarity {similarity:.2f} \"\n                f\"(threshold: {self.similarity_threshold})\"\n            )\n\n        return is_echo, similarity\n\n    def check_secrets(self, text: str) -&gt; list[tuple[str, str]]:\n        \"\"\"\n        Check for known secret patterns.\n\n        Args:\n            text: Text to check\n\n        Returns:\n            List of detected secrets (pattern_name, redacted_match)\n        \"\"\"\n        return detect_secrets(text)\n\n    def truncate_output(self, text: str) -&gt; str:\n        \"\"\"\n        Truncate output while preserving head and tail (most useful parts).\n\n        Args:\n            text: Text to truncate\n\n        Returns:\n            Truncated text with indicator\n        \"\"\"\n        if len(text) &lt;= self.max_output_bytes:\n            return text\n\n        # Preserve first 1KB and last 3KB (errors usually at the end)\n        head_size = 1000\n        tail_size = 3000\n        truncated = len(text) - head_size - tail_size\n\n        head = text[:head_size]\n        tail = text[-tail_size:]\n\n        return f\"{head}\\n\\n... [TRUNCATED {truncated} bytes] ...\\n\\n{tail}\"\n\n    def filter(\n        self,\n        output: str,\n        raise_on_leak: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Apply all egress filters to output.\n\n        Args:\n            output: Raw output to filter\n            raise_on_leak: Whether to raise DataLeakageError on detection\n\n        Returns:\n            Sanitized output\n\n        Raises:\n            DataLeakageError: If raise_on_leak is True and leakage is detected\n        \"\"\"\n        if not output:\n            return output\n\n        redactions = []\n\n        # Check for secrets\n        secrets = self.check_secrets(output)\n        if secrets:\n            if raise_on_leak:\n                raise DataLeakageError(\n                    message=\"Secrets detected in output\",\n                    leak_type=\"secret_pattern\",\n                    details={\"patterns\": [s[0] for s in secrets]},\n                )\n            for pattern_name, matched in secrets:\n                redactions.append(f\"[REDACTED: {pattern_name}]\")\n                # Remove the matched content\n                output = re.sub(re.escape(matched), f\"[REDACTED: {pattern_name}]\", output)\n\n        # Check for high entropy\n        is_high_entropy, entropy = self.check_entropy(output)\n        if is_high_entropy:\n            if raise_on_leak:\n                raise DataLeakageError(\n                    message=\"High entropy data detected - potential secret leak\",\n                    leak_type=\"high_entropy\",\n                    details={\"entropy\": entropy},\n                )\n            output = f\"[SECURITY REDACTION: High Entropy Data Detected - Potential Secret Leak]\\n(Entropy: {entropy:.2f})\"\n\n        # Check for context echo\n        is_echo, similarity = self.check_context_echo(output)\n        if is_echo:\n            if raise_on_leak:\n                raise DataLeakageError(\n                    message=\"Do not print raw context. Summarize it.\",\n                    leak_type=\"context_echo\",\n                    details={\"similarity\": similarity},\n                )\n            output = f\"[SECURITY REDACTION: Context Echo Detected]\\nDo not print raw context data. Use ctx.search() and ctx.snippet() to extract specific information.\"\n\n        # Truncate if needed\n        output = self.truncate_output(output)\n\n        if redactions:\n            logger.info(f\"Egress filter applied {len(redactions)} redactions\")\n\n        return output\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.EgressFilter.filter","title":"filter","text":"<pre><code>filter(output: str, raise_on_leak: bool = False) -&gt; str\n</code></pre> <p>Apply all egress filters to output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>Raw output to filter</p> required <code>raise_on_leak</code> <code>bool</code> <p>Whether to raise DataLeakageError on detection</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Sanitized output</p> <p>Raises:</p> Type Description <code>DataLeakageError</code> <p>If raise_on_leak is True and leakage is detected</p> Source code in <code>src/rlm/security/egress.py</code> <pre><code>def filter(\n    self,\n    output: str,\n    raise_on_leak: bool = False,\n) -&gt; str:\n    \"\"\"\n    Apply all egress filters to output.\n\n    Args:\n        output: Raw output to filter\n        raise_on_leak: Whether to raise DataLeakageError on detection\n\n    Returns:\n        Sanitized output\n\n    Raises:\n        DataLeakageError: If raise_on_leak is True and leakage is detected\n    \"\"\"\n    if not output:\n        return output\n\n    redactions = []\n\n    # Check for secrets\n    secrets = self.check_secrets(output)\n    if secrets:\n        if raise_on_leak:\n            raise DataLeakageError(\n                message=\"Secrets detected in output\",\n                leak_type=\"secret_pattern\",\n                details={\"patterns\": [s[0] for s in secrets]},\n            )\n        for pattern_name, matched in secrets:\n            redactions.append(f\"[REDACTED: {pattern_name}]\")\n            # Remove the matched content\n            output = re.sub(re.escape(matched), f\"[REDACTED: {pattern_name}]\", output)\n\n    # Check for high entropy\n    is_high_entropy, entropy = self.check_entropy(output)\n    if is_high_entropy:\n        if raise_on_leak:\n            raise DataLeakageError(\n                message=\"High entropy data detected - potential secret leak\",\n                leak_type=\"high_entropy\",\n                details={\"entropy\": entropy},\n            )\n        output = f\"[SECURITY REDACTION: High Entropy Data Detected - Potential Secret Leak]\\n(Entropy: {entropy:.2f})\"\n\n    # Check for context echo\n    is_echo, similarity = self.check_context_echo(output)\n    if is_echo:\n        if raise_on_leak:\n            raise DataLeakageError(\n                message=\"Do not print raw context. Summarize it.\",\n                leak_type=\"context_echo\",\n                details={\"similarity\": similarity},\n            )\n        output = f\"[SECURITY REDACTION: Context Echo Detected]\\nDo not print raw context data. Use ctx.search() and ctx.snippet() to extract specific information.\"\n\n    # Truncate if needed\n    output = self.truncate_output(output)\n\n    if redactions:\n        logger.info(f\"Egress filter applied {len(redactions)} redactions\")\n\n    return output\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.EgressFilter.check_entropy","title":"check_entropy","text":"<pre><code>check_entropy(text: str) -&gt; tuple[bool, float]\n</code></pre> <p>Check if text has suspiciously high entropy.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to check</p> required <p>Returns:</p> Type Description <code>tuple[bool, float]</code> <p>Tuple of (is_suspicious, entropy_value)</p> Source code in <code>src/rlm/security/egress.py</code> <pre><code>def check_entropy(self, text: str) -&gt; tuple[bool, float]:\n    \"\"\"\n    Check if text has suspiciously high entropy.\n\n    Args:\n        text: Text to check\n\n    Returns:\n        Tuple of (is_suspicious, entropy_value)\n    \"\"\"\n    if len(text) &lt; self.min_entropy_length:\n        return False, 0.0\n\n    entropy = calculate_shannon_entropy(text)\n    is_high = entropy &gt; self.entropy_threshold\n\n    if is_high:\n        logger.warning(f\"High entropy detected: {entropy:.2f} (threshold: {self.entropy_threshold})\")\n\n    return is_high, entropy\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.EgressFilter.check_context_echo","title":"check_context_echo","text":"<pre><code>check_context_echo(output: str) -&gt; tuple[bool, float]\n</code></pre> <p>Check if output is echoing the context (data exfiltration).</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>Output to check</p> required <p>Returns:</p> Type Description <code>tuple[bool, float]</code> <p>Tuple of (is_echo, similarity_score)</p> Source code in <code>src/rlm/security/egress.py</code> <pre><code>def check_context_echo(self, output: str) -&gt; tuple[bool, float]:\n    \"\"\"\n    Check if output is echoing the context (data exfiltration).\n\n    Args:\n        output: Output to check\n\n    Returns:\n        Tuple of (is_echo, similarity_score)\n    \"\"\"\n    if not self.context_sample:\n        return False, 0.0\n\n    similarity = calculate_similarity(output, self.context_sample)\n    is_echo = similarity &gt; self.similarity_threshold\n\n    if is_echo:\n        logger.warning(\n            f\"Context echo detected: similarity {similarity:.2f} \"\n            f\"(threshold: {self.similarity_threshold})\"\n        )\n\n    return is_echo, similarity\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.EgressFilter.check_secrets","title":"check_secrets","text":"<pre><code>check_secrets(text: str) -&gt; list[tuple[str, str]]\n</code></pre> <p>Check for known secret patterns.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to check</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>List of detected secrets (pattern_name, redacted_match)</p> Source code in <code>src/rlm/security/egress.py</code> <pre><code>def check_secrets(self, text: str) -&gt; list[tuple[str, str]]:\n    \"\"\"\n    Check for known secret patterns.\n\n    Args:\n        text: Text to check\n\n    Returns:\n        List of detected secrets (pattern_name, redacted_match)\n    \"\"\"\n    return detect_secrets(text)\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.EgressFilter.truncate_output","title":"truncate_output","text":"<pre><code>truncate_output(text: str) -&gt; str\n</code></pre> <p>Truncate output while preserving head and tail (most useful parts).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to truncate</p> required <p>Returns:</p> Type Description <code>str</code> <p>Truncated text with indicator</p> Source code in <code>src/rlm/security/egress.py</code> <pre><code>def truncate_output(self, text: str) -&gt; str:\n    \"\"\"\n    Truncate output while preserving head and tail (most useful parts).\n\n    Args:\n        text: Text to truncate\n\n    Returns:\n        Truncated text with indicator\n    \"\"\"\n    if len(text) &lt;= self.max_output_bytes:\n        return text\n\n    # Preserve first 1KB and last 3KB (errors usually at the end)\n    head_size = 1000\n    tail_size = 3000\n    truncated = len(text) - head_size - tail_size\n\n    head = text[:head_size]\n    tail = text[-tail_size:]\n\n    return f\"{head}\\n\\n... [TRUNCATED {truncated} bytes] ...\\n\\n{tail}\"\n</code></pre>"},{"location":"api/egress/#utility-functions","title":"Utility Functions","text":""},{"location":"api/egress/#rlm.security.egress.calculate_shannon_entropy","title":"rlm.security.egress.calculate_shannon_entropy","text":"<pre><code>calculate_shannon_entropy(data: str) -&gt; float\n</code></pre> <p>Calculate Shannon entropy of a string.</p> <p>Higher entropy indicates more randomness - potential secrets or encoded binary data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>String to analyze</p> required <p>Returns:</p> Type Description <code>float</code> <p>Shannon entropy value (typically 0-8 for ASCII text)</p> <code>float</code> <ul> <li>0-2: Very low entropy (repetitive)</li> </ul> <code>float</code> <ul> <li>2-4: Low entropy (natural language)</li> </ul> <code>float</code> <ul> <li>4-5: Medium entropy (code, mixed content)</li> </ul> <code>float</code> <ul> <li>5-8: High entropy (secrets, binary, compressed)</li> </ul> Source code in <code>src/rlm/security/egress.py</code> <pre><code>def calculate_shannon_entropy(data: str) -&gt; float:\n    \"\"\"\n    Calculate Shannon entropy of a string.\n\n    Higher entropy indicates more randomness - potential secrets or\n    encoded binary data.\n\n    Args:\n        data: String to analyze\n\n    Returns:\n        Shannon entropy value (typically 0-8 for ASCII text)\n        - 0-2: Very low entropy (repetitive)\n        - 2-4: Low entropy (natural language)\n        - 4-5: Medium entropy (code, mixed content)\n        - 5-8: High entropy (secrets, binary, compressed)\n    \"\"\"\n    if not data:\n        return 0.0\n\n    entropy = 0.0\n    counter = Counter(data)\n    length = len(data)\n\n    for count in counter.values():\n        if count &gt; 0:\n            probability = count / length\n            entropy -= probability * math.log2(probability)\n\n    return entropy\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.calculate_similarity","title":"rlm.security.egress.calculate_similarity","text":"<pre><code>calculate_similarity(s1: str, s2: str, ngram_size: int = 3) -&gt; float\n</code></pre> <p>Calculate Jaccard similarity between two strings using n-grams.</p> <p>This is used to detect \"echo attacks\" where the LLM prints the raw context back.</p> <p>Parameters:</p> Name Type Description Default <code>s1</code> <code>str</code> <p>First string</p> required <code>s2</code> <code>str</code> <p>Second string</p> required <code>ngram_size</code> <code>int</code> <p>Size of n-grams to use</p> <code>3</code> <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> Source code in <code>src/rlm/security/egress.py</code> <pre><code>def calculate_similarity(s1: str, s2: str, ngram_size: int = 3) -&gt; float:\n    \"\"\"\n    Calculate Jaccard similarity between two strings using n-grams.\n\n    This is used to detect \"echo attacks\" where the LLM prints\n    the raw context back.\n\n    Args:\n        s1: First string\n        s2: Second string\n        ngram_size: Size of n-grams to use\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n    \"\"\"\n    if not s1 or not s2:\n        return 0.0\n\n    # Generate n-grams\n    def get_ngrams(s: str) -&gt; set:\n        s = s.lower()\n        return {s[i : i + ngram_size] for i in range(len(s) - ngram_size + 1)}\n\n    ngrams1 = get_ngrams(s1)\n    ngrams2 = get_ngrams(s2)\n\n    if not ngrams1 or not ngrams2:\n        return 0.0\n\n    intersection = len(ngrams1 &amp; ngrams2)\n    union = len(ngrams1 | ngrams2)\n\n    return intersection / union if union &gt; 0 else 0.0\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.detect_secrets","title":"rlm.security.egress.detect_secrets","text":"<pre><code>detect_secrets(text: str) -&gt; list[tuple[str, str]]\n</code></pre> <p>Detect potential secrets in text using pattern matching.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to scan</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>List of (pattern_name, matched_text) tuples</p> Source code in <code>src/rlm/security/egress.py</code> <pre><code>def detect_secrets(text: str) -&gt; list[tuple[str, str]]:\n    \"\"\"\n    Detect potential secrets in text using pattern matching.\n\n    Args:\n        text: Text to scan\n\n    Returns:\n        List of (pattern_name, matched_text) tuples\n    \"\"\"\n    matches = []\n    pattern_names = [\n        \"api_key\",\n        \"aws_access_key\",\n        \"aws_secret\",\n        \"private_key\",\n        \"jwt\",\n        \"generic_secret\",\n        \"bearer_token\",\n    ]\n\n    for pattern, name in zip(SECRET_PATTERNS, pattern_names):\n        for match in re.finditer(pattern, text):\n            # Truncate match for logging\n            matched = match.group()[:50] + \"...\" if len(match.group()) &gt; 50 else match.group()\n            matches.append((name, matched))\n\n    return matches\n</code></pre>"},{"location":"api/egress/#rlm.security.egress.sanitize_output","title":"rlm.security.egress.sanitize_output","text":"<pre><code>sanitize_output(output: str, context: Optional[str] = None, raise_on_leak: bool = False) -&gt; str\n</code></pre> <p>Convenience function to sanitize code execution output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>Raw output from code execution</p> required <code>context</code> <code>Optional[str]</code> <p>Optional context data to protect from leakage</p> <code>None</code> <code>raise_on_leak</code> <code>bool</code> <p>Whether to raise exception on detected leakage</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Sanitized output</p> Source code in <code>src/rlm/security/egress.py</code> <pre><code>def sanitize_output(\n    output: str,\n    context: Optional[str] = None,\n    raise_on_leak: bool = False,\n) -&gt; str:\n    \"\"\"\n    Convenience function to sanitize code execution output.\n\n    Args:\n        output: Raw output from code execution\n        context: Optional context data to protect from leakage\n        raise_on_leak: Whether to raise exception on detected leakage\n\n    Returns:\n        Sanitized output\n    \"\"\"\n    filter_instance = EgressFilter(context=context)\n    return filter_instance.filter(output, raise_on_leak=raise_on_leak)\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":"<p>All RLM exceptions inherit from <code>RLMError</code>.</p>"},{"location":"api/exceptions/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>RLMError\n\u251c\u2500\u2500 SecurityViolationError  # gVisor missing, security bypass attempted\n\u251c\u2500\u2500 DataLeakageError        # Egress filter detected secret\n\u251c\u2500\u2500 SandboxError            # Docker execution failed\n\u251c\u2500\u2500 ContextError            # Context file issues\n\u251c\u2500\u2500 LLMError                # LLM API failures\n\u251c\u2500\u2500 BudgetExceededError     # Token budget exceeded\n\u2514\u2500\u2500 ConfigurationError      # Invalid configuration\n</code></pre>"},{"location":"api/exceptions/#reference","title":"Reference","text":""},{"location":"api/exceptions/#rlm.core.exceptions.RLMError","title":"RLMError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all RLM errors.</p> Source code in <code>src/rlm/core/exceptions.py</code> <pre><code>class RLMError(Exception):\n    \"\"\"Base exception for all RLM errors.\"\"\"\n\n    def __init__(self, message: str, details: Optional[dict] = None) -&gt; None:\n        super().__init__(message)\n        self.message = message\n        self.details = details or {}\n\n    def __str__(self) -&gt; str:\n        if self.details:\n            return f\"{self.message} | Details: {self.details}\"\n        return self.message\n</code></pre>"},{"location":"api/exceptions/#rlm.core.exceptions.SecurityViolationError","title":"SecurityViolationError","text":"<p>               Bases: <code>RLMError</code></p> <p>Raised when a security boundary is violated.</p> <p>This includes attempts to escape the sandbox, access restricted resources, or perform unauthorized operations.</p> Source code in <code>src/rlm/core/exceptions.py</code> <pre><code>class SecurityViolationError(RLMError):\n    \"\"\"\n    Raised when a security boundary is violated.\n\n    This includes attempts to escape the sandbox, access restricted\n    resources, or perform unauthorized operations.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Security violation detected\",\n        violation_type: Optional[str] = None,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        super().__init__(message, details)\n        self.violation_type = violation_type\n</code></pre>"},{"location":"api/exceptions/#rlm.core.exceptions.DataLeakageError","title":"DataLeakageError","text":"<p>               Bases: <code>SecurityViolationError</code></p> <p>Raised when potential data leakage is detected.</p> <p>This is triggered by the egress filter when: - High entropy data (potential secrets) is detected in output - Output closely matches the input context (echo attack)</p> Source code in <code>src/rlm/core/exceptions.py</code> <pre><code>class DataLeakageError(SecurityViolationError):\n    \"\"\"\n    Raised when potential data leakage is detected.\n\n    This is triggered by the egress filter when:\n    - High entropy data (potential secrets) is detected in output\n    - Output closely matches the input context (echo attack)\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Potential data leakage detected\",\n        leak_type: Optional[str] = None,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        super().__init__(message, violation_type=\"data_leakage\", details=details)\n        self.leak_type = leak_type\n</code></pre>"},{"location":"api/exceptions/#rlm.core.exceptions.SandboxError","title":"SandboxError","text":"<p>               Bases: <code>RLMError</code></p> <p>Raised when sandbox execution fails.</p> <p>This includes container startup failures, timeout issues, and resource limit violations (OOM, CPU quota, etc.).</p> Source code in <code>src/rlm/core/exceptions.py</code> <pre><code>class SandboxError(RLMError):\n    \"\"\"\n    Raised when sandbox execution fails.\n\n    This includes container startup failures, timeout issues,\n    and resource limit violations (OOM, CPU quota, etc.).\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Sandbox execution failed\",\n        exit_code: Optional[int] = None,\n        stderr: Optional[str] = None,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        details = details or {}\n        if exit_code is not None:\n            details[\"exit_code\"] = exit_code\n        if stderr:\n            details[\"stderr\"] = stderr[:500]  # Truncate for safety\n        super().__init__(message, details)\n        self.exit_code = exit_code\n        self.stderr = stderr\n\n    @property\n    def is_oom_killed(self) -&gt; bool:\n        \"\"\"Check if the container was killed due to OOM.\"\"\"\n        return self.exit_code == 137\n\n    @property\n    def is_timeout(self) -&gt; bool:\n        \"\"\"Check if the execution timed out.\"\"\"\n        return self.exit_code == 124\n</code></pre>"},{"location":"api/exceptions/#rlm.core.exceptions.SandboxError.is_oom_killed","title":"is_oom_killed  <code>property</code>","text":"<pre><code>is_oom_killed: bool\n</code></pre> <p>Check if the container was killed due to OOM.</p>"},{"location":"api/exceptions/#rlm.core.exceptions.SandboxError.is_timeout","title":"is_timeout  <code>property</code>","text":"<pre><code>is_timeout: bool\n</code></pre> <p>Check if the execution timed out.</p>"},{"location":"api/exceptions/#rlm.core.exceptions.ContextError","title":"ContextError","text":"<p>               Bases: <code>RLMError</code></p> <p>Raised when there's an issue with context handling.</p> <p>This includes file not found, encoding issues, or memory mapping failures.</p> Source code in <code>src/rlm/core/exceptions.py</code> <pre><code>class ContextError(RLMError):\n    \"\"\"\n    Raised when there's an issue with context handling.\n\n    This includes file not found, encoding issues, or memory\n    mapping failures.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Context handling error\",\n        path: Optional[str] = None,\n        details: Optional[dict] = None,\n    ) -&gt; None:\n        details = details or {}\n        if path:\n            details[\"path\"] = path\n        super().__init__(message, details)\n        self.path = path\n</code></pre>"},{"location":"api/exceptions/#handling-exceptions","title":"Handling Exceptions","text":"<pre><code>from rlm import Orchestrator\nfrom rlm.core.exceptions import (\n    SecurityViolationError,\n    DataLeakageError,\n    SandboxError,\n    BudgetExceededError,\n)\n\nagent = Orchestrator()\n\ntry:\n    result = await agent.arun(\"Do something\")\nexcept SecurityViolationError as e:\n    # gVisor not available, unsafe runtime not allowed\n    print(f\"Security issue: {e}\")\nexcept DataLeakageError as e:\n    # Egress filter detected potential secret\n    print(f\"Data leak prevented: {e}\")\n    print(f\"Leak type: {e.leak_type}\")\nexcept SandboxError as e:\n    # Docker execution failed\n    print(f\"Sandbox error: {e}\")\n    print(f\"Exit code: {e.exit_code}\")\nexcept BudgetExceededError as e:\n    # Token budget exceeded\n    print(f\"Budget exceeded: {e}\")\n</code></pre>"},{"location":"api/llm/","title":"LLM Clients API","text":""},{"location":"api/llm/#base-client","title":"Base Client","text":""},{"location":"api/llm/#rlm.llm.base.BaseLLMClient","title":"rlm.llm.base.BaseLLMClient","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for LLM provider clients.</p> <p>v2.1: All providers must implement both sync and async methods: - complete() / acomplete() for full responses - stream() / astream() for streaming responses</p> Source code in <code>src/rlm/llm/base.py</code> <pre><code>class BaseLLMClient(ABC):\n    \"\"\"\n    Abstract base class for LLM provider clients.\n\n    v2.1: All providers must implement both sync and async methods:\n    - complete() / acomplete() for full responses\n    - stream() / astream() for streaming responses\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str,\n        temperature: float = 0.0,\n        max_tokens: int = 4096,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the LLM client.\n\n        Args:\n            api_key: API key for the provider\n            model: Model name to use\n            temperature: Sampling temperature (0.0-2.0)\n            max_tokens: Maximum tokens in response\n        \"\"\"\n        self.api_key = api_key\n        self.model = model\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n\n    @property\n    @abstractmethod\n    def provider_name(self) -&gt; str:\n        \"\"\"Return the provider name (e.g., 'openai', 'anthropic').\"\"\"\n        pass\n\n    # ==================== Sync Methods ====================\n\n    @abstractmethod\n    def complete(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; LLMResponse:\n        \"\"\"\n        Generate a completion from the model (synchronous).\n\n        Args:\n            messages: List of conversation messages\n            system_prompt: Optional system prompt\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            LLMResponse containing the generated content\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def stream(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"\n        Stream a completion from the model (synchronous).\n\n        Args:\n            messages: List of conversation messages\n            system_prompt: Optional system prompt\n            **kwargs: Additional provider-specific arguments\n\n        Yields:\n            Chunks of the generated content\n        \"\"\"\n        pass\n\n    # ==================== Async Methods (v2.1) ====================\n\n    @abstractmethod\n    async def acomplete(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; LLMResponse:\n        \"\"\"\n        Generate a completion from the model (asynchronous).\n\n        Args:\n            messages: List of conversation messages\n            system_prompt: Optional system prompt\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            LLMResponse containing the generated content\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def astream(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Stream a completion from the model (asynchronous).\n\n        Args:\n            messages: List of conversation messages\n            system_prompt: Optional system prompt\n            **kwargs: Additional provider-specific arguments\n\n        Yields:\n            Chunks of the generated content\n        \"\"\"\n        pass\n        # Required for abstract async generator\n        yield  # type: ignore\n\n    # ==================== Utilities ====================\n\n    def validate_api_key(self) -&gt; bool:\n        \"\"\"\n        Validate that the API key is working.\n\n        Returns:\n            True if the API key is valid\n        \"\"\"\n        try:\n            # Try a minimal request\n            self.complete([Message(role=\"user\", content=\"test\")])\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.BaseLLMClient.provider_name","title":"provider_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name (e.g., 'openai', 'anthropic').</p>"},{"location":"api/llm/#rlm.llm.base.BaseLLMClient.complete","title":"complete  <code>abstractmethod</code>","text":"<pre><code>complete(messages: list[Message], system_prompt: Optional[str] = None, **kwargs) -&gt; LLMResponse\n</code></pre> <p>Generate a completion from the model (synchronous).</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse containing the generated content</p> Source code in <code>src/rlm/llm/base.py</code> <pre><code>@abstractmethod\ndef complete(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; LLMResponse:\n    \"\"\"\n    Generate a completion from the model (synchronous).\n\n    Args:\n        messages: List of conversation messages\n        system_prompt: Optional system prompt\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        LLMResponse containing the generated content\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.BaseLLMClient.stream","title":"stream  <code>abstractmethod</code>","text":"<pre><code>stream(messages: list[Message], system_prompt: Optional[str] = None, **kwargs) -&gt; Iterator[str]\n</code></pre> <p>Stream a completion from the model (synchronous).</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of the generated content</p> Source code in <code>src/rlm/llm/base.py</code> <pre><code>@abstractmethod\ndef stream(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Stream a completion from the model (synchronous).\n\n    Args:\n        messages: List of conversation messages\n        system_prompt: Optional system prompt\n        **kwargs: Additional provider-specific arguments\n\n    Yields:\n        Chunks of the generated content\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.BaseLLMClient.validate_api_key","title":"validate_api_key","text":"<pre><code>validate_api_key() -&gt; bool\n</code></pre> <p>Validate that the API key is working.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the API key is valid</p> Source code in <code>src/rlm/llm/base.py</code> <pre><code>def validate_api_key(self) -&gt; bool:\n    \"\"\"\n    Validate that the API key is working.\n\n    Returns:\n        True if the API key is valid\n    \"\"\"\n    try:\n        # Try a minimal request\n        self.complete([Message(role=\"user\", content=\"test\")])\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.Message","title":"rlm.llm.base.Message  <code>dataclass</code>","text":"<p>A message in the conversation history.</p> Source code in <code>src/rlm/llm/base.py</code> <pre><code>@dataclass\nclass Message:\n    \"\"\"A message in the conversation history.\"\"\"\n\n    role: Role | Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n    name: Optional[str] = None\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert to dictionary format for API calls.\"\"\"\n        d = {\"role\": str(self.role.value if isinstance(self.role, Role) else self.role), \"content\": self.content}\n        if self.name:\n            d[\"name\"] = self.name\n        return d\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.Message.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary format for API calls.</p> Source code in <code>src/rlm/llm/base.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary format for API calls.\"\"\"\n    d = {\"role\": str(self.role.value if isinstance(self.role, Role) else self.role), \"content\": self.content}\n    if self.name:\n        d[\"name\"] = self.name\n    return d\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.LLMResponse","title":"rlm.llm.base.LLMResponse  <code>dataclass</code>","text":"<p>Response from an LLM API call.</p> Source code in <code>src/rlm/llm/base.py</code> <pre><code>@dataclass\nclass LLMResponse:\n    \"\"\"Response from an LLM API call.\"\"\"\n\n    content: str\n    model: str\n    usage: TokenUsage = field(default_factory=TokenUsage)\n    finish_reason: Optional[str] = None\n    raw_response: Optional[dict] = None\n</code></pre>"},{"location":"api/llm/#rlm.llm.base.TokenUsage","title":"rlm.llm.base.TokenUsage  <code>dataclass</code>","text":"<p>Token usage information from an API response.</p> Source code in <code>src/rlm/llm/base.py</code> <pre><code>@dataclass\nclass TokenUsage:\n    \"\"\"Token usage information from an API response.\"\"\"\n\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n    total_tokens: int = 0\n</code></pre>"},{"location":"api/llm/#openai-client","title":"OpenAI Client","text":""},{"location":"api/llm/#rlm.llm.openai_client.OpenAIClient","title":"rlm.llm.openai_client.OpenAIClient","text":"<p>               Bases: <code>BaseLLMClient</code></p> <p>OpenAI API client for GPT models.</p> <p>Supports both regular and streaming completions.</p> Example <p>client = OpenAIClient(api_key=\"sk-...\", model=\"gpt-4o\") response = client.complete([Message(role=\"user\", content=\"Hello!\")]) print(response.content)</p> Source code in <code>src/rlm/llm/openai_client.py</code> <pre><code>class OpenAIClient(BaseLLMClient):\n    \"\"\"\n    OpenAI API client for GPT models.\n\n    Supports both regular and streaming completions.\n\n    Example:\n        &gt;&gt;&gt; client = OpenAIClient(api_key=\"sk-...\", model=\"gpt-4o\")\n        &gt;&gt;&gt; response = client.complete([Message(role=\"user\", content=\"Hello!\")])\n        &gt;&gt;&gt; print(response.content)\n    \"\"\"\n\n    DEFAULT_MODEL = \"gpt-4o\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str = DEFAULT_MODEL,\n        temperature: float = 0.0,\n        max_tokens: int = 4096,\n        base_url: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the OpenAI client.\n\n        Args:\n            api_key: OpenAI API key\n            model: Model name (default: gpt-4o)\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens in response\n            base_url: Optional custom base URL (for Azure or proxies)\n        \"\"\"\n        super().__init__(api_key, model, temperature, max_tokens)\n        self._client = OpenAI(api_key=api_key, base_url=base_url)\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"openai\"\n\n    def complete(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; LLMResponse:\n        \"\"\"\n        Generate a completion using OpenAI's API.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments (passed to API)\n\n        Returns:\n            LLMResponse with the generated content\n        \"\"\"\n        all_messages = []\n\n        if system_prompt:\n            all_messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n        all_messages.extend([m.to_dict() for m in messages])\n\n        try:\n            response = self._client.chat.completions.create(\n                model=self.model,\n                messages=all_messages,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                **kwargs,\n            )\n\n            choice = response.choices[0]\n            usage = response.usage\n\n            return LLMResponse(\n                content=choice.message.content or \"\",\n                model=response.model,\n                usage=TokenUsage(\n                    prompt_tokens=usage.prompt_tokens if usage else 0,\n                    completion_tokens=usage.completion_tokens if usage else 0,\n                    total_tokens=usage.total_tokens if usage else 0,\n                ),\n                finish_reason=choice.finish_reason,\n                raw_response=response.model_dump() if hasattr(response, \"model_dump\") else None,\n            )\n\n        except Exception as e:\n            logger.error(f\"OpenAI API error: {e}\")\n            raise LLMError(\n                message=f\"OpenAI API request failed: {e}\",\n                provider=\"openai\",\n            ) from e\n\n    def stream(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"\n        Stream a completion from OpenAI.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments\n\n        Yields:\n            Chunks of the generated content\n        \"\"\"\n        all_messages = []\n\n        if system_prompt:\n            all_messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n        all_messages.extend([m.to_dict() for m in messages])\n\n        try:\n            stream = self._client.chat.completions.create(\n                model=self.model,\n                messages=all_messages,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                stream=True,\n                **kwargs,\n            )\n\n            for chunk in stream:\n                if chunk.choices and chunk.choices[0].delta.content:\n                    yield chunk.choices[0].delta.content\n\n        except Exception as e:\n            logger.error(f\"OpenAI streaming error: {e}\")\n            raise LLMError(\n                message=f\"OpenAI streaming failed: {e}\",\n                provider=\"openai\",\n            ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.openai_client.OpenAIClient.__init__","title":"__init__","text":"<pre><code>__init__(api_key: str, model: str = DEFAULT_MODEL, temperature: float = 0.0, max_tokens: int = 4096, base_url: Optional[str] = None) -&gt; None\n</code></pre> <p>Initialize the OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key</p> required <code>model</code> <code>str</code> <p>Model name (default: gpt-4o)</p> <code>DEFAULT_MODEL</code> <code>temperature</code> <code>float</code> <p>Sampling temperature</p> <code>0.0</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens in response</p> <code>4096</code> <code>base_url</code> <code>Optional[str]</code> <p>Optional custom base URL (for Azure or proxies)</p> <code>None</code> Source code in <code>src/rlm/llm/openai_client.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.0,\n    max_tokens: int = 4096,\n    base_url: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the OpenAI client.\n\n    Args:\n        api_key: OpenAI API key\n        model: Model name (default: gpt-4o)\n        temperature: Sampling temperature\n        max_tokens: Maximum tokens in response\n        base_url: Optional custom base URL (for Azure or proxies)\n    \"\"\"\n    super().__init__(api_key, model, temperature, max_tokens)\n    self._client = OpenAI(api_key=api_key, base_url=base_url)\n</code></pre>"},{"location":"api/llm/#rlm.llm.openai_client.OpenAIClient.complete","title":"complete","text":"<pre><code>complete(messages: list[Message], system_prompt: Optional[str] = None, **kwargs) -&gt; LLMResponse\n</code></pre> <p>Generate a completion using OpenAI's API.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments (passed to API)</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with the generated content</p> Source code in <code>src/rlm/llm/openai_client.py</code> <pre><code>def complete(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; LLMResponse:\n    \"\"\"\n    Generate a completion using OpenAI's API.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments (passed to API)\n\n    Returns:\n        LLMResponse with the generated content\n    \"\"\"\n    all_messages = []\n\n    if system_prompt:\n        all_messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n    all_messages.extend([m.to_dict() for m in messages])\n\n    try:\n        response = self._client.chat.completions.create(\n            model=self.model,\n            messages=all_messages,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            **kwargs,\n        )\n\n        choice = response.choices[0]\n        usage = response.usage\n\n        return LLMResponse(\n            content=choice.message.content or \"\",\n            model=response.model,\n            usage=TokenUsage(\n                prompt_tokens=usage.prompt_tokens if usage else 0,\n                completion_tokens=usage.completion_tokens if usage else 0,\n                total_tokens=usage.total_tokens if usage else 0,\n            ),\n            finish_reason=choice.finish_reason,\n            raw_response=response.model_dump() if hasattr(response, \"model_dump\") else None,\n        )\n\n    except Exception as e:\n        logger.error(f\"OpenAI API error: {e}\")\n        raise LLMError(\n            message=f\"OpenAI API request failed: {e}\",\n            provider=\"openai\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.openai_client.OpenAIClient.stream","title":"stream","text":"<pre><code>stream(messages: list[Message], system_prompt: Optional[str] = None, **kwargs) -&gt; Iterator[str]\n</code></pre> <p>Stream a completion from OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of the generated content</p> Source code in <code>src/rlm/llm/openai_client.py</code> <pre><code>def stream(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Stream a completion from OpenAI.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments\n\n    Yields:\n        Chunks of the generated content\n    \"\"\"\n    all_messages = []\n\n    if system_prompt:\n        all_messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n    all_messages.extend([m.to_dict() for m in messages])\n\n    try:\n        stream = self._client.chat.completions.create(\n            model=self.model,\n            messages=all_messages,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            stream=True,\n            **kwargs,\n        )\n\n        for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content:\n                yield chunk.choices[0].delta.content\n\n    except Exception as e:\n        logger.error(f\"OpenAI streaming error: {e}\")\n        raise LLMError(\n            message=f\"OpenAI streaming failed: {e}\",\n            provider=\"openai\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#anthropic-client","title":"Anthropic Client","text":""},{"location":"api/llm/#rlm.llm.anthropic_client.AnthropicClient","title":"rlm.llm.anthropic_client.AnthropicClient","text":"<p>               Bases: <code>BaseLLMClient</code></p> <p>Anthropic API client for Claude models.</p> <p>Supports both regular and streaming completions.</p> Example <p>client = AnthropicClient(api_key=\"sk-...\", model=\"claude-3-sonnet-20240229\") response = client.complete([Message(role=\"user\", content=\"Hello!\")]) print(response.content)</p> Source code in <code>src/rlm/llm/anthropic_client.py</code> <pre><code>class AnthropicClient(BaseLLMClient):\n    \"\"\"\n    Anthropic API client for Claude models.\n\n    Supports both regular and streaming completions.\n\n    Example:\n        &gt;&gt;&gt; client = AnthropicClient(api_key=\"sk-...\", model=\"claude-3-sonnet-20240229\")\n        &gt;&gt;&gt; response = client.complete([Message(role=\"user\", content=\"Hello!\")])\n        &gt;&gt;&gt; print(response.content)\n    \"\"\"\n\n    DEFAULT_MODEL = \"claude-3-sonnet-20240229\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str = DEFAULT_MODEL,\n        temperature: float = 0.0,\n        max_tokens: int = 4096,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Anthropic client.\n\n        Args:\n            api_key: Anthropic API key\n            model: Model name (default: claude-3-sonnet)\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens in response\n        \"\"\"\n        super().__init__(api_key, model, temperature, max_tokens)\n        self._client = anthropic.Anthropic(api_key=api_key)\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"anthropic\"\n\n    def _convert_messages(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n    ) -&gt; tuple[Optional[str], list[dict]]:\n        \"\"\"\n        Convert messages to Anthropic's format.\n\n        Anthropic uses a separate system parameter instead of a message.\n\n        Returns:\n            Tuple of (system_prompt, messages)\n        \"\"\"\n        system = system_prompt\n        converted = []\n\n        for msg in messages:\n            role = msg.role.value if hasattr(msg.role, \"value\") else msg.role\n\n            # Extract system messages\n            if role == \"system\":\n                system = msg.content\n                continue\n\n            converted.append({\n                \"role\": role,\n                \"content\": msg.content,\n            })\n\n        return system, converted\n\n    def complete(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; LLMResponse:\n        \"\"\"\n        Generate a completion using Anthropic's API.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments\n\n        Returns:\n            LLMResponse with the generated content\n        \"\"\"\n        system, converted_messages = self._convert_messages(messages, system_prompt)\n\n        try:\n            create_kwargs = {\n                \"model\": self.model,\n                \"messages\": converted_messages,\n                \"temperature\": self.temperature,\n                \"max_tokens\": self.max_tokens,\n                **kwargs,\n            }\n\n            if system:\n                create_kwargs[\"system\"] = system\n\n            response = self._client.messages.create(**create_kwargs)\n\n            # Extract content from response\n            content = \"\"\n            if response.content:\n                content = response.content[0].text\n\n            return LLMResponse(\n                content=content,\n                model=response.model,\n                usage=TokenUsage(\n                    prompt_tokens=response.usage.input_tokens,\n                    completion_tokens=response.usage.output_tokens,\n                    total_tokens=response.usage.input_tokens + response.usage.output_tokens,\n                ),\n                finish_reason=response.stop_reason,\n                raw_response=response.model_dump() if hasattr(response, \"model_dump\") else None,\n            )\n\n        except Exception as e:\n            logger.error(f\"Anthropic API error: {e}\")\n            raise LLMError(\n                message=f\"Anthropic API request failed: {e}\",\n                provider=\"anthropic\",\n            ) from e\n\n    def stream(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"\n        Stream a completion from Anthropic.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments\n\n        Yields:\n            Chunks of the generated content\n        \"\"\"\n        system, converted_messages = self._convert_messages(messages, system_prompt)\n\n        try:\n            create_kwargs = {\n                \"model\": self.model,\n                \"messages\": converted_messages,\n                \"temperature\": self.temperature,\n                \"max_tokens\": self.max_tokens,\n                **kwargs,\n            }\n\n            if system:\n                create_kwargs[\"system\"] = system\n\n            with self._client.messages.stream(**create_kwargs) as stream:\n                for text in stream.text_stream:\n                    yield text\n\n        except Exception as e:\n            logger.error(f\"Anthropic streaming error: {e}\")\n            raise LLMError(\n                message=f\"Anthropic streaming failed: {e}\",\n                provider=\"anthropic\",\n            ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.anthropic_client.AnthropicClient.__init__","title":"__init__","text":"<pre><code>__init__(api_key: str, model: str = DEFAULT_MODEL, temperature: float = 0.0, max_tokens: int = 4096) -&gt; None\n</code></pre> <p>Initialize the Anthropic client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Anthropic API key</p> required <code>model</code> <code>str</code> <p>Model name (default: claude-3-sonnet)</p> <code>DEFAULT_MODEL</code> <code>temperature</code> <code>float</code> <p>Sampling temperature</p> <code>0.0</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens in response</p> <code>4096</code> Source code in <code>src/rlm/llm/anthropic_client.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.0,\n    max_tokens: int = 4096,\n) -&gt; None:\n    \"\"\"\n    Initialize the Anthropic client.\n\n    Args:\n        api_key: Anthropic API key\n        model: Model name (default: claude-3-sonnet)\n        temperature: Sampling temperature\n        max_tokens: Maximum tokens in response\n    \"\"\"\n    super().__init__(api_key, model, temperature, max_tokens)\n    self._client = anthropic.Anthropic(api_key=api_key)\n</code></pre>"},{"location":"api/llm/#rlm.llm.anthropic_client.AnthropicClient.complete","title":"complete","text":"<pre><code>complete(messages: list[Message], system_prompt: Optional[str] = None, **kwargs) -&gt; LLMResponse\n</code></pre> <p>Generate a completion using Anthropic's API.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with the generated content</p> Source code in <code>src/rlm/llm/anthropic_client.py</code> <pre><code>def complete(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; LLMResponse:\n    \"\"\"\n    Generate a completion using Anthropic's API.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments\n\n    Returns:\n        LLMResponse with the generated content\n    \"\"\"\n    system, converted_messages = self._convert_messages(messages, system_prompt)\n\n    try:\n        create_kwargs = {\n            \"model\": self.model,\n            \"messages\": converted_messages,\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            **kwargs,\n        }\n\n        if system:\n            create_kwargs[\"system\"] = system\n\n        response = self._client.messages.create(**create_kwargs)\n\n        # Extract content from response\n        content = \"\"\n        if response.content:\n            content = response.content[0].text\n\n        return LLMResponse(\n            content=content,\n            model=response.model,\n            usage=TokenUsage(\n                prompt_tokens=response.usage.input_tokens,\n                completion_tokens=response.usage.output_tokens,\n                total_tokens=response.usage.input_tokens + response.usage.output_tokens,\n            ),\n            finish_reason=response.stop_reason,\n            raw_response=response.model_dump() if hasattr(response, \"model_dump\") else None,\n        )\n\n    except Exception as e:\n        logger.error(f\"Anthropic API error: {e}\")\n        raise LLMError(\n            message=f\"Anthropic API request failed: {e}\",\n            provider=\"anthropic\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.anthropic_client.AnthropicClient.stream","title":"stream","text":"<pre><code>stream(messages: list[Message], system_prompt: Optional[str] = None, **kwargs) -&gt; Iterator[str]\n</code></pre> <p>Stream a completion from Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of the generated content</p> Source code in <code>src/rlm/llm/anthropic_client.py</code> <pre><code>def stream(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Stream a completion from Anthropic.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments\n\n    Yields:\n        Chunks of the generated content\n    \"\"\"\n    system, converted_messages = self._convert_messages(messages, system_prompt)\n\n    try:\n        create_kwargs = {\n            \"model\": self.model,\n            \"messages\": converted_messages,\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            **kwargs,\n        }\n\n        if system:\n            create_kwargs[\"system\"] = system\n\n        with self._client.messages.stream(**create_kwargs) as stream:\n            for text in stream.text_stream:\n                yield text\n\n    except Exception as e:\n        logger.error(f\"Anthropic streaming error: {e}\")\n        raise LLMError(\n            message=f\"Anthropic streaming failed: {e}\",\n            provider=\"anthropic\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#google-client","title":"Google Client","text":""},{"location":"api/llm/#rlm.llm.google_client.GoogleClient","title":"rlm.llm.google_client.GoogleClient","text":"<p>               Bases: <code>BaseLLMClient</code></p> <p>Google Generative AI client for Gemini models.</p> <p>Supports both regular and streaming completions.</p> Example <p>client = GoogleClient(api_key=\"...\", model=\"gemini-1.5-pro\") response = client.complete([Message(role=\"user\", content=\"Hello!\")]) print(response.content)</p> Source code in <code>src/rlm/llm/google_client.py</code> <pre><code>class GoogleClient(BaseLLMClient):\n    \"\"\"\n    Google Generative AI client for Gemini models.\n\n    Supports both regular and streaming completions.\n\n    Example:\n        &gt;&gt;&gt; client = GoogleClient(api_key=\"...\", model=\"gemini-1.5-pro\")\n        &gt;&gt;&gt; response = client.complete([Message(role=\"user\", content=\"Hello!\")])\n        &gt;&gt;&gt; print(response.content)\n    \"\"\"\n\n    DEFAULT_MODEL = \"gemini-1.5-pro\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str = DEFAULT_MODEL,\n        temperature: float = 0.0,\n        max_tokens: int = 4096,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Google client.\n\n        Args:\n            api_key: Google API key\n            model: Model name (default: gemini-1.5-pro)\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens in response\n        \"\"\"\n        super().__init__(api_key, model, temperature, max_tokens)\n\n        genai.configure(api_key=api_key)\n        self._model = genai.GenerativeModel(model)\n        self._generation_config = GenerationConfig(\n            temperature=temperature,\n            max_output_tokens=max_tokens,\n        )\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"google\"\n\n    def _convert_messages(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n    ) -&gt; tuple[Optional[str], list[dict]]:\n        \"\"\"\n        Convert messages to Google's format.\n\n        Google uses 'user' and 'model' roles.\n        \"\"\"\n        system = system_prompt\n        converted = []\n\n        for msg in messages:\n            role = msg.role.value if hasattr(msg.role, \"value\") else msg.role\n\n            if role == \"system\":\n                system = msg.content\n                continue\n\n            # Map assistant to model\n            google_role = \"model\" if role == \"assistant\" else \"user\"\n\n            converted.append({\n                \"role\": google_role,\n                \"parts\": [msg.content],\n            })\n\n        return system, converted\n\n    def complete(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; LLMResponse:\n        \"\"\"\n        Generate a completion using Google's API.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments\n\n        Returns:\n            LLMResponse with the generated content\n        \"\"\"\n        system, converted_messages = self._convert_messages(messages, system_prompt)\n\n        try:\n            # Rebuild model with system instruction if provided\n            if system:\n                model = genai.GenerativeModel(\n                    self.model,\n                    system_instruction=system,\n                )\n            else:\n                model = self._model\n\n            response = model.generate_content(\n                converted_messages,\n                generation_config=self._generation_config,\n                **kwargs,\n            )\n\n            # Extract usage information\n            usage = TokenUsage()\n            if hasattr(response, \"usage_metadata\") and response.usage_metadata:\n                usage = TokenUsage(\n                    prompt_tokens=getattr(response.usage_metadata, \"prompt_token_count\", 0),\n                    completion_tokens=getattr(response.usage_metadata, \"candidates_token_count\", 0),\n                    total_tokens=getattr(response.usage_metadata, \"total_token_count\", 0),\n                )\n\n            return LLMResponse(\n                content=response.text if response.text else \"\",\n                model=self.model,\n                usage=usage,\n                finish_reason=str(response.candidates[0].finish_reason) if response.candidates else None,\n                raw_response=None,  # Google's response doesn't have a simple dict export\n            )\n\n        except Exception as e:\n            logger.error(f\"Google API error: {e}\")\n            raise LLMError(\n                message=f\"Google API request failed: {e}\",\n                provider=\"google\",\n            ) from e\n\n    def stream(\n        self,\n        messages: list[Message],\n        system_prompt: Optional[str] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"\n        Stream a completion from Google.\n\n        Args:\n            messages: Conversation history\n            system_prompt: Optional system prompt\n            **kwargs: Additional arguments\n\n        Yields:\n            Chunks of the generated content\n        \"\"\"\n        system, converted_messages = self._convert_messages(messages, system_prompt)\n\n        try:\n            if system:\n                model = genai.GenerativeModel(\n                    self.model,\n                    system_instruction=system,\n                )\n            else:\n                model = self._model\n\n            response = model.generate_content(\n                converted_messages,\n                generation_config=self._generation_config,\n                stream=True,\n                **kwargs,\n            )\n\n            for chunk in response:\n                if chunk.text:\n                    yield chunk.text\n\n        except Exception as e:\n            logger.error(f\"Google streaming error: {e}\")\n            raise LLMError(\n                message=f\"Google streaming failed: {e}\",\n                provider=\"google\",\n            ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.google_client.GoogleClient.__init__","title":"__init__","text":"<pre><code>__init__(api_key: str, model: str = DEFAULT_MODEL, temperature: float = 0.0, max_tokens: int = 4096) -&gt; None\n</code></pre> <p>Initialize the Google client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Google API key</p> required <code>model</code> <code>str</code> <p>Model name (default: gemini-1.5-pro)</p> <code>DEFAULT_MODEL</code> <code>temperature</code> <code>float</code> <p>Sampling temperature</p> <code>0.0</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens in response</p> <code>4096</code> Source code in <code>src/rlm/llm/google_client.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.0,\n    max_tokens: int = 4096,\n) -&gt; None:\n    \"\"\"\n    Initialize the Google client.\n\n    Args:\n        api_key: Google API key\n        model: Model name (default: gemini-1.5-pro)\n        temperature: Sampling temperature\n        max_tokens: Maximum tokens in response\n    \"\"\"\n    super().__init__(api_key, model, temperature, max_tokens)\n\n    genai.configure(api_key=api_key)\n    self._model = genai.GenerativeModel(model)\n    self._generation_config = GenerationConfig(\n        temperature=temperature,\n        max_output_tokens=max_tokens,\n    )\n</code></pre>"},{"location":"api/llm/#rlm.llm.google_client.GoogleClient.complete","title":"complete","text":"<pre><code>complete(messages: list[Message], system_prompt: Optional[str] = None, **kwargs) -&gt; LLMResponse\n</code></pre> <p>Generate a completion using Google's API.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with the generated content</p> Source code in <code>src/rlm/llm/google_client.py</code> <pre><code>def complete(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; LLMResponse:\n    \"\"\"\n    Generate a completion using Google's API.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments\n\n    Returns:\n        LLMResponse with the generated content\n    \"\"\"\n    system, converted_messages = self._convert_messages(messages, system_prompt)\n\n    try:\n        # Rebuild model with system instruction if provided\n        if system:\n            model = genai.GenerativeModel(\n                self.model,\n                system_instruction=system,\n            )\n        else:\n            model = self._model\n\n        response = model.generate_content(\n            converted_messages,\n            generation_config=self._generation_config,\n            **kwargs,\n        )\n\n        # Extract usage information\n        usage = TokenUsage()\n        if hasattr(response, \"usage_metadata\") and response.usage_metadata:\n            usage = TokenUsage(\n                prompt_tokens=getattr(response.usage_metadata, \"prompt_token_count\", 0),\n                completion_tokens=getattr(response.usage_metadata, \"candidates_token_count\", 0),\n                total_tokens=getattr(response.usage_metadata, \"total_token_count\", 0),\n            )\n\n        return LLMResponse(\n            content=response.text if response.text else \"\",\n            model=self.model,\n            usage=usage,\n            finish_reason=str(response.candidates[0].finish_reason) if response.candidates else None,\n            raw_response=None,  # Google's response doesn't have a simple dict export\n        )\n\n    except Exception as e:\n        logger.error(f\"Google API error: {e}\")\n        raise LLMError(\n            message=f\"Google API request failed: {e}\",\n            provider=\"google\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#rlm.llm.google_client.GoogleClient.stream","title":"stream","text":"<pre><code>stream(messages: list[Message], system_prompt: Optional[str] = None, **kwargs) -&gt; Iterator[str]\n</code></pre> <p>Stream a completion from Google.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation history</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of the generated content</p> Source code in <code>src/rlm/llm/google_client.py</code> <pre><code>def stream(\n    self,\n    messages: list[Message],\n    system_prompt: Optional[str] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Stream a completion from Google.\n\n    Args:\n        messages: Conversation history\n        system_prompt: Optional system prompt\n        **kwargs: Additional arguments\n\n    Yields:\n        Chunks of the generated content\n    \"\"\"\n    system, converted_messages = self._convert_messages(messages, system_prompt)\n\n    try:\n        if system:\n            model = genai.GenerativeModel(\n                self.model,\n                system_instruction=system,\n            )\n        else:\n            model = self._model\n\n        response = model.generate_content(\n            converted_messages,\n            generation_config=self._generation_config,\n            stream=True,\n            **kwargs,\n        )\n\n        for chunk in response:\n            if chunk.text:\n                yield chunk.text\n\n    except Exception as e:\n        logger.error(f\"Google streaming error: {e}\")\n        raise LLMError(\n            message=f\"Google streaming failed: {e}\",\n            provider=\"google\",\n        ) from e\n</code></pre>"},{"location":"api/llm/#factory-function","title":"Factory Function","text":""},{"location":"api/llm/#rlm.llm.factory.create_llm_client","title":"rlm.llm.factory.create_llm_client","text":"<pre><code>create_llm_client(provider: Optional[Literal['openai', 'anthropic', 'google']] = None, api_key: Optional[str] = None, model: Optional[str] = None, **kwargs) -&gt; BaseLLMClient\n</code></pre> <p>Create an LLM client based on provider.</p> <p>Uses settings as defaults, but allows overrides.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[Literal['openai', 'anthropic', 'google']]</code> <p>LLM provider (default: from settings)</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key (default: from settings)</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model name (default: from settings)</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the client</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseLLMClient</code> <p>Configured LLM client</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If provider is unknown or API key is missing</p> Source code in <code>src/rlm/llm/factory.py</code> <pre><code>def create_llm_client(\n    provider: Optional[Literal[\"openai\", \"anthropic\", \"google\"]] = None,\n    api_key: Optional[str] = None,\n    model: Optional[str] = None,\n    **kwargs,\n) -&gt; BaseLLMClient:\n    \"\"\"\n    Create an LLM client based on provider.\n\n    Uses settings as defaults, but allows overrides.\n\n    Args:\n        provider: LLM provider (default: from settings)\n        api_key: API key (default: from settings)\n        model: Model name (default: from settings)\n        **kwargs: Additional arguments passed to the client\n\n    Returns:\n        Configured LLM client\n\n    Raises:\n        ConfigurationError: If provider is unknown or API key is missing\n    \"\"\"\n    provider = provider or settings.api_provider\n    api_key = api_key or settings.api_key.get_secret_value()\n    model = model or settings.model_name\n\n    if not api_key:\n        raise ConfigurationError(\n            message=f\"API key not configured for provider: {provider}\",\n            setting_name=\"api_key\",\n        )\n\n    if provider == \"openai\":\n        from rlm.llm.openai_client import OpenAIClient\n        return OpenAIClient(api_key=api_key, model=model, **kwargs)\n\n    elif provider == \"anthropic\":\n        from rlm.llm.anthropic_client import AnthropicClient\n        return AnthropicClient(api_key=api_key, model=model, **kwargs)\n\n    elif provider == \"google\":\n        from rlm.llm.google_client import GoogleClient\n        return GoogleClient(api_key=api_key, model=model, **kwargs)\n\n    else:\n        raise ConfigurationError(\n            message=f\"Unknown LLM provider: {provider}\",\n            setting_name=\"api_provider\",\n            details={\"valid_providers\": [\"openai\", \"anthropic\", \"google\"]},\n        )\n</code></pre>"},{"location":"api/orchestrator/","title":"Orchestrator","text":""},{"location":"api/orchestrator/#rlm.core.orchestrator.Orchestrator","title":"Orchestrator","text":"<p>Main orchestrator for RLM code execution (v3.0).</p> <p>v3.0 Architecture: - _execute_cycle() is the Single Source of Truth - arun() is the public async interface - run() is a pure sync wrapper (no duplicated logic) - CPU-bound egress filtering runs in ThreadPoolExecutor</p> Example <p>orchestrator = Orchestrator() result = orchestrator.run(\"What is 2+2?\")  # Sync result = await orchestrator.arun(\"What is 2+2?\")  # Async</p> Source code in <code>src/rlm/core/orchestrator.py</code> <pre><code>class Orchestrator:\n    \"\"\"\n    Main orchestrator for RLM code execution (v3.0).\n\n    v3.0 Architecture:\n    - _execute_cycle() is the Single Source of Truth\n    - arun() is the public async interface\n    - run() is a pure sync wrapper (no duplicated logic)\n    - CPU-bound egress filtering runs in ThreadPoolExecutor\n\n    Example:\n        &gt;&gt;&gt; orchestrator = Orchestrator()\n        &gt;&gt;&gt; result = orchestrator.run(\"What is 2+2?\")  # Sync\n        &gt;&gt;&gt; result = await orchestrator.arun(\"What is 2+2?\")  # Async\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_client: Optional[BaseLLMClient] = None,\n        sandbox: Optional[DockerSandbox] = None,\n        config: Optional[OrchestratorConfig] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the orchestrator.\"\"\"\n        self.config = config or OrchestratorConfig()\n        self._llm_client = llm_client\n        self._sandbox = sandbox\n        self.budget = BudgetManager()\n        self.egress_filter: Optional[EgressFilter] = None\n        self.history: list[Message] = []\n        self.steps: list[ExecutionStep] = []\n\n    @property\n    def llm(self) -&gt; BaseLLMClient:\n        \"\"\"Lazy-load LLM client.\"\"\"\n        if self._llm_client is None:\n            self._llm_client = create_llm_client()\n        return self._llm_client\n\n    @property\n    def sandbox(self) -&gt; DockerSandbox:\n        \"\"\"Lazy-load Docker sandbox.\"\"\"\n        if self._sandbox is None:\n            from rlm.core.repl.docker import SandboxConfig\n            sandbox_config = SandboxConfig(\n                allow_unsafe_runtime=self.config.allow_unsafe_runtime,\n            )\n            self._sandbox = DockerSandbox(config=sandbox_config)\n        return self._sandbox\n\n    def _get_system_prompt(self) -&gt; str:\n        \"\"\"Build the system prompt.\"\"\"\n        context_available = self.config.context_path is not None\n        return get_system_prompt(\n            mode=self.config.system_prompt_mode,\n            context_available=context_available,\n            custom_instructions=self.config.custom_instructions,\n        )\n\n    # ==================== Core Async Methods ====================\n\n    async def _acall_llm(self, iteration: int) -&gt; LLMResponse:\n        \"\"\"Call the LLM asynchronously.\"\"\"\n        system_prompt = self._get_system_prompt()\n\n        response = await self.llm.acomplete(\n            messages=self.history,\n            system_prompt=system_prompt,\n        )\n\n        self.budget.record_usage(\n            model=response.model,\n            input_tokens=response.usage.prompt_tokens,\n            output_tokens=response.usage.completion_tokens,\n        )\n\n        self.steps.append(ExecutionStep(\n            iteration=iteration,\n            action=\"llm_call\",\n            input_data=self.history[-1].content if self.history else \"\",\n            output_data=response.content,\n            success=True,\n        ))\n\n        return response\n\n    async def _aexecute_code(self, code: str, iteration: int) -&gt; ExecutionResult:\n        \"\"\"\n        Execute code asynchronously with CPU offloading for egress filter.\n\n        v3.0: Egress filtering runs in ThreadPoolExecutor to avoid blocking.\n        \"\"\"\n        context_mount = str(self.config.context_path) if self.config.context_path else None\n\n        try:\n            result = await self.sandbox.execute_async(code, context_mount=context_mount)\n\n            # v3.0: CPU OFFLOAD - Move heavy egress filtering to thread pool\n            if self.egress_filter:\n                loop = asyncio.get_running_loop()\n                filter_func = partial(\n                    self.egress_filter.filter,\n                    result.stdout,\n                    raise_on_leak=self.config.raise_on_leak,\n                )\n                result.stdout = await loop.run_in_executor(None, filter_func)\n\n            self.steps.append(ExecutionStep(\n                iteration=iteration,\n                action=\"code_execution\",\n                input_data=code,\n                output_data=result.stdout,\n                success=result.success,\n                error=result.stderr if not result.success else None,\n            ))\n\n            return result\n\n        except SandboxError as e:\n            self.steps.append(ExecutionStep(\n                iteration=iteration,\n                action=\"code_execution\",\n                input_data=code,\n                output_data=\"\",\n                success=False,\n                error=str(e),\n            ))\n            raise\n\n    # ==================== Single Source of Truth ====================\n\n    async def _execute_cycle(\n        self,\n        query: str,\n        context_path: Optional[Path],\n    ) -&gt; OrchestratorResult:\n        \"\"\"\n        Core logic (Single Source of Truth).\n\n        v3.0: All orchestration logic lives here. Both run() and arun()\n        delegate to this method, eliminating code duplication.\n\n        Contains the complete loop: LLM -&gt; Parse -&gt; Execute -&gt; Filter.\n        \"\"\"\n        # Reset state\n        self.history = []\n        self.steps = []\n\n        # Setup context\n        if context_path:\n            self.config.context_path = context_path\n            # Read context sample in thread to avoid blocking\n            loop = asyncio.get_running_loop()\n            context_sample = await loop.run_in_executor(\n                None,\n                lambda: Path(context_path).read_text(encoding=\"utf-8\", errors=\"replace\")[:5000]\n            )\n            self.egress_filter = EgressFilter(context=context_sample)\n        else:\n            self.egress_filter = EgressFilter()\n\n        # Add initial user message\n        self.history.append(Message(role=\"user\", content=query))\n\n        try:\n            for iteration in range(self.config.max_iterations):\n                logger.info(f\"Iteration {iteration + 1}/{self.config.max_iterations}\")\n\n                # 1. Call LLM\n                response = await self._acall_llm(iteration)\n\n                # 2. Check for final answer in LLM response\n                final_answer = extract_final_answer(response.content)\n                if final_answer:\n                    self.steps.append(ExecutionStep(\n                        iteration=iteration,\n                        action=\"final_answer\",\n                        input_data=response.content,\n                        output_data=final_answer,\n                        success=True,\n                    ))\n                    return OrchestratorResult(\n                        final_answer=final_answer,\n                        success=True,\n                        iterations=iteration + 1,\n                        steps=self.steps,\n                        budget_summary=self.budget.summary(),\n                    )\n\n                # Add assistant response to history\n                self.history.append(Message(role=\"assistant\", content=response.content))\n\n                # 3. Parse code blocks using strict mistletoe parser\n                code_blocks = extract_python_code(response.content)\n\n                if not code_blocks:\n                    # No code - might be final answer or needs more info\n                    if iteration &gt; 0:\n                        return OrchestratorResult(\n                            final_answer=response.content,\n                            success=True,\n                            iterations=iteration + 1,\n                            steps=self.steps,\n                            budget_summary=self.budget.summary(),\n                        )\n                    continue\n\n                # 4. Execute code blocks\n                combined_output = []\n                for code in code_blocks:\n                    result = await self._aexecute_code(code, iteration)\n\n                    if result.oom_killed:\n                        combined_output.append(\"Error: Memory Limit Exceeded (OOMKilled)\")\n                    elif result.timed_out:\n                        combined_output.append(\"Error: Execution Timeout\")\n                    elif not result.success:\n                        combined_output.append(f\"Error (exit {result.exit_code}):\\n{result.stderr}\")\n                    else:\n                        combined_output.append(result.stdout)\n\n                    # Check for final answer in code output\n                    final = extract_final_answer(result.stdout)\n                    if final:\n                        self.steps.append(ExecutionStep(\n                            iteration=iteration,\n                            action=\"final_answer\",\n                            input_data=result.stdout,\n                            output_data=final,\n                            success=True,\n                        ))\n                        return OrchestratorResult(\n                            final_answer=final,\n                            success=True,\n                            iterations=iteration + 1,\n                            steps=self.steps,\n                            budget_summary=self.budget.summary(),\n                        )\n\n                # 5. Add observation to history\n                observation = \"\\n---\\n\".join(combined_output)\n                self.history.append(Message(role=\"user\", content=f\"Observation:\\n{observation}\"))\n\n            # Max iterations reached\n            return OrchestratorResult(\n                final_answer=None,\n                success=False,\n                iterations=self.config.max_iterations,\n                steps=self.steps,\n                budget_summary=self.budget.summary(),\n                error=\"Max iterations reached without final answer\",\n            )\n\n        except BudgetExceededError as e:\n            return OrchestratorResult(\n                final_answer=None,\n                success=False,\n                iterations=len([s for s in self.steps if s.action == \"llm_call\"]),\n                steps=self.steps,\n                budget_summary=self.budget.summary(),\n                error=str(e),\n            )\n\n        except RLMError as e:\n            return OrchestratorResult(\n                final_answer=None,\n                success=False,\n                iterations=len([s for s in self.steps if s.action == \"llm_call\"]),\n                steps=self.steps,\n                budget_summary=self.budget.summary(),\n                error=str(e),\n            )\n\n    # ==================== Public Interfaces ====================\n\n    async def arun(\n        self,\n        query: str,\n        context_path: Optional[str | Path] = None,\n    ) -&gt; OrchestratorResult:\n        \"\"\"\n        Run the orchestration loop (asynchronous).\n\n        v3.0: Delegates to _execute_cycle() (Single Source of Truth).\n\n        Args:\n            query: User's question or task\n            context_path: Optional path to context file\n\n        Returns:\n            OrchestratorResult with the final answer\n        \"\"\"\n        path = Path(context_path) if context_path else None\n        return await self._execute_cycle(query, path)\n\n    def run(\n        self,\n        query: str,\n        context_path: Optional[str | Path] = None,\n    ) -&gt; OrchestratorResult:\n        \"\"\"\n        Run the orchestration loop (synchronous wrapper).\n\n        v3.0: Pure wrapper with NO duplicated logic.\n        Creates a disposable event loop or uses existing one safely.\n\n        Args:\n            query: User's question or task\n            context_path: Optional path to context file\n\n        Returns:\n            OrchestratorResult with the final answer\n        \"\"\"\n        try:\n            # Standard case: no event loop running\n            return asyncio.run(self.arun(query, context_path))\n        except RuntimeError:\n            # Fallback: event loop already running (e.g., Jupyter, some web frameworks)\n            # This requires the existing loop to be cooperative\n            loop = asyncio.get_event_loop()\n            if loop.is_running():\n                # Cannot use run_until_complete on running loop\n                # User should use arun() directly in async contexts\n                raise RuntimeError(\n                    \"Event loop is already running. Use 'await orchestrator.arun()' \"\n                    \"in async contexts, or use nest_asyncio if needed.\"\n                )\n            return loop.run_until_complete(self.arun(query, context_path))\n\n    def chat(self, message: str) -&gt; str:\n        \"\"\"Simple chat interface for one-off questions.\"\"\"\n        result = self.run(message)\n        return result.final_answer or (result.steps[-1].output_data if result.steps else \"\")\n</code></pre>"},{"location":"api/orchestrator/#rlm.core.orchestrator.Orchestrator.__init__","title":"__init__","text":"<pre><code>__init__(llm_client: Optional[BaseLLMClient] = None, sandbox: Optional[DockerSandbox] = None, config: Optional[OrchestratorConfig] = None) -&gt; None\n</code></pre> <p>Initialize the orchestrator.</p> Source code in <code>src/rlm/core/orchestrator.py</code> <pre><code>def __init__(\n    self,\n    llm_client: Optional[BaseLLMClient] = None,\n    sandbox: Optional[DockerSandbox] = None,\n    config: Optional[OrchestratorConfig] = None,\n) -&gt; None:\n    \"\"\"Initialize the orchestrator.\"\"\"\n    self.config = config or OrchestratorConfig()\n    self._llm_client = llm_client\n    self._sandbox = sandbox\n    self.budget = BudgetManager()\n    self.egress_filter: Optional[EgressFilter] = None\n    self.history: list[Message] = []\n    self.steps: list[ExecutionStep] = []\n</code></pre>"},{"location":"api/orchestrator/#rlm.core.orchestrator.Orchestrator.run","title":"run","text":"<pre><code>run(query: str, context_path: Optional[str | Path] = None) -&gt; OrchestratorResult\n</code></pre> <p>Run the orchestration loop (synchronous wrapper).</p> <p>v3.0: Pure wrapper with NO duplicated logic. Creates a disposable event loop or uses existing one safely.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>User's question or task</p> required <code>context_path</code> <code>Optional[str | Path]</code> <p>Optional path to context file</p> <code>None</code> <p>Returns:</p> Type Description <code>OrchestratorResult</code> <p>OrchestratorResult with the final answer</p> Source code in <code>src/rlm/core/orchestrator.py</code> <pre><code>def run(\n    self,\n    query: str,\n    context_path: Optional[str | Path] = None,\n) -&gt; OrchestratorResult:\n    \"\"\"\n    Run the orchestration loop (synchronous wrapper).\n\n    v3.0: Pure wrapper with NO duplicated logic.\n    Creates a disposable event loop or uses existing one safely.\n\n    Args:\n        query: User's question or task\n        context_path: Optional path to context file\n\n    Returns:\n        OrchestratorResult with the final answer\n    \"\"\"\n    try:\n        # Standard case: no event loop running\n        return asyncio.run(self.arun(query, context_path))\n    except RuntimeError:\n        # Fallback: event loop already running (e.g., Jupyter, some web frameworks)\n        # This requires the existing loop to be cooperative\n        loop = asyncio.get_event_loop()\n        if loop.is_running():\n            # Cannot use run_until_complete on running loop\n            # User should use arun() directly in async contexts\n            raise RuntimeError(\n                \"Event loop is already running. Use 'await orchestrator.arun()' \"\n                \"in async contexts, or use nest_asyncio if needed.\"\n            )\n        return loop.run_until_complete(self.arun(query, context_path))\n</code></pre>"},{"location":"api/orchestrator/#rlm.core.orchestrator.Orchestrator.arun","title":"arun  <code>async</code>","text":"<pre><code>arun(query: str, context_path: Optional[str | Path] = None) -&gt; OrchestratorResult\n</code></pre> <p>Run the orchestration loop (asynchronous).</p> <p>v3.0: Delegates to _execute_cycle() (Single Source of Truth).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>User's question or task</p> required <code>context_path</code> <code>Optional[str | Path]</code> <p>Optional path to context file</p> <code>None</code> <p>Returns:</p> Type Description <code>OrchestratorResult</code> <p>OrchestratorResult with the final answer</p> Source code in <code>src/rlm/core/orchestrator.py</code> <pre><code>async def arun(\n    self,\n    query: str,\n    context_path: Optional[str | Path] = None,\n) -&gt; OrchestratorResult:\n    \"\"\"\n    Run the orchestration loop (asynchronous).\n\n    v3.0: Delegates to _execute_cycle() (Single Source of Truth).\n\n    Args:\n        query: User's question or task\n        context_path: Optional path to context file\n\n    Returns:\n        OrchestratorResult with the final answer\n    \"\"\"\n    path = Path(context_path) if context_path else None\n    return await self._execute_cycle(query, path)\n</code></pre>"},{"location":"api/orchestrator/#rlm.core.orchestrator.Orchestrator.chat","title":"chat","text":"<pre><code>chat(message: str) -&gt; str\n</code></pre> <p>Simple chat interface for one-off questions.</p> Source code in <code>src/rlm/core/orchestrator.py</code> <pre><code>def chat(self, message: str) -&gt; str:\n    \"\"\"Simple chat interface for one-off questions.\"\"\"\n    result = self.run(message)\n    return result.final_answer or (result.steps[-1].output_data if result.steps else \"\")\n</code></pre>"},{"location":"api/orchestrator/#usage-examples","title":"Usage Examples","text":""},{"location":"api/orchestrator/#basic-async","title":"Basic Async","text":"<pre><code>import asyncio\nfrom rlm import Orchestrator\n\nasync def main():\n    agent = Orchestrator()\n    result = await agent.arun(\"Calculate sqrt(144)\")\n    print(result.final_answer)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/orchestrator/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code>from rlm import Orchestrator\nfrom rlm.core import OrchestratorConfig\n\nconfig = OrchestratorConfig(\n    max_iterations=5,\n    raise_on_leak=True,\n)\n\nagent = Orchestrator(config=config)\nresult = agent.run(\"What is 2+2?\")\n</code></pre>"},{"location":"api/orchestrator/#with-context-file","title":"With Context File","text":"<pre><code>result = await agent.arun(\n    query=\"Summarize this document\",\n    context_path=\"/path/to/document.txt\"\n)\n</code></pre>"},{"location":"api/orchestrator/#result-object","title":"Result Object","text":""},{"location":"api/orchestrator/#rlm.core.orchestrator.OrchestratorResult","title":"OrchestratorResult  <code>dataclass</code>","text":"<p>Result of an orchestration run.</p> Source code in <code>src/rlm/core/orchestrator.py</code> <pre><code>@dataclass\nclass OrchestratorResult:\n    \"\"\"Result of an orchestration run.\"\"\"\n\n    final_answer: Optional[str]\n    success: bool\n    iterations: int\n    steps: list[ExecutionStep]\n    budget_summary: dict\n    error: Optional[str] = None\n</code></pre>"},{"location":"api/orchestrator/#configuration","title":"Configuration","text":""},{"location":"api/orchestrator/#rlm.core.orchestrator.OrchestratorConfig","title":"OrchestratorConfig  <code>dataclass</code>","text":"<p>Configuration for the orchestrator.</p> Source code in <code>src/rlm/core/orchestrator.py</code> <pre><code>@dataclass\nclass OrchestratorConfig:\n    \"\"\"Configuration for the orchestrator.\"\"\"\n\n    max_iterations: int = field(default_factory=lambda: settings.max_recursion_depth)\n    context_path: Optional[Path] = None\n    system_prompt_mode: str = \"full\"\n    custom_instructions: Optional[str] = None\n    raise_on_leak: bool = False\n    allow_unsafe_runtime: bool = False\n</code></pre>"},{"location":"api/sandbox/","title":"Docker Sandbox","text":""},{"location":"api/sandbox/#rlm.core.repl.docker.DockerSandbox","title":"DockerSandbox","text":"<p>Hardened Docker sandbox for executing untrusted Python code.</p> <p>v2.1 Changes: - Clean Boot: agent_lib mounted as volume instead of string injection - Fail-Closed: Raises error if gVisor unavailable (unless explicitly allowed) - No ImportBlocker: Relies on OS-level isolation only</p> <p>Security features: - gVisor runtime (runsc) - REQUIRED by default - Network isolation (network_mode=\"none\") - Memory limits to prevent OOM attacks - Process limits to prevent fork bombs - CPU quotas to prevent crypto mining - Privilege escalation prevention - Read-only context mounting</p> Example <p>sandbox = DockerSandbox() result = sandbox.execute(\"print('Hello, World!')\") print(result.stdout) Hello, World!</p> Source code in <code>src/rlm/core/repl/docker.py</code> <pre><code>class DockerSandbox:\n    \"\"\"\n    Hardened Docker sandbox for executing untrusted Python code.\n\n    v2.1 Changes:\n    - Clean Boot: agent_lib mounted as volume instead of string injection\n    - Fail-Closed: Raises error if gVisor unavailable (unless explicitly allowed)\n    - No ImportBlocker: Relies on OS-level isolation only\n\n    Security features:\n    - gVisor runtime (runsc) - REQUIRED by default\n    - Network isolation (network_mode=\"none\")\n    - Memory limits to prevent OOM attacks\n    - Process limits to prevent fork bombs\n    - CPU quotas to prevent crypto mining\n    - Privilege escalation prevention\n    - Read-only context mounting\n\n    Example:\n        &gt;&gt;&gt; sandbox = DockerSandbox()\n        &gt;&gt;&gt; result = sandbox.execute(\"print('Hello, World!')\")\n        &gt;&gt;&gt; print(result.stdout)\n        Hello, World!\n    \"\"\"\n\n    def __init__(\n        self,\n        image: Optional[str] = None,\n        timeout: Optional[int] = None,\n        config: Optional[SandboxConfig] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Docker sandbox.\n\n        Args:\n            image: Docker image to use (default: from settings)\n            timeout: Execution timeout in seconds (default: from settings)\n            config: Full sandbox configuration (overrides individual params)\n        \"\"\"\n        self.config = config or SandboxConfig()\n        if image:\n            self.config.image = image\n        if timeout:\n            self.config.timeout = timeout\n\n        self._client: Optional[docker.DockerClient] = None\n        self._runtime: Optional[str] = None\n\n    @property\n    def client(self) -&gt; docker.DockerClient:\n        \"\"\"Lazy-load Docker client.\"\"\"\n        if self._client is None:\n            try:\n                self._client = docker.from_env()\n                # Verify Docker is accessible\n                self._client.ping()\n            except DockerException as e:\n                raise SandboxError(\n                    message=\"Failed to connect to Docker daemon\",\n                    details={\"error\": str(e)},\n                ) from e\n        return self._client\n\n    @property\n    def runtime(self) -&gt; str:\n        \"\"\"Detect and cache the best available runtime.\"\"\"\n        if self._runtime is None:\n            self._runtime = self._detect_runtime()\n        return self._runtime\n\n    def _detect_runtime(self) -&gt; str:\n        \"\"\"\n        Detect the most secure available Docker runtime.\n\n        v2.1: Fail-closed - raises error if gVisor not available\n        unless allow_unsafe_runtime is set.\n\n        Returns:\n            Runtime name to use.\n\n        Raises:\n            SecurityViolationError: If gVisor not found and unsafe not allowed.\n        \"\"\"\n        if self.config.runtime != \"auto\":\n            logger.info(f\"Using configured runtime: {self.config.runtime}\")\n            return self.config.runtime\n\n        try:\n            info = self.client.info()\n            runtimes = info.get(\"Runtimes\", {})\n\n            if \"runsc\" in runtimes:\n                logger.info(\"\u2713 Secure runtime 'runsc' (gVisor) detected and enabled.\")\n                return \"runsc\"\n            else:\n                # v2.1: Fail-closed security\n                if self.config.allow_unsafe_runtime:\n                    logger.warning(\n                        \"\u26a0 SECURITY WARNING: gVisor (runsc) not found! \"\n                        \"Using standard 'runc' isolation. \"\n                        \"This is explicitly allowed but NOT RECOMMENDED for production.\"\n                    )\n                    return \"runc\"\n                else:\n                    raise SecurityViolationError(\n                        \"gVisor (runsc) not found! Execution aborted for security. \"\n                        \"Install gVisor or set RLM_ALLOW_UNSAFE_RUNTIME=1 to allow \"\n                        \"execution with reduced isolation (NOT RECOMMENDED).\",\n                        details={\"available_runtimes\": list(runtimes.keys())},\n                    )\n\n        except SecurityViolationError:\n            raise\n        except Exception as e:\n            logger.error(f\"Failed to detect Docker runtimes: {e}\")\n            if self.config.allow_unsafe_runtime:\n                return \"runc\"\n            raise SecurityViolationError(\n                f\"Failed to verify secure runtime: {e}\",\n                details={\"error\": str(e)},\n            ) from e\n\n    def _ensure_image(self) -&gt; None:\n        \"\"\"Pull the Docker image if not available locally.\"\"\"\n        try:\n            self.client.images.get(self.config.image)\n        except ImageNotFound:\n            logger.info(f\"Pulling Docker image: {self.config.image}\")\n            self.client.images.pull(self.config.image)\n\n    def execute(\n        self,\n        code: str,\n        context_mount: Optional[str] = None,\n    ) -&gt; ExecutionResult:\n        \"\"\"\n        Execute Python code in a secure Docker container.\n\n        v3.0: Uses TemporaryDirectory for crash-safe cleanup.\n        No temp file leaks even on SIGKILL.\n\n        Args:\n            code: Python code to execute\n            context_mount: Optional path to context file to mount read-only\n\n        Returns:\n            ExecutionResult with stdout, stderr, and exit code\n\n        Raises:\n            SandboxError: If container execution fails\n            SecurityViolationError: If security configuration is invalid\n        \"\"\"\n        self._ensure_image()\n\n        # v3.0: TemporaryDirectory for crash-safe cleanup\n        with tempfile.TemporaryDirectory(prefix=\"rlm_exec_\") as tmpdir:\n            script_path = Path(tmpdir) / \"user_code.py\"\n            script_path.write_text(code, encoding=\"utf-8\")\n\n            try:\n                # Configure volumes - CLEAN BOOT: mount agent_lib as volume\n                volumes = {\n                    # Mount agent_lib as read-only Python package\n                    str(AGENT_LIB_PATH): {\"bind\": \"/opt/rlm_agent_lib\", \"mode\": \"ro\"},\n                    # Mount user script\n                    str(script_path): {\"bind\": \"/tmp/user_code.py\", \"mode\": \"ro\"},\n                }\n\n                if context_mount:\n                    volumes[context_mount] = {\"bind\": \"/mnt/context\", \"mode\": \"ro\"}\n\n                # Configure network\n                network_mode = \"bridge\" if self.config.network_enabled else \"none\"\n                if self.config.network_enabled:\n                    logger.warning(\"\u26a0 Network access enabled - this is a security risk!\")\n\n                # Security options\n                security_opt = [\"no-new-privileges:true\"]\n\n                # CPU configuration (nano_cpus = 10^9 * cores)\n                nano_cpus = int(self.config.cpu_limit * 1_000_000_000)\n\n                logger.debug(\n                    f\"Executing code in sandbox (runtime={self.runtime}, \"\n                    f\"network={network_mode}, mem={self.config.memory_limit})\"\n                )\n\n                # Build the boot command\n                command = [\n                    \"sh\", \"-c\",\n                    \"export PYTHONPATH=/opt:$PYTHONPATH &amp;&amp; \"\n                    \"python3 -c \\\"\"\n                    \"import sys; sys.path.insert(0, '/opt'); \"\n                    \"from rlm_agent_lib.boot import setup_environment, execute_code; \"\n                    \"env = setup_environment(); \"\n                    \"code = open('/tmp/user_code.py').read(); \"\n                    \"execute_code(code, env)\"\n                    \"\\\"\"\n                ]\n\n                # Run the container\n                container = self.client.containers.run(\n                    image=self.config.image,\n                    command=command,\n                    detach=True,\n                    runtime=self.runtime,\n                    network_mode=network_mode,\n                    mem_limit=self.config.memory_limit,\n                    memswap_limit=self.config.memory_limit,\n                    nano_cpus=nano_cpus,\n                    pids_limit=self.config.pids_limit,\n                    security_opt=security_opt,\n                    ipc_mode=\"none\",\n                    volumes=volumes,\n                    environment={\n                        \"PYTHONPATH\": \"/opt\",\n                        \"PYTHONDONTWRITEBYTECODE\": \"1\",\n                    },\n                    remove=False,\n                )\n\n                try:\n                    result = container.wait(timeout=self.config.timeout)\n                    exit_code = result.get(\"StatusCode\", -1)\n                    timed_out = False\n                except Exception:\n                    logger.warning(\"Container execution timed out, killing...\")\n                    container.kill()\n                    exit_code = 124\n                    timed_out = True\n\n                # Get logs\n                stdout = container.logs(stdout=True, stderr=False).decode(\"utf-8\", errors=\"replace\")\n                stderr = container.logs(stdout=False, stderr=True).decode(\"utf-8\", errors=\"replace\")\n\n                # Check for OOM\n                container.reload()\n                oom_killed = container.attrs.get(\"State\", {}).get(\"OOMKilled\", False)\n\n                # Cleanup container\n                container.remove(force=True)\n\n                # Truncate output for safety\n                max_bytes = settings.max_stdout_bytes\n                if len(stdout) &gt; max_bytes:\n                    head = stdout[:1000]\n                    tail = stdout[-3000:]\n                    truncated = len(stdout) - max_bytes\n                    stdout = f\"{head}\\n... [TRUNCATED {truncated} bytes] ...\\n{tail}\"\n\n                return ExecutionResult(\n                    stdout=stdout,\n                    stderr=stderr,\n                    exit_code=exit_code,\n                    timed_out=timed_out,\n                    oom_killed=oom_killed,\n                )\n\n            except ContainerError as e:\n                raise SandboxError(\n                    message=\"Container execution failed\",\n                    exit_code=e.exit_status,\n                    stderr=str(e.stderr),\n                ) from e\n\n            except DockerException as e:\n                raise SandboxError(\n                    message=\"Docker error during execution\",\n                    details={\"error\": str(e)},\n                ) from e\n\n        # Temp directory auto-cleaned here\n\n    async def execute_async(\n        self,\n        code: str,\n        context_mount: Optional[str] = None,\n    ) -&gt; ExecutionResult:\n        \"\"\"\n        Execute Python code asynchronously.\n\n        This runs the synchronous execute() in a thread pool to avoid\n        blocking the event loop. For true async Docker, use aiodocker.\n\n        Args:\n            code: Python code to execute\n            context_mount: Optional path to context file\n\n        Returns:\n            ExecutionResult with stdout, stderr, and exit code\n        \"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            None,\n            lambda: self.execute(code, context_mount),\n        )\n\n    def validate_security(self) -&gt; dict:\n        \"\"\"\n        Validate the security configuration.\n\n        Returns:\n            Dictionary with security checks and their status.\n        \"\"\"\n        checks = {\n            \"docker_available\": False,\n            \"gvisor_available\": False,\n            \"gvisor_required\": not self.config.allow_unsafe_runtime,\n            \"network_disabled\": not self.config.network_enabled,\n            \"memory_limited\": bool(self.config.memory_limit),\n            \"pids_limited\": self.config.pids_limit &lt; 100,\n            \"agent_lib_available\": AGENT_LIB_PATH.exists(),\n        }\n\n        try:\n            self.client.ping()\n            checks[\"docker_available\"] = True\n\n            info = self.client.info()\n            runtimes = info.get(\"Runtimes\", {})\n            checks[\"gvisor_available\"] = \"runsc\" in runtimes\n        except Exception:\n            pass\n\n        # Overall security status\n        checks[\"secure\"] = (\n            checks[\"docker_available\"]\n            and (checks[\"gvisor_available\"] or self.config.allow_unsafe_runtime)\n            and checks[\"network_disabled\"]\n            and checks[\"memory_limited\"]\n            and checks[\"agent_lib_available\"]\n        )\n\n        return checks\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.docker.DockerSandbox.__init__","title":"__init__","text":"<pre><code>__init__(image: Optional[str] = None, timeout: Optional[int] = None, config: Optional[SandboxConfig] = None) -&gt; None\n</code></pre> <p>Initialize the Docker sandbox.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Optional[str]</code> <p>Docker image to use (default: from settings)</p> <code>None</code> <code>timeout</code> <code>Optional[int]</code> <p>Execution timeout in seconds (default: from settings)</p> <code>None</code> <code>config</code> <code>Optional[SandboxConfig]</code> <p>Full sandbox configuration (overrides individual params)</p> <code>None</code> Source code in <code>src/rlm/core/repl/docker.py</code> <pre><code>def __init__(\n    self,\n    image: Optional[str] = None,\n    timeout: Optional[int] = None,\n    config: Optional[SandboxConfig] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the Docker sandbox.\n\n    Args:\n        image: Docker image to use (default: from settings)\n        timeout: Execution timeout in seconds (default: from settings)\n        config: Full sandbox configuration (overrides individual params)\n    \"\"\"\n    self.config = config or SandboxConfig()\n    if image:\n        self.config.image = image\n    if timeout:\n        self.config.timeout = timeout\n\n    self._client: Optional[docker.DockerClient] = None\n    self._runtime: Optional[str] = None\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.docker.DockerSandbox.execute","title":"execute","text":"<pre><code>execute(code: str, context_mount: Optional[str] = None) -&gt; ExecutionResult\n</code></pre> <p>Execute Python code in a secure Docker container.</p> <p>v3.0: Uses TemporaryDirectory for crash-safe cleanup. No temp file leaks even on SIGKILL.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Python code to execute</p> required <code>context_mount</code> <code>Optional[str]</code> <p>Optional path to context file to mount read-only</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with stdout, stderr, and exit code</p> <p>Raises:</p> Type Description <code>SandboxError</code> <p>If container execution fails</p> <code>SecurityViolationError</code> <p>If security configuration is invalid</p> Source code in <code>src/rlm/core/repl/docker.py</code> <pre><code>def execute(\n    self,\n    code: str,\n    context_mount: Optional[str] = None,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute Python code in a secure Docker container.\n\n    v3.0: Uses TemporaryDirectory for crash-safe cleanup.\n    No temp file leaks even on SIGKILL.\n\n    Args:\n        code: Python code to execute\n        context_mount: Optional path to context file to mount read-only\n\n    Returns:\n        ExecutionResult with stdout, stderr, and exit code\n\n    Raises:\n        SandboxError: If container execution fails\n        SecurityViolationError: If security configuration is invalid\n    \"\"\"\n    self._ensure_image()\n\n    # v3.0: TemporaryDirectory for crash-safe cleanup\n    with tempfile.TemporaryDirectory(prefix=\"rlm_exec_\") as tmpdir:\n        script_path = Path(tmpdir) / \"user_code.py\"\n        script_path.write_text(code, encoding=\"utf-8\")\n\n        try:\n            # Configure volumes - CLEAN BOOT: mount agent_lib as volume\n            volumes = {\n                # Mount agent_lib as read-only Python package\n                str(AGENT_LIB_PATH): {\"bind\": \"/opt/rlm_agent_lib\", \"mode\": \"ro\"},\n                # Mount user script\n                str(script_path): {\"bind\": \"/tmp/user_code.py\", \"mode\": \"ro\"},\n            }\n\n            if context_mount:\n                volumes[context_mount] = {\"bind\": \"/mnt/context\", \"mode\": \"ro\"}\n\n            # Configure network\n            network_mode = \"bridge\" if self.config.network_enabled else \"none\"\n            if self.config.network_enabled:\n                logger.warning(\"\u26a0 Network access enabled - this is a security risk!\")\n\n            # Security options\n            security_opt = [\"no-new-privileges:true\"]\n\n            # CPU configuration (nano_cpus = 10^9 * cores)\n            nano_cpus = int(self.config.cpu_limit * 1_000_000_000)\n\n            logger.debug(\n                f\"Executing code in sandbox (runtime={self.runtime}, \"\n                f\"network={network_mode}, mem={self.config.memory_limit})\"\n            )\n\n            # Build the boot command\n            command = [\n                \"sh\", \"-c\",\n                \"export PYTHONPATH=/opt:$PYTHONPATH &amp;&amp; \"\n                \"python3 -c \\\"\"\n                \"import sys; sys.path.insert(0, '/opt'); \"\n                \"from rlm_agent_lib.boot import setup_environment, execute_code; \"\n                \"env = setup_environment(); \"\n                \"code = open('/tmp/user_code.py').read(); \"\n                \"execute_code(code, env)\"\n                \"\\\"\"\n            ]\n\n            # Run the container\n            container = self.client.containers.run(\n                image=self.config.image,\n                command=command,\n                detach=True,\n                runtime=self.runtime,\n                network_mode=network_mode,\n                mem_limit=self.config.memory_limit,\n                memswap_limit=self.config.memory_limit,\n                nano_cpus=nano_cpus,\n                pids_limit=self.config.pids_limit,\n                security_opt=security_opt,\n                ipc_mode=\"none\",\n                volumes=volumes,\n                environment={\n                    \"PYTHONPATH\": \"/opt\",\n                    \"PYTHONDONTWRITEBYTECODE\": \"1\",\n                },\n                remove=False,\n            )\n\n            try:\n                result = container.wait(timeout=self.config.timeout)\n                exit_code = result.get(\"StatusCode\", -1)\n                timed_out = False\n            except Exception:\n                logger.warning(\"Container execution timed out, killing...\")\n                container.kill()\n                exit_code = 124\n                timed_out = True\n\n            # Get logs\n            stdout = container.logs(stdout=True, stderr=False).decode(\"utf-8\", errors=\"replace\")\n            stderr = container.logs(stdout=False, stderr=True).decode(\"utf-8\", errors=\"replace\")\n\n            # Check for OOM\n            container.reload()\n            oom_killed = container.attrs.get(\"State\", {}).get(\"OOMKilled\", False)\n\n            # Cleanup container\n            container.remove(force=True)\n\n            # Truncate output for safety\n            max_bytes = settings.max_stdout_bytes\n            if len(stdout) &gt; max_bytes:\n                head = stdout[:1000]\n                tail = stdout[-3000:]\n                truncated = len(stdout) - max_bytes\n                stdout = f\"{head}\\n... [TRUNCATED {truncated} bytes] ...\\n{tail}\"\n\n            return ExecutionResult(\n                stdout=stdout,\n                stderr=stderr,\n                exit_code=exit_code,\n                timed_out=timed_out,\n                oom_killed=oom_killed,\n            )\n\n        except ContainerError as e:\n            raise SandboxError(\n                message=\"Container execution failed\",\n                exit_code=e.exit_status,\n                stderr=str(e.stderr),\n            ) from e\n\n        except DockerException as e:\n            raise SandboxError(\n                message=\"Docker error during execution\",\n                details={\"error\": str(e)},\n            ) from e\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.docker.DockerSandbox.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(code: str, context_mount: Optional[str] = None) -&gt; ExecutionResult\n</code></pre> <p>Execute Python code asynchronously.</p> <p>This runs the synchronous execute() in a thread pool to avoid blocking the event loop. For true async Docker, use aiodocker.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Python code to execute</p> required <code>context_mount</code> <code>Optional[str]</code> <p>Optional path to context file</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with stdout, stderr, and exit code</p> Source code in <code>src/rlm/core/repl/docker.py</code> <pre><code>async def execute_async(\n    self,\n    code: str,\n    context_mount: Optional[str] = None,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute Python code asynchronously.\n\n    This runs the synchronous execute() in a thread pool to avoid\n    blocking the event loop. For true async Docker, use aiodocker.\n\n    Args:\n        code: Python code to execute\n        context_mount: Optional path to context file\n\n    Returns:\n        ExecutionResult with stdout, stderr, and exit code\n    \"\"\"\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(\n        None,\n        lambda: self.execute(code, context_mount),\n    )\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.docker.DockerSandbox.validate_security","title":"validate_security","text":"<pre><code>validate_security() -&gt; dict\n</code></pre> <p>Validate the security configuration.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with security checks and their status.</p> Source code in <code>src/rlm/core/repl/docker.py</code> <pre><code>def validate_security(self) -&gt; dict:\n    \"\"\"\n    Validate the security configuration.\n\n    Returns:\n        Dictionary with security checks and their status.\n    \"\"\"\n    checks = {\n        \"docker_available\": False,\n        \"gvisor_available\": False,\n        \"gvisor_required\": not self.config.allow_unsafe_runtime,\n        \"network_disabled\": not self.config.network_enabled,\n        \"memory_limited\": bool(self.config.memory_limit),\n        \"pids_limited\": self.config.pids_limit &lt; 100,\n        \"agent_lib_available\": AGENT_LIB_PATH.exists(),\n    }\n\n    try:\n        self.client.ping()\n        checks[\"docker_available\"] = True\n\n        info = self.client.info()\n        runtimes = info.get(\"Runtimes\", {})\n        checks[\"gvisor_available\"] = \"runsc\" in runtimes\n    except Exception:\n        pass\n\n    # Overall security status\n    checks[\"secure\"] = (\n        checks[\"docker_available\"]\n        and (checks[\"gvisor_available\"] or self.config.allow_unsafe_runtime)\n        and checks[\"network_disabled\"]\n        and checks[\"memory_limited\"]\n        and checks[\"agent_lib_available\"]\n    )\n\n    return checks\n</code></pre>"},{"location":"api/sandbox/#sandbox-configuration","title":"Sandbox Configuration","text":""},{"location":"api/sandbox/#rlm.core.repl.docker.SandboxConfig","title":"SandboxConfig  <code>dataclass</code>","text":"<p>Configuration for Docker sandbox.</p> Source code in <code>src/rlm/core/repl/docker.py</code> <pre><code>@dataclass\nclass SandboxConfig:\n    \"\"\"Configuration for Docker sandbox.\"\"\"\n\n    image: str = field(default_factory=lambda: settings.docker_image)\n    timeout: int = field(default_factory=lambda: settings.execution_timeout)\n    memory_limit: str = field(default_factory=lambda: settings.memory_limit)\n    cpu_limit: float = field(default_factory=lambda: settings.cpu_limit)\n    pids_limit: int = field(default_factory=lambda: settings.pids_limit)\n    network_enabled: bool = field(default_factory=lambda: settings.network_enabled)\n    runtime: str = field(default_factory=lambda: settings.docker_runtime)\n    # v2.1: Fail-closed security - require gVisor by default\n    allow_unsafe_runtime: bool = False\n</code></pre>"},{"location":"api/sandbox/#execution-result","title":"Execution Result","text":""},{"location":"api/sandbox/#rlm.core.repl.docker.ExecutionResult","title":"ExecutionResult  <code>dataclass</code>","text":"<p>Result of code execution in the sandbox.</p> Source code in <code>src/rlm/core/repl/docker.py</code> <pre><code>@dataclass\nclass ExecutionResult:\n    \"\"\"Result of code execution in the sandbox.\"\"\"\n\n    stdout: str\n    stderr: str\n    exit_code: int\n    timed_out: bool = False\n    oom_killed: bool = False\n    execution_time_ms: int = 0\n\n    @property\n    def success(self) -&gt; bool:\n        \"\"\"Check if execution was successful.\"\"\"\n        return self.exit_code == 0 and not self.timed_out and not self.oom_killed\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.docker.ExecutionResult.success","title":"success  <code>property</code>","text":"<pre><code>success: bool\n</code></pre> <p>Check if execution was successful.</p>"},{"location":"api/sandbox/#async-sandbox","title":"Async Sandbox","text":"<p>For true async Docker operations, use <code>AsyncDockerSandbox</code>:</p>"},{"location":"api/sandbox/#rlm.core.repl.async_docker.AsyncDockerSandbox","title":"AsyncDockerSandbox","text":"<p>Async Docker sandbox using aiodocker for non-blocking operations.</p> <p>v3.0: Uses TemporaryDirectory for crash-safe cleanup. No leftover files in /tmp even on SIGKILL.</p> Example <p>sandbox = AsyncDockerSandbox() result = await sandbox.execute(\"print('Hello!')\") print(result.stdout) Hello!</p> Source code in <code>src/rlm/core/repl/async_docker.py</code> <pre><code>class AsyncDockerSandbox:\n    \"\"\"\n    Async Docker sandbox using aiodocker for non-blocking operations.\n\n    v3.0: Uses TemporaryDirectory for crash-safe cleanup.\n    No leftover files in /tmp even on SIGKILL.\n\n    Example:\n        &gt;&gt;&gt; sandbox = AsyncDockerSandbox()\n        &gt;&gt;&gt; result = await sandbox.execute(\"print('Hello!')\")\n        &gt;&gt;&gt; print(result.stdout)\n        Hello!\n    \"\"\"\n\n    def __init__(\n        self,\n        image: Optional[str] = None,\n        timeout: Optional[int] = None,\n        config: Optional[SandboxConfig] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the async sandbox.\"\"\"\n        self.config = config or SandboxConfig()\n        if image:\n            self.config.image = image\n        if timeout:\n            self.config.timeout = timeout\n\n        self._runtime: Optional[str] = None\n\n    async def _detect_runtime(self, docker: aiodocker.Docker) -&gt; str:\n        \"\"\"Detect the best available runtime (fail-closed by default).\"\"\"\n        if self.config.runtime != \"auto\":\n            return self.config.runtime\n\n        try:\n            info = await docker.system.info()\n            runtimes = info.get(\"Runtimes\", {})\n\n            if \"runsc\" in runtimes:\n                logger.info(\"\u2713 Secure runtime 'runsc' (gVisor) detected.\")\n                return \"runsc\"\n            else:\n                if self.config.allow_unsafe_runtime:\n                    logger.warning(\"\u26a0 gVisor not found, using runc.\")\n                    return \"runc\"\n                else:\n                    raise SecurityViolationError(\n                        \"gVisor (runsc) not found! Set RLM_ALLOW_UNSAFE_RUNTIME=1 to allow.\"\n                    )\n        except SecurityViolationError:\n            raise\n        except Exception as e:\n            if self.config.allow_unsafe_runtime:\n                return \"runc\"\n            raise SecurityViolationError(f\"Runtime detection failed: {e}\") from e\n\n    async def _ensure_image(self, docker: aiodocker.Docker) -&gt; None:\n        \"\"\"Pull image if not available.\"\"\"\n        try:\n            await docker.images.inspect(self.config.image)\n        except DockerError:\n            logger.info(f\"Pulling image: {self.config.image}\")\n            await docker.images.pull(self.config.image)\n\n    async def execute(\n        self,\n        code: str,\n        context_mount: Optional[str] = None,\n    ) -&gt; ExecutionResult:\n        \"\"\"\n        Execute code in an async Docker container.\n\n        v3.0: Uses TemporaryDirectory for crash-safe cleanup.\n        Directory auto-destructs on exit even under exceptions.\n\n        Args:\n            code: Python code to execute\n            context_mount: Optional context file path\n\n        Returns:\n            ExecutionResult with stdout, stderr, exit_code\n        \"\"\"\n        # v3.0: TemporaryDirectory for crash-safe cleanup\n        with tempfile.TemporaryDirectory(prefix=\"rlm_exec_\") as tmpdir:\n            script_path = Path(tmpdir) / \"user_code.py\"\n            script_path.write_text(code, encoding=\"utf-8\")\n\n            try:\n                async with aiodocker.Docker() as docker:\n                    runtime = await self._detect_runtime(docker)\n                    await self._ensure_image(docker)\n\n                    # Build volume binds\n                    binds = [\n                        f\"{AGENT_LIB_PATH}:/opt/rlm_agent_lib:ro\",\n                        f\"{str(script_path)}:/tmp/user_code.py:ro\",\n                    ]\n                    if context_mount:\n                        binds.append(f\"{context_mount}:/mnt/context:ro\")\n\n                    config = {\n                        \"Image\": self.config.image,\n                        \"Cmd\": [\n                            \"sh\", \"-c\",\n                            \"export PYTHONPATH=/opt:$PYTHONPATH &amp;&amp; \"\n                            \"python3 -c \\\"\"\n                            \"import sys; sys.path.insert(0, '/opt'); \"\n                            \"from rlm_agent_lib.boot import setup_environment, execute_code; \"\n                            \"env = setup_environment(); \"\n                            \"code = open('/tmp/user_code.py').read(); \"\n                            \"execute_code(code, env)\"\n                            \"\\\"\"\n                        ],\n                        \"HostConfig\": {\n                            \"Binds\": binds,\n                            \"NetworkMode\": \"none\" if not self.config.network_enabled else \"bridge\",\n                            \"Memory\": self._parse_memory_limit(self.config.memory_limit),\n                            \"MemorySwap\": self._parse_memory_limit(self.config.memory_limit),\n                            \"NanoCPUs\": int(self.config.cpu_limit * 1_000_000_000),\n                            \"PidsLimit\": self.config.pids_limit,\n                            \"SecurityOpt\": [\"no-new-privileges:true\"],\n                            \"IpcMode\": \"none\",\n                            \"Runtime\": runtime,\n                        },\n                        \"Env\": [\n                            \"PYTHONPATH=/opt\",\n                            \"PYTHONDONTWRITEBYTECODE=1\",\n                        ],\n                    }\n\n                    # Create and start container\n                    container = await docker.containers.create(config=config)\n\n                    try:\n                        await container.start()\n\n                        # Wait with timeout\n                        try:\n                            result = await asyncio.wait_for(\n                                container.wait(),\n                                timeout=self.config.timeout,\n                            )\n                            exit_code = result.get(\"StatusCode\", -1)\n                            timed_out = False\n                        except asyncio.TimeoutError:\n                            logger.warning(\"Container timed out, killing...\")\n                            await container.kill()\n                            exit_code = 124\n                            timed_out = True\n\n                        # Get logs\n                        logs = await container.log(stdout=True, stderr=True)\n                        stdout = \"\".join(logs)\n                        stderr = \"\"\n\n                        # Check OOM\n                        info = await container.show()\n                        oom_killed = info.get(\"State\", {}).get(\"OOMKilled\", False)\n\n                        # Truncate if needed\n                        max_bytes = settings.max_stdout_bytes\n                        if len(stdout) &gt; max_bytes:\n                            stdout = stdout[:1000] + \"\\n...[TRUNCATED]...\\n\" + stdout[-3000:]\n\n                        return ExecutionResult(\n                            stdout=stdout,\n                            stderr=stderr,\n                            exit_code=exit_code,\n                            timed_out=timed_out,\n                            oom_killed=oom_killed,\n                        )\n\n                    finally:\n                        await container.delete(force=True)\n\n            except DockerError as e:\n                raise SandboxError(\n                    message=\"Async Docker execution failed\",\n                    details={\"error\": str(e)},\n                ) from e\n\n        # Temp directory auto-cleaned here\n\n    @staticmethod\n    def _parse_memory_limit(limit: str) -&gt; int:\n        \"\"\"Parse memory limit string to bytes.\"\"\"\n        limit = limit.lower()\n        multipliers = {\n            'k': 1024,\n            'm': 1024**2,\n            'g': 1024**3,\n        }\n\n        for suffix, mult in multipliers.items():\n            if limit.endswith(suffix):\n                return int(limit[:-1]) * mult\n\n        return int(limit)\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.async_docker.AsyncDockerSandbox.__init__","title":"__init__","text":"<pre><code>__init__(image: Optional[str] = None, timeout: Optional[int] = None, config: Optional[SandboxConfig] = None) -&gt; None\n</code></pre> <p>Initialize the async sandbox.</p> Source code in <code>src/rlm/core/repl/async_docker.py</code> <pre><code>def __init__(\n    self,\n    image: Optional[str] = None,\n    timeout: Optional[int] = None,\n    config: Optional[SandboxConfig] = None,\n) -&gt; None:\n    \"\"\"Initialize the async sandbox.\"\"\"\n    self.config = config or SandboxConfig()\n    if image:\n        self.config.image = image\n    if timeout:\n        self.config.timeout = timeout\n\n    self._runtime: Optional[str] = None\n</code></pre>"},{"location":"api/sandbox/#rlm.core.repl.async_docker.AsyncDockerSandbox.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(code: str, context_mount: Optional[str] = None) -&gt; ExecutionResult\n</code></pre> <p>Execute code in an async Docker container.</p> <p>v3.0: Uses TemporaryDirectory for crash-safe cleanup. Directory auto-destructs on exit even under exceptions.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Python code to execute</p> required <code>context_mount</code> <code>Optional[str]</code> <p>Optional context file path</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with stdout, stderr, exit_code</p> Source code in <code>src/rlm/core/repl/async_docker.py</code> <pre><code>async def execute(\n    self,\n    code: str,\n    context_mount: Optional[str] = None,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute code in an async Docker container.\n\n    v3.0: Uses TemporaryDirectory for crash-safe cleanup.\n    Directory auto-destructs on exit even under exceptions.\n\n    Args:\n        code: Python code to execute\n        context_mount: Optional context file path\n\n    Returns:\n        ExecutionResult with stdout, stderr, exit_code\n    \"\"\"\n    # v3.0: TemporaryDirectory for crash-safe cleanup\n    with tempfile.TemporaryDirectory(prefix=\"rlm_exec_\") as tmpdir:\n        script_path = Path(tmpdir) / \"user_code.py\"\n        script_path.write_text(code, encoding=\"utf-8\")\n\n        try:\n            async with aiodocker.Docker() as docker:\n                runtime = await self._detect_runtime(docker)\n                await self._ensure_image(docker)\n\n                # Build volume binds\n                binds = [\n                    f\"{AGENT_LIB_PATH}:/opt/rlm_agent_lib:ro\",\n                    f\"{str(script_path)}:/tmp/user_code.py:ro\",\n                ]\n                if context_mount:\n                    binds.append(f\"{context_mount}:/mnt/context:ro\")\n\n                config = {\n                    \"Image\": self.config.image,\n                    \"Cmd\": [\n                        \"sh\", \"-c\",\n                        \"export PYTHONPATH=/opt:$PYTHONPATH &amp;&amp; \"\n                        \"python3 -c \\\"\"\n                        \"import sys; sys.path.insert(0, '/opt'); \"\n                        \"from rlm_agent_lib.boot import setup_environment, execute_code; \"\n                        \"env = setup_environment(); \"\n                        \"code = open('/tmp/user_code.py').read(); \"\n                        \"execute_code(code, env)\"\n                        \"\\\"\"\n                    ],\n                    \"HostConfig\": {\n                        \"Binds\": binds,\n                        \"NetworkMode\": \"none\" if not self.config.network_enabled else \"bridge\",\n                        \"Memory\": self._parse_memory_limit(self.config.memory_limit),\n                        \"MemorySwap\": self._parse_memory_limit(self.config.memory_limit),\n                        \"NanoCPUs\": int(self.config.cpu_limit * 1_000_000_000),\n                        \"PidsLimit\": self.config.pids_limit,\n                        \"SecurityOpt\": [\"no-new-privileges:true\"],\n                        \"IpcMode\": \"none\",\n                        \"Runtime\": runtime,\n                    },\n                    \"Env\": [\n                        \"PYTHONPATH=/opt\",\n                        \"PYTHONDONTWRITEBYTECODE=1\",\n                    ],\n                }\n\n                # Create and start container\n                container = await docker.containers.create(config=config)\n\n                try:\n                    await container.start()\n\n                    # Wait with timeout\n                    try:\n                        result = await asyncio.wait_for(\n                            container.wait(),\n                            timeout=self.config.timeout,\n                        )\n                        exit_code = result.get(\"StatusCode\", -1)\n                        timed_out = False\n                    except asyncio.TimeoutError:\n                        logger.warning(\"Container timed out, killing...\")\n                        await container.kill()\n                        exit_code = 124\n                        timed_out = True\n\n                    # Get logs\n                    logs = await container.log(stdout=True, stderr=True)\n                    stdout = \"\".join(logs)\n                    stderr = \"\"\n\n                    # Check OOM\n                    info = await container.show()\n                    oom_killed = info.get(\"State\", {}).get(\"OOMKilled\", False)\n\n                    # Truncate if needed\n                    max_bytes = settings.max_stdout_bytes\n                    if len(stdout) &gt; max_bytes:\n                        stdout = stdout[:1000] + \"\\n...[TRUNCATED]...\\n\" + stdout[-3000:]\n\n                    return ExecutionResult(\n                        stdout=stdout,\n                        stderr=stderr,\n                        exit_code=exit_code,\n                        timed_out=timed_out,\n                        oom_killed=oom_killed,\n                    )\n\n                finally:\n                    await container.delete(force=True)\n\n        except DockerError as e:\n            raise SandboxError(\n                message=\"Async Docker execution failed\",\n                details={\"error\": str(e)},\n            ) from e\n</code></pre>"},{"location":"api/sandbox/#direct-usage","title":"Direct Usage","text":"<pre><code>from rlm.core.repl import DockerSandbox, SandboxConfig\n\n# Custom configuration\nconfig = SandboxConfig(\n    image=\"python:3.11-slim\",\n    timeout=60,\n    memory_limit=\"512m\",\n    allow_unsafe_runtime=False,  # Require gVisor\n)\n\nsandbox = DockerSandbox(config=config)\n\n# Execute code\nresult = sandbox.execute(\"print('Hello from sandbox!')\")\nprint(result.stdout)\n</code></pre>"},{"location":"api/sandbox/#security-validation","title":"Security Validation","text":"<pre><code>checks = sandbox.validate_security()\n\nprint(f\"Docker available: {checks['docker_available']}\")\nprint(f\"gVisor available: {checks['gvisor_available']}\")\nprint(f\"Network disabled: {checks['network_disabled']}\")\nprint(f\"Overall secure: {checks['secure']}\")\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>RLM-Python is configured via environment variables.</p>"},{"location":"getting-started/configuration/#all-settings","title":"All Settings","text":"Variable Default Description Docker <code>RLM_DOCKER_IMAGE</code> <code>python:3.11-slim</code> Container image for execution <code>RLM_DOCKER_RUNTIME</code> <code>auto</code> Runtime: <code>auto</code>, <code>runsc</code>, or <code>runc</code> <code>RLM_ALLOW_UNSAFE_RUNTIME</code> <code>0</code> Allow execution without gVisor Limits <code>RLM_MEMORY_LIMIT</code> <code>256m</code> Memory limit per container <code>RLM_CPU_LIMIT</code> <code>0.5</code> CPU cores (fractional allowed) <code>RLM_PIDS_LIMIT</code> <code>50</code> Max processes in container <code>RLM_EXECUTION_TIMEOUT</code> <code>30</code> Timeout in seconds Network <code>RLM_NETWORK_ENABLED</code> <code>0</code> Allow network access (risky!) Egress Filtering <code>RLM_ENTROPY_THRESHOLD</code> <code>4.5</code> Shannon entropy for secrets <code>RLM_SIMILARITY_THRESHOLD</code> <code>0.8</code> Context echo detection <code>RLM_MAX_STDOUT_BYTES</code> <code>4000</code> Output truncation limit LLM <code>RLM_LLM_PROVIDER</code> <code>openai</code> Provider: openai, anthropic, google <code>RLM_LLM_MODEL</code> <code>gpt-4</code> Model name <code>RLM_MAX_RECURSION_DEPTH</code> <code>10</code> Max LLM iterations Budget <code>RLM_MAX_BUDGET_DOLLARS</code> <code>1.0</code> Maximum spend per run"},{"location":"getting-started/configuration/#configuration-via-code","title":"Configuration via Code","text":"<pre><code>from rlm import Orchestrator\nfrom rlm.core.repl import SandboxConfig\nfrom rlm.core import OrchestratorConfig\n\n# Sandbox configuration\nsandbox_config = SandboxConfig(\n    image=\"python:3.11-slim\",\n    timeout=60,\n    memory_limit=\"512m\",\n    cpu_limit=1.0,\n    network_enabled=False,\n    allow_unsafe_runtime=False,  # Require gVisor\n)\n\n# Orchestrator configuration\norch_config = OrchestratorConfig(\n    max_iterations=15,\n    raise_on_leak=True,  # Raise exception instead of redacting\n)\n\n# Create with custom configs\nagent = Orchestrator(config=orch_config)\n</code></pre>"},{"location":"getting-started/configuration/#security-configurations","title":"Security Configurations","text":""},{"location":"getting-started/configuration/#production-maximum-security","title":"Production (Maximum Security)","text":"<pre><code>export RLM_DOCKER_RUNTIME=runsc  # Force gVisor\nexport RLM_NETWORK_ENABLED=0     # Block network\nexport RLM_MEMORY_LIMIT=128m     # Tight limits\nexport RLM_EXECUTION_TIMEOUT=15  # Short timeout\n</code></pre>"},{"location":"getting-started/configuration/#development-relaxed","title":"Development (Relaxed)","text":"<pre><code>export RLM_ALLOW_UNSAFE_RUNTIME=1  # Allow without gVisor\nexport RLM_EXECUTION_TIMEOUT=120   # Longer timeout\nexport RLM_MEMORY_LIMIT=512m       # More resources\n</code></pre> <p>Never in Production</p> <p>Never use <code>RLM_ALLOW_UNSAFE_RUNTIME=1</code> in production environments.</p>"},{"location":"getting-started/configuration/#custom-docker-images","title":"Custom Docker Images","text":"<p>To use additional Python packages, create a custom image:</p> <pre><code>FROM python:3.11-slim\n\n# Install your dependencies\nRUN pip install numpy pandas scikit-learn matplotlib\n\n# Keep the image small\nRUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> <p>Build and use:</p> <pre><code>docker build -t my-rlm-image:latest .\nexport RLM_DOCKER_IMAGE=my-rlm-image:latest\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Complete installation guide for RLM-Python.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.10, 3.11, or 3.12</li> <li>Docker Engine: Required for sandbox execution</li> <li>gVisor (optional): Recommended for production security</li> </ul>"},{"location":"getting-started/installation/#install-the-package","title":"Install the Package","text":"<pre><code>pip install rlm-python\n</code></pre>"},{"location":"getting-started/installation/#docker-setup","title":"Docker Setup","text":""},{"location":"getting-started/installation/#linux","title":"Linux","text":"<pre><code># Install Docker Engine\ncurl -fsSL https://get.docker.com | sh\nsudo usermod -aG docker $USER\n\n# Verify installation\ndocker run hello-world\n</code></pre>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<p>Download and install Docker Desktop for Mac.</p>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<p>Download and install Docker Desktop for Windows with WSL 2 backend.</p>"},{"location":"getting-started/installation/#gvisor-installation-production","title":"gVisor Installation (Production)","text":"<p>Recommended for Production</p> <p>gVisor provides kernel-level syscall interception, blocking dangerous operations before they reach your host.</p>"},{"location":"getting-started/installation/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code># Add gVisor repository\ncurl -fsSL https://gvisor.dev/archive.key | sudo gpg --dearmor -o /usr/share/keyrings/gvisor-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/gvisor-archive-keyring.gpg] https://storage.googleapis.com/gvisor/releases release main\" | sudo tee /etc/apt/sources.list.d/gvisor.list &gt; /dev/null\n\n# Install runsc\nsudo apt-get update &amp;&amp; sudo apt-get install -y runsc\n\n# Configure Docker\nsudo runsc install\nsudo systemctl restart docker\n\n# Verify\ndocker run --runtime=runsc hello-world\n</code></pre>"},{"location":"getting-started/installation/#without-gvisor","title":"Without gVisor","text":"<p>If you can't install gVisor, you can explicitly allow unsafe runtime:</p> <pre><code>export RLM_ALLOW_UNSAFE_RUNTIME=1\n</code></pre> <p>Security Warning</p> <p>Running without gVisor provides reduced isolation. The code still runs in Docker containers with network disabled, but syscall-level protection is not available.</p>"},{"location":"getting-started/installation/#llm-provider-setup","title":"LLM Provider Setup","text":"<p>RLM supports multiple LLM providers. Set your API key:</p> OpenAIAnthropicGoogle <pre><code>export OPENAI_API_KEY=sk-...\nexport RLM_LLM_PROVIDER=openai\nexport RLM_LLM_MODEL=gpt-4\n</code></pre> <pre><code>export ANTHROPIC_API_KEY=sk-ant-...\nexport RLM_LLM_PROVIDER=anthropic\nexport RLM_LLM_MODEL=claude-3-opus\n</code></pre> <pre><code>export GOOGLE_API_KEY=...\nexport RLM_LLM_PROVIDER=google\nexport RLM_LLM_MODEL=gemini-pro\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>from rlm import Orchestrator, settings\n\n# Check configuration\nprint(f\"Docker Image: {settings.docker_image}\")\nprint(f\"LLM Provider: {settings.llm_provider}\")\n\n# Test execution\nagent = Orchestrator()\nresult = agent.run(\"Print 'Hello, RLM!'\")\nprint(result.final_answer)\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"Docker permission denied <p>Add your user to the docker group: <pre><code>sudo usermod -aG docker $USER\nnewgrp docker\n</code></pre></p> gVisor not detected <p>Check if runsc is installed: <pre><code>runsc --version\ndocker info | grep -i runtime\n</code></pre></p> LLM API errors <p>Verify your API key is set: <pre><code>echo $OPENAI_API_KEY  # Should not be empty\n</code></pre></p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get RLM-Python running in under 2 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Docker Required</p> <p>RLM executes code in Docker containers. Make sure Docker Engine is installed and running.</p> <pre><code>docker --version  # Should show Docker version\ndocker ps         # Should work without errors\n</code></pre>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"pippoetryuv <pre><code>pip install rlm-python\n</code></pre> <pre><code>poetry add rlm-python\n</code></pre> <pre><code>uv add rlm-python\n</code></pre>"},{"location":"getting-started/quickstart/#your-first-agent","title":"Your First Agent","text":""},{"location":"getting-started/quickstart/#async-recommended","title":"Async (Recommended)","text":"<pre><code>import asyncio\nfrom rlm import Orchestrator\n\nasync def main():\n    # Initialize the orchestrator\n    agent = Orchestrator()\n\n    # Execute a query (non-blocking)\n    result = await agent.arun(\"What is 42 * 17?\")\n\n    print(f\"Success: {result.success}\")\n    print(f\"Answer: {result.final_answer}\")\n    print(f\"Iterations: {result.iterations}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#synchronous","title":"Synchronous","text":"<pre><code>from rlm import Orchestrator\n\nagent = Orchestrator()\nresult = agent.run(\"Calculate the factorial of 10\")\n\nprint(result.final_answer)  # 3628800\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-results","title":"Understanding Results","text":"<p>The <code>OrchestratorResult</code> contains:</p> Field Type Description <code>final_answer</code> <code>str</code> The extracted answer from <code>FINAL(...)</code> <code>success</code> <code>bool</code> Whether execution completed successfully <code>iterations</code> <code>int</code> Number of LLM \u2194 Code cycles <code>steps</code> <code>list</code> Detailed execution steps <code>budget_summary</code> <code>dict</code> Token usage and cost"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li> Configuration - Customize settings</li> <li> Async Guide - Deep dive into async</li> <li> Security - Understand the security model</li> </ul>"},{"location":"guide/async/","title":"Async/Await Guide","text":"<p>RLM v3.0 is async-first. Learn how to leverage non-blocking execution.</p>"},{"location":"guide/async/#why-async","title":"Why Async?","text":"Sync (<code>run()</code>) Async (<code>arun()</code>) Blocks the thread Non-blocking One request at a time Concurrent requests Simple scripts Web servers, APIs"},{"location":"guide/async/#basic-async","title":"Basic Async","text":"<pre><code>import asyncio\nfrom rlm import Orchestrator\n\nasync def main():\n    agent = Orchestrator()\n\n    result = await agent.arun(\"Calculate fibonacci(20)\")\n    print(result.final_answer)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guide/async/#concurrent-execution","title":"Concurrent Execution","text":"<p>Process multiple queries simultaneously:</p> <pre><code>import asyncio\nfrom rlm import Orchestrator\n\nasync def process_query(agent: Orchestrator, query: str) -&gt; str:\n    result = await agent.arun(query)\n    return result.final_answer\n\nasync def main():\n    agent = Orchestrator()\n\n    queries = [\n        \"What is 2 + 2?\",\n        \"What is 10 * 10?\",\n        \"What is sqrt(144)?\",\n    ]\n\n    # Execute all concurrently\n    results = await asyncio.gather(*[\n        process_query(agent, q) for q in queries\n    ])\n\n    for query, answer in zip(queries, results):\n        print(f\"{query} \u2192 {answer}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guide/async/#fastapi-integration","title":"FastAPI Integration","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom rlm import Orchestrator\nfrom rlm.core.exceptions import RLMError\n\napp = FastAPI()\norchestrator = Orchestrator()\n\nclass QueryRequest(BaseModel):\n    query: str\n    context_path: str | None = None\n\nclass QueryResponse(BaseModel):\n    answer: str\n    success: bool\n    iterations: int\n\n@app.post(\"/execute\", response_model=QueryResponse)\nasync def execute(request: QueryRequest):\n    try:\n        result = await orchestrator.arun(\n            request.query,\n            context_path=request.context_path,\n        )\n        return QueryResponse(\n            answer=result.final_answer or \"\",\n            success=result.success,\n            iterations=result.iterations,\n        )\n    except RLMError as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"guide/async/#aiohttp-integration","title":"aiohttp Integration","text":"<pre><code>from aiohttp import web\nfrom rlm import Orchestrator\n\norchestrator = Orchestrator()\n\nasync def handle_execute(request: web.Request) -&gt; web.Response:\n    data = await request.json()\n    query = data.get(\"query\", \"\")\n\n    result = await orchestrator.arun(query)\n\n    return web.json_response({\n        \"answer\": result.final_answer,\n        \"success\": result.success,\n    })\n\napp = web.Application()\napp.router.add_post(\"/execute\", handle_execute)\n</code></pre>"},{"location":"guide/async/#architecture-note","title":"Architecture Note","text":"<p>v3.0 uses a Single Source of Truth architecture:</p> <pre><code># Internal structure\nasync def _execute_cycle(self, ...):\n    \"\"\"All logic lives here\"\"\"\n\nasync def arun(self, ...):\n    \"\"\"Public async - delegates to _execute_cycle\"\"\"\n    return await self._execute_cycle(...)\n\ndef run(self, ...):\n    \"\"\"Public sync - pure wrapper\"\"\"\n    return asyncio.run(self.arun(...))\n</code></pre> <p>This ensures:</p> <ul> <li>\u2705 Zero code duplication</li> <li>\u2705 Consistent behavior sync/async</li> <li>\u2705 All fixes apply to both paths</li> </ul>"},{"location":"guide/async/#running-in-jupyter","title":"Running in Jupyter","text":"<p>If you're in Jupyter/IPython (which has a running event loop):</p> <pre><code># Option 1: Use nest_asyncio\nimport nest_asyncio\nnest_asyncio.apply()\n\nresult = orchestrator.run(\"query\")\n\n# Option 2: Use await directly (preferred)\nresult = await orchestrator.arun(\"query\")\n</code></pre>"},{"location":"guide/async/#cpu-offloading","title":"CPU Offloading","text":"<p>v3.0 runs CPU-intensive operations (egress filtering) in a thread pool:</p> <pre><code># This happens automatically - you don't need to do anything\n# But here's what's happening internally:\n\nloop = asyncio.get_running_loop()\nresult = await loop.run_in_executor(\n    None,  # Uses default ThreadPoolExecutor\n    self.egress_filter.filter,\n    output_text,\n)\n</code></pre> <p>This prevents large outputs from blocking your event loop.</p>"},{"location":"guide/basic-usage/","title":"Basic Usage","text":"<p>Learn the fundamentals of RLM-Python.</p>"},{"location":"guide/basic-usage/#the-agent-loop","title":"The Agent Loop","text":"<p>RLM implements a ReAct-style agent loop:</p> <pre><code>graph LR\n    A[Query] --&gt; B[LLM]\n    B --&gt; C{Code?}\n    C --&gt;|Yes| D[Execute in Sandbox]\n    D --&gt; E[Filter Output]\n    E --&gt; B\n    C --&gt;|FINAL()| F[Return Answer]</code></pre> <ol> <li>Query \u2192 Send to LLM with system prompt</li> <li>LLM \u2192 Generates Python code in markdown blocks</li> <li>Execute \u2192 Run code in Docker sandbox</li> <li>Filter \u2192 Apply egress filtering to output</li> <li>Observe \u2192 Feed output back to LLM</li> <li>Repeat until <code>FINAL(answer)</code> is returned</li> </ol>"},{"location":"guide/basic-usage/#basic-example","title":"Basic Example","text":"<pre><code>from rlm import Orchestrator\n\nagent = Orchestrator()\n\n# Simple query\nresult = agent.run(\"What is the square root of 144?\")\n\nprint(result.final_answer)  # \"12\" or \"12.0\"\n</code></pre>"},{"location":"guide/basic-usage/#understanding-responses","title":"Understanding Responses","text":"<pre><code>result = agent.run(\"Calculate 2^10\")\n\n# Check if successful\nif result.success:\n    print(f\"Answer: {result.final_answer}\")\nelse:\n    print(f\"Error: {result.error}\")\n\n# Inspect execution details\nprint(f\"Iterations: {result.iterations}\")\nprint(f\"Total steps: {len(result.steps)}\")\n\n# View cost\nsummary = result.budget_summary\nprint(f\"Tokens used: {summary['total_tokens']}\")\nprint(f\"Cost: ${summary['total_cost']:.4f}\")\n</code></pre>"},{"location":"guide/basic-usage/#execution-steps","title":"Execution Steps","text":"<p>Each step records what happened:</p> <pre><code>for step in result.steps:\n    print(f\"[{step.action}] Iteration {step.iteration}\")\n\n    if step.action == \"llm_call\":\n        print(f\"  LLM response: {step.output_data[:100]}...\")\n    elif step.action == \"code_execution\":\n        print(f\"  Code: {step.input_data[:100]}...\")\n        print(f\"  Output: {step.output_data}\")\n    elif step.action == \"final_answer\":\n        print(f\"  Answer: {step.output_data}\")\n</code></pre>"},{"location":"guide/basic-usage/#error-handling","title":"Error Handling","text":"<pre><code>from rlm import Orchestrator\nfrom rlm.core.exceptions import RLMError, SandboxError\n\nagent = Orchestrator()\n\ntry:\n    result = agent.run(\"Analyze this data\")\n\n    if result.success:\n        print(result.final_answer)\n    else:\n        print(f\"Agent failed: {result.error}\")\n\nexcept SandboxError as e:\n    print(f\"Docker error: {e}\")\nexcept RLMError as e:\n    print(f\"RLM error: {e}\")\n</code></pre>"},{"location":"guide/basic-usage/#the-final-convention","title":"The FINAL() Convention","text":"<p>The LLM signals completion with <code>FINAL(answer)</code>:</p> <pre><code># The LLM generates code like:\n\"\"\"\nresult = calculate_something()\nprint(f\"FINAL({result})\")\n\"\"\"\n\n# RLM extracts \"answer\" from FINAL(answer)\n</code></pre> <p>Multiple formats are supported:</p> <ul> <li><code>FINAL(42)</code></li> <li><code>FINAL: The answer is 42</code></li> <li><code>Final Answer: 42</code></li> </ul>"},{"location":"guide/budget/","title":"Budget Management","text":"<p>The <code>BudgetManager</code> tracks API costs and enforces spending limits.</p>"},{"location":"guide/budget/#why-budget-management","title":"Why Budget Management?","text":"<p>LLM APIs charge per token. Without limits, a runaway loop could:</p> <ul> <li>Consume thousands of tokens</li> <li>Cost hundreds of dollars</li> <li>Drain your API budget</li> </ul>"},{"location":"guide/budget/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm.utils import BudgetManager\n\nbudget = BudgetManager(limit_usd=5.0)\n\n# Record usage\ncost = budget.record_usage(\n    model=\"gpt-4o\",\n    input_tokens=1000,\n    output_tokens=500\n)\n\nprint(f\"This call cost: ${cost:.4f}\")\nprint(f\"Total spent: ${budget.total_spent:.4f}\")\nprint(f\"Remaining: ${budget.remaining_budget:.4f}\")\n</code></pre>"},{"location":"guide/budget/#budget-enforcement","title":"Budget Enforcement","text":"<p>When the limit is exceeded:</p> <pre><code>from rlm.core.exceptions import BudgetExceededError\n\ntry:\n    budget.record_usage(\"gpt-4o\", 100000, 50000)\nexcept BudgetExceededError as e:\n    print(f\"Budget exceeded: ${e.spent:.2f} / ${e.limit:.2f}\")\n</code></pre>"},{"location":"guide/budget/#usage-summary","title":"Usage Summary","text":"<pre><code>summary = budget.summary()\nprint(summary)\n# {\n#     'total_spent_usd': 0.0125,\n#     'limit_usd': 5.0,\n#     'remaining_usd': 4.9875,\n#     'usage_percentage': 0.25,\n#     'total_requests': 3,\n#     'total_input_tokens': 5000,\n#     'total_output_tokens': 2000\n# }\n</code></pre>"},{"location":"guide/budget/#pricing-data","title":"Pricing Data","text":"<p>Pricing is loaded from <code>pricing.json</code>:</p> <pre><code>{\n  \"models\": {\n    \"gpt-4o\": {\n      \"input_cost_per_m\": 5.00,\n      \"output_cost_per_m\": 15.00\n    }\n  }\n}\n</code></pre> <p>Costs are in USD per million tokens.</p>"},{"location":"guide/budget/#custom-pricing","title":"Custom Pricing","text":"<pre><code>RLM_PRICING_PATH=/path/to/custom/pricing.json\n</code></pre>"},{"location":"guide/budget/#configuration","title":"Configuration","text":"<pre><code>RLM_COST_LIMIT_USD=5.0   # Default budget limit\n</code></pre>"},{"location":"guide/budget/#with-orchestrator","title":"With Orchestrator","text":"<p>The Orchestrator automatically tracks costs:</p> <pre><code>from rlm import Orchestrator\n\norchestrator = Orchestrator()\nresult = orchestrator.run(\"Complex query...\")\n\nprint(f\"Cost: ${result.budget_summary['total_spent_usd']:.4f}\")\nprint(f\"Tokens: {result.budget_summary['total_input_tokens'] + result.budget_summary['total_output_tokens']}\")\n</code></pre>"},{"location":"guide/budget/#resetting-budget","title":"Resetting Budget","text":"<pre><code>budget.reset()\nprint(budget.total_spent)  # 0.0\n</code></pre>"},{"location":"guide/context-handle/","title":"Context Handling","text":"<p>The <code>ContextHandle</code> class provides memory-efficient access to large files.</p>"},{"location":"guide/context-handle/#why-contexthandle","title":"Why ContextHandle?","text":"<p>When working with large context files (100MB, 1GB, or more), loading the entire file into memory is:</p> <ul> <li>Slow - Takes time to read</li> <li>Expensive - Uses lots of RAM</li> <li>Risky - Can crash with OOM</li> </ul> <p><code>ContextHandle</code> uses memory-mapping (<code>mmap</code>) for O(1) random access without loading the entire file.</p>"},{"location":"guide/context-handle/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm import ContextHandle\n\nwith ContextHandle(\"/path/to/large_file.txt\") as ctx:\n    print(f\"File size: {ctx.size_mb:.2f} MB\")\n\n    # Read first 1000 bytes\n    header = ctx.head(1000)\n\n    # Read last 1000 bytes\n    footer = ctx.tail(1000)\n</code></pre>"},{"location":"guide/context-handle/#search-api","title":"Search API","text":"<p>Find patterns without reading the entire file:</p> <pre><code>ctx = ContextHandle(\"/path/to/logs.txt\")\n\n# Search with regex\nmatches = ctx.search(r\"ERROR.*timeout\", max_results=10)\n\nfor offset, match_text in matches:\n    print(f\"Found at byte {offset}: {match_text}\")\n\n    # Get surrounding context\n    snippet = ctx.snippet(offset, window=500)\n    print(snippet)\n</code></pre>"},{"location":"guide/context-handle/#line-based-search","title":"Line-Based Search","text":"<pre><code># Search and get line numbers\nmatches = ctx.search_lines(r\"CRITICAL\", max_results=5)\n\nfor line_num, line, context in matches:\n    print(f\"Line {line_num}: {line}\")\n</code></pre>"},{"location":"guide/context-handle/#windowed-reading","title":"Windowed Reading","text":"<p>Read specific portions:</p> <pre><code># Read around a specific offset\ntext = ctx.read_window(offset=50000, radius=500)\n\n# Or using snippet\ntext = ctx.snippet(offset=50000, window=1000)\n\n# Read exact range\ntext = ctx.read(start=1000, length=500)\n</code></pre>"},{"location":"guide/context-handle/#iterating-lines","title":"Iterating Lines","text":"<p>For streaming access:</p> <pre><code>for line_num, line in ctx.iterate_lines(start_line=100):\n    if line_num &gt; 200:\n        break\n    process(line)\n</code></pre>"},{"location":"guide/context-handle/#api-reference","title":"API Reference","text":"<pre><code>class ContextHandle:\n    size: int           # Total size in bytes\n    size_mb: float      # Size in megabytes\n    path: Path          # File path\n\n    def read(start: int, length: int) -&gt; str\n    def read_window(offset: int, radius: int = 500) -&gt; str\n    def snippet(offset: int, window: int = 500) -&gt; str\n    def head(n_bytes: int = 1000) -&gt; str\n    def tail(n_bytes: int = 1000) -&gt; str\n\n    def search(pattern: str, max_results: int = 10) -&gt; List[Tuple[int, str]]\n    def search_lines(pattern: str, max_results: int = 10) -&gt; List[Tuple[int, str, str]]\n\n    def iterate_lines(start_line: int = 1) -&gt; Iterator[Tuple[int, str]]\n\n    def close() -&gt; None\n</code></pre>"},{"location":"guide/context-handle/#in-the-sandbox","title":"In the Sandbox","text":"<p>When mounted in the Docker sandbox, use the <code>ctx</code> global variable:</p> <pre><code># Inside sandbox execution\nmatches = ctx.search(r\"important pattern\")\nfor offset, match in matches:\n    print(ctx.snippet(offset))\n</code></pre>"},{"location":"guide/context/","title":"Context Files","text":"<p>Work with large files without loading them into memory.</p>"},{"location":"guide/context/#overview","title":"Overview","text":"<p>RLM uses <code>ContextHandle</code> to provide memory-efficient access to large files. The LLM receives tools to search and read snippets, never loading the entire file.</p>"},{"location":"guide/context/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm import Orchestrator\n\nagent = Orchestrator()\n\nresult = await agent.arun(\n    query=\"Find all error messages in this log file\",\n    context_path=\"/var/log/app.log\"  # 500MB log file\n)\n</code></pre> <p>Internally, RLM:</p> <ol> <li>Creates a <code>ContextHandle</code> with mmap</li> <li>Provides search/read methods to the LLM</li> <li>The LLM searches for patterns</li> <li>Reads only relevant snippets</li> </ol>"},{"location":"guide/context/#how-the-llm-uses-context","title":"How the LLM Uses Context","text":"<p>The LLM receives code tools like:</p> <pre><code># Search for patterns\nmatches = ctx.search(r\"ERROR|CRITICAL\", max_results=10)\n\n# Read snippets around matches\nfor offset, match_text in matches:\n    snippet = ctx.snippet(offset, window=500)\n    print(snippet)\n</code></pre>"},{"location":"guide/context/#direct-contexthandle-usage","title":"Direct ContextHandle Usage","text":"<pre><code>from rlm.core.memory import ContextHandle\n\n# Open a large file\nctx = ContextHandle(\"/path/to/data.csv\")\n\nprint(f\"Size: {ctx.size_mb:.2f} MB\")\n\n# Search for patterns\nmatches = ctx.search(r\"revenue &gt; 1000000\")\n\n# Read around matches\nfor offset, match in matches:\n    print(ctx.read_window(offset, radius=200))\n\n# Stream lines (memory efficient)\nfor line_no, line in ctx.iterate_lines():\n    if \"important\" in line:\n        print(f\"Line {line_no}: {line}\")\n</code></pre>"},{"location":"guide/context/#supported-files","title":"Supported Files","text":""},{"location":"guide/context/#text-files","title":"\u2705 Text Files","text":"<ul> <li>Log files (.log, .txt)</li> <li>CSV files (.csv)</li> <li>JSON files (.json)</li> <li>Source code (.py, .js, etc.)</li> <li>Markdown (.md)</li> </ul>"},{"location":"guide/context/#binary-files-v30","title":"\u274c Binary Files (v3.0+)","text":"<p>RLM v3.0 rejects binary files:</p> <pre><code>from rlm.core.memory import ContextHandle\nfrom rlm.core.exceptions import ContextError\n\ntry:\n    ctx = ContextHandle(\"/path/to/image.png\")\nexcept ContextError as e:\n    print(e)  # \"Binary file detected via null bytes...\"\n</code></pre> <p>This prevents LLM hallucination on garbage input.</p>"},{"location":"guide/context/#large-file-tips","title":"Large File Tips","text":""},{"location":"guide/context/#1-use-specific-queries","title":"1. Use Specific Queries","text":"<pre><code># \u274c Too vague - LLM might try to read everything\nresult = await agent.arun(\n    \"Tell me about this file\",\n    context_path=\"huge_data.csv\"\n)\n\n# \u2705 Specific - LLM searches for relevant data\nresult = await agent.arun(\n    \"Find the top 5 customers by total_revenue column\",\n    context_path=\"huge_data.csv\"\n)\n</code></pre>"},{"location":"guide/context/#2-provide-structure-hints","title":"2. Provide Structure Hints","text":"<pre><code># \u2705 Tell the LLM about the file format\nresult = await agent.arun(\n    \"This CSV has columns: id, name, email, revenue. \"\n    \"Find all customers with revenue &gt; 100000\",\n    context_path=\"customers.csv\"\n)\n</code></pre>"},{"location":"guide/context/#3-check-file-size-first","title":"3. Check File Size First","text":"<pre><code>from rlm.core.memory import ContextHandle\n\nctx = ContextHandle(\"/path/to/file.csv\")\n\nif ctx.size_mb &gt; 100:\n    print(f\"Warning: Large file ({ctx.size_mb:.0f} MB)\")\n    print(\"Consider pre-processing or sampling\")\n</code></pre>"},{"location":"guide/context/#memory-efficiency","title":"Memory Efficiency","text":"<p>ContextHandle uses <code>mmap</code> for zero-copy access:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Virtual Memory Space      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  ContextHandle.read(100, 50)  \u2502  \u2190 Only maps 50 bytes\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          Physical File        \u2502  \u2190 10GB on disk\n\u2502       (memory-mapped)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>No matter how large the file, memory usage stays constant.</p>"},{"location":"guide/egress-filtering/","title":"Egress Filtering","text":"<p>Egress filtering prevents sensitive data from leaking through code execution output.</p>"},{"location":"guide/egress-filtering/#how-it-works","title":"How It Works","text":"<p>The <code>EgressFilter</code> applies multiple detection layers:</p> <ol> <li>Entropy Detection - High-entropy strings suggest secrets</li> <li>Pattern Matching - Known secret formats (API keys, JWTs)</li> <li>Context Echo - Prevents printing raw context back</li> <li>Size Limiting - Truncates excessive output</li> </ol>"},{"location":"guide/egress-filtering/#detection-methods","title":"Detection Methods","text":""},{"location":"guide/egress-filtering/#shannon-entropy","title":"Shannon Entropy","text":"<p>Secrets have high entropy (randomness). Normal text has lower entropy.</p> Content Type Typical Entropy Repetitive text 0-2 Natural language 2-4 Code 4-4.5 Secrets/keys 4.5-6+ <pre><code>from rlm.security.egress import calculate_shannon_entropy\n\n# Normal text\nentropy = calculate_shannon_entropy(\"Hello, how are you?\")\nprint(f\"Text entropy: {entropy:.2f}\")  # ~3.5\n\n# API key\nentropy = calculate_shannon_entropy(\"sk-a1b2c3d4e5f6g7h8i9j0\")\nprint(f\"Key entropy: {entropy:.2f}\")  # ~4.5+\n</code></pre>"},{"location":"guide/egress-filtering/#pattern-detection","title":"Pattern Detection","text":"<p>Known secret patterns are detected:</p> <ul> <li>AWS Access Keys (<code>AKIA...</code>)</li> <li>API Keys (<code>api_key=...</code>)</li> <li>Private Keys (<code>-----BEGIN...</code>)</li> <li>JWT Tokens (<code>eyJ...</code>)</li> <li>Bearer Tokens</li> </ul> <pre><code>from rlm.security.egress import detect_secrets\n\ntext = \"My key is sk-abc123def456ghi789\"\nsecrets = detect_secrets(text)\nprint(secrets)  # [('api_key', 'sk-abc123def...')]\n</code></pre>"},{"location":"guide/egress-filtering/#context-echo-prevention","title":"Context Echo Prevention","text":"<p>Prevents the LLM from printing the raw context:</p> <pre><code>from rlm.security.egress import EgressFilter\n\ncontext = \"Sensitive company data here...\"\nfilter = EgressFilter(context=context)\n\n# Try to echo the context\noutput = \"Sensitive company data here...\"\nis_echo, similarity = filter.check_context_echo(output)\nprint(f\"Echo detected: {is_echo}\")  # True\n</code></pre>"},{"location":"guide/egress-filtering/#using-egressfilter","title":"Using EgressFilter","text":""},{"location":"guide/egress-filtering/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm.security.egress import sanitize_output\n\nraw_output = \"Result: AKIAIOSFODNN7EXAMPLE\"\nsafe_output = sanitize_output(raw_output)\nprint(safe_output)  # Result: [REDACTED: aws_access_key]\n</code></pre>"},{"location":"guide/egress-filtering/#with-context-protection","title":"With Context Protection","text":"<pre><code>from rlm.security.egress import EgressFilter\n\nfilter = EgressFilter(\n    context=\"My secret document...\",\n    entropy_threshold=4.5,\n    similarity_threshold=0.8,\n)\n\noutput = filter.filter(raw_output)\n</code></pre>"},{"location":"guide/egress-filtering/#raise-on-leak","title":"Raise on Leak","text":"<pre><code>from rlm.security.egress import sanitize_output\nfrom rlm.core.exceptions import DataLeakageError\n\ntry:\n    sanitize_output(\n        \"-----BEGIN RSA PRIVATE KEY-----\",\n        raise_on_leak=True\n    )\nexcept DataLeakageError as e:\n    print(f\"Leak prevented: {e}\")\n</code></pre>"},{"location":"guide/egress-filtering/#configuration","title":"Configuration","text":"<p>Adjust thresholds via environment variables:</p> <pre><code>RLM_ENTROPY_THRESHOLD=4.5      # Higher = less sensitive\nRLM_SIMILARITY_THRESHOLD=0.8   # Higher = exact matches only\nRLM_MIN_ENTROPY_LENGTH=256     # Minimum string length to check\nRLM_MAX_STDOUT_BYTES=4000      # Truncation limit\n</code></pre>"},{"location":"guide/integrations/","title":"Integrations","text":"<p>Use RLM with popular frameworks.</p>"},{"location":"guide/integrations/#langchain","title":"LangChain","text":"<p>Use RLM as a LangChain tool:</p> <pre><code>from langchain.agents import Tool\nfrom langchain.agents import initialize_agent\nfrom langchain.llms import OpenAI\nfrom rlm import Orchestrator\n\n# Create RLM tool\nrlm_agent = Orchestrator()\n\ndef execute_code(query: str) -&gt; str:\n    result = rlm_agent.run(query)\n    return result.final_answer or result.error or \"No result\"\n\ntools = [\n    Tool(\n        name=\"SecureCodeExecution\",\n        func=execute_code,\n        description=\"Execute Python code securely in a sandbox. \"\n                    \"Use for calculations, data analysis, etc.\"\n    )\n]\n\n# Create LangChain agent\nllm = OpenAI(temperature=0)\nagent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\")\n\n# Use it\nresult = agent.run(\"Calculate the first 10 fibonacci numbers\")\n</code></pre>"},{"location":"guide/integrations/#crewai","title":"CrewAI","text":"<pre><code>from crewai import Agent, Task, Crew\nfrom rlm import Orchestrator\n\nrlm = Orchestrator()\n\ndef secure_execute(code_request: str) -&gt; str:\n    result = rlm.run(code_request)\n    return result.final_answer\n\n# Create agent with RLM capability\nanalyst = Agent(\n    role=\"Data Analyst\",\n    goal=\"Analyze data securely\",\n    backstory=\"Expert data analyst with secure execution environment\",\n    tools=[secure_execute],\n)\n\ntask = Task(\n    description=\"Analyze the sales data and find trends\",\n    agent=analyst,\n)\n\ncrew = Crew(agents=[analyst], tasks=[task])\nresult = crew.kickoff()\n</code></pre>"},{"location":"guide/integrations/#celery-background-tasks","title":"Celery (Background Tasks)","text":"<pre><code>from celery import Celery\nfrom rlm import Orchestrator\n\napp = Celery('tasks', broker='redis://localhost:6379')\n\n@app.task\ndef execute_query(query: str, context_path: str = None) -&gt; dict:\n    agent = Orchestrator()\n    result = agent.run(query, context_path=context_path)\n\n    return {\n        \"answer\": result.final_answer,\n        \"success\": result.success,\n        \"iterations\": result.iterations,\n    }\n\n# Usage\ntask = execute_query.delay(\"Calculate sqrt(144)\")\nresult = task.get()  # {\"answer\": \"12.0\", \"success\": True, ...}\n</code></pre>"},{"location":"guide/integrations/#streamlit","title":"Streamlit","text":"<pre><code>import streamlit as st\nimport asyncio\nfrom rlm import Orchestrator\n\nst.title(\"\ud83d\udee1\ufe0f RLM Secure Executor\")\n\n# Initialize (cached)\n@st.cache_resource\ndef get_agent():\n    return Orchestrator()\n\nagent = get_agent()\n\n# User input\nquery = st.text_area(\"Enter your query:\", height=100)\n\nif st.button(\"Execute\"):\n    with st.spinner(\"Running in secure sandbox...\"):\n        result = agent.run(query)\n\n    if result.success:\n        st.success(f\"**Answer:** {result.final_answer}\")\n    else:\n        st.error(f\"Error: {result.error}\")\n\n    with st.expander(\"Execution Details\"):\n        st.json({\n            \"iterations\": result.iterations,\n            \"steps\": len(result.steps),\n            \"budget\": result.budget_summary,\n        })\n</code></pre>"},{"location":"guide/integrations/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<pre><code># Install nest_asyncio for Jupyter compatibility\n!pip install nest_asyncio\n\nimport nest_asyncio\nnest_asyncio.apply()\n\nfrom rlm import Orchestrator\n\nagent = Orchestrator()\n\n# Now you can use sync interface in Jupyter\nresult = agent.run(\"Plot a sine wave from 0 to 2\u03c0\")\nprint(result.final_answer)\n</code></pre> <p>Or use async directly:</p> <pre><code>result = await agent.arun(\"What is 2 + 2?\")\n</code></pre>"},{"location":"guide/integrations/#django","title":"Django","text":"<pre><code># views.py\nfrom django.http import JsonResponse\nfrom django.views import View\nfrom rlm import Orchestrator\nimport asyncio\n\nclass ExecuteView(View):\n    def post(self, request):\n        query = request.POST.get(\"query\", \"\")\n\n        agent = Orchestrator()\n\n        # Run async in sync context\n        result = asyncio.run(agent.arun(query))\n\n        return JsonResponse({\n            \"answer\": result.final_answer,\n            \"success\": result.success,\n        })\n</code></pre> <p>For async Django (ASGI):</p> <pre><code># views.py (async)\nfrom django.http import JsonResponse\nfrom rlm import Orchestrator\n\nasync def execute_view(request):\n    query = request.POST.get(\"query\", \"\")\n\n    agent = Orchestrator()\n    result = await agent.arun(query)\n\n    return JsonResponse({\n        \"answer\": result.final_answer,\n        \"success\": result.success,\n    })\n</code></pre>"},{"location":"guide/llm-providers/","title":"LLM Providers","text":"<p>RLM supports multiple LLM providers with a unified interface.</p>"},{"location":"guide/llm-providers/#supported-providers","title":"Supported Providers","text":"Provider Models Streaming OpenAI GPT-4, GPT-4o, GPT-4o-mini \u2705 Anthropic Claude 3 Opus, Sonnet, Haiku \u2705 Google Gemini 1.5 Pro \u2705"},{"location":"guide/llm-providers/#configuration","title":"Configuration","text":""},{"location":"guide/llm-providers/#openai","title":"OpenAI","text":"<pre><code>RLM_API_PROVIDER=openai\nRLM_API_KEY=sk-...\nRLM_MODEL_NAME=gpt-4o\n</code></pre>"},{"location":"guide/llm-providers/#anthropic","title":"Anthropic","text":"<pre><code>RLM_API_PROVIDER=anthropic\nRLM_API_KEY=sk-ant-api03-...\nRLM_MODEL_NAME=claude-3-sonnet-20240229\n</code></pre>"},{"location":"guide/llm-providers/#google","title":"Google","text":"<pre><code>RLM_API_PROVIDER=google\nRLM_API_KEY=AIza...\nRLM_MODEL_NAME=gemini-1.5-pro\n</code></pre>"},{"location":"guide/llm-providers/#programmatic-usage","title":"Programmatic Usage","text":""},{"location":"guide/llm-providers/#using-the-factory","title":"Using the Factory","text":"<pre><code>from rlm.llm import create_llm_client\n\nclient = create_llm_client(\n    provider=\"openai\",\n    api_key=\"sk-...\",\n    model=\"gpt-4o\",\n    temperature=0.0,\n)\n</code></pre>"},{"location":"guide/llm-providers/#direct-client-usage","title":"Direct Client Usage","text":"<pre><code>from rlm.llm.openai_client import OpenAIClient\nfrom rlm.llm.base import Message\n\nclient = OpenAIClient(\n    api_key=\"sk-...\",\n    model=\"gpt-4o\",\n)\n\nresponse = client.complete([\n    Message(role=\"user\", content=\"What is 2+2?\")\n])\n\nprint(response.content)  # \"4\"\nprint(response.usage.total_tokens)  # Token count\n</code></pre>"},{"location":"guide/llm-providers/#streaming","title":"Streaming","text":"<pre><code>for chunk in client.stream([\n    Message(role=\"user\", content=\"Write a haiku\")\n]):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"guide/llm-providers/#custom-system-prompts","title":"Custom System Prompts","text":"<pre><code>response = client.complete(\n    messages=[Message(role=\"user\", content=\"Hello\")],\n    system_prompt=\"You are a helpful assistant.\",\n)\n</code></pre>"},{"location":"guide/llm-providers/#response-object","title":"Response Object","text":"<pre><code>@dataclass\nclass LLMResponse:\n    content: str              # Generated text\n    model: str                # Model used\n    usage: TokenUsage         # Token counts\n    finish_reason: str        # Why generation stopped\n\n@dataclass\nclass TokenUsage:\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n</code></pre>"},{"location":"guide/llm-providers/#error-handling","title":"Error Handling","text":"<pre><code>from rlm.core.exceptions import LLMError\n\ntry:\n    response = client.complete([...])\nexcept LLMError as e:\n    print(f\"Provider: {e.provider}\")\n    print(f\"Error: {e.message}\")\n</code></pre>"},{"location":"guide/orchestrator/","title":"Orchestrator","text":"<p>The <code>Orchestrator</code> is the main class for running LLM-powered code execution loops.</p>"},{"location":"guide/orchestrator/#overview","title":"Overview","text":"<p>The Orchestrator coordinates:</p> <ol> <li>Sending queries to the LLM</li> <li>Extracting code blocks from responses</li> <li>Executing code in the Docker sandbox</li> <li>Filtering output through egress controls</li> <li>Returning results to the LLM for iteration</li> </ol> <pre><code>graph LR\n    A[User Query] --&gt; B[Orchestrator]\n    B --&gt; C[LLM]\n    C --&gt; D[Code Extraction]\n    D --&gt; E[Docker Sandbox]\n    E --&gt; F[Egress Filter]\n    F --&gt; C\n    C --&gt; G[Final Answer]</code></pre>"},{"location":"guide/orchestrator/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm import Orchestrator\n\norchestrator = Orchestrator()\nresult = orchestrator.run(\"What is the square root of 144?\")\n\nprint(result.final_answer)  # 12.0\nprint(result.success)       # True\nprint(result.iterations)    # 1\n</code></pre>"},{"location":"guide/orchestrator/#configuration","title":"Configuration","text":"<pre><code>from rlm import Orchestrator\nfrom rlm.core.orchestrator import OrchestratorConfig\n\nconfig = OrchestratorConfig(\n    max_iterations=10,\n    system_prompt_mode=\"full\",  # or \"simple\"\n    custom_instructions=\"Always show your work.\",\n)\n\norchestrator = Orchestrator(config=config)\n</code></pre>"},{"location":"guide/orchestrator/#result-object","title":"Result Object","text":"<pre><code>@dataclass\nclass OrchestratorResult:\n    final_answer: Optional[str]  # The extracted answer\n    success: bool                # Whether execution succeeded\n    iterations: int              # Number of LLM calls\n    steps: list[ExecutionStep]   # Detailed execution log\n    budget_summary: dict         # Cost tracking\n    error: Optional[str]         # Error message if failed\n</code></pre>"},{"location":"guide/orchestrator/#custom-llm-client","title":"Custom LLM Client","text":"<pre><code>from rlm import Orchestrator\nfrom rlm.llm import create_llm_client\n\n# Use a different provider\nclient = create_llm_client(\n    provider=\"anthropic\",\n    api_key=\"sk-ant-...\",\n    model=\"claude-3-sonnet-20240229\"\n)\n\norchestrator = Orchestrator(llm_client=client)\n</code></pre>"},{"location":"guide/orchestrator/#with-context-file","title":"With Context File","text":"<pre><code>result = orchestrator.run(\n    query=\"Summarize the key findings\",\n    context_path=\"/data/research_paper.txt\"\n)\n</code></pre>"},{"location":"guide/orchestrator/#simple-chat-interface","title":"Simple Chat Interface","text":"<p>For one-off questions without the full result object:</p> <pre><code>answer = orchestrator.chat(\"What is 2+2?\")\nprint(answer)  # 4\n</code></pre>"},{"location":"guide/sandbox/","title":"Docker Sandbox","text":"<p>The <code>DockerSandbox</code> provides secure, isolated code execution in Docker containers.</p>"},{"location":"guide/sandbox/#security-features","title":"Security Features","text":"Feature Default Description gVisor Runtime Auto-detect Intercepts syscalls in userspace Network Isolation Enabled <code>network_mode=\"none\"</code> Memory Limit 512MB Prevents OOM attacks PID Limit 50 Prevents fork bombs CPU Quota 1 core Prevents crypto mining No New Privileges Enabled Blocks privilege escalation"},{"location":"guide/sandbox/#basic-usage","title":"Basic Usage","text":"<pre><code>from rlm import DockerSandbox\n\nsandbox = DockerSandbox()\n\nresult = sandbox.execute(\"\"\"\nprint(\"Hello from sandbox!\")\nimport sys\nprint(f\"Python version: {sys.version}\")\n\"\"\")\n\nprint(result.stdout)\nprint(f\"Exit code: {result.exit_code}\")\nprint(f\"Success: {result.success}\")\n</code></pre>"},{"location":"guide/sandbox/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from rlm.core.repl.docker import DockerSandbox, SandboxConfig\n\nconfig = SandboxConfig(\n    image=\"python:3.11-slim\",\n    timeout=60,\n    memory_limit=\"1g\",\n    cpu_limit=2.0,\n    pids_limit=100,\n    network_enabled=False,\n)\n\nsandbox = DockerSandbox(config=config)\n</code></pre>"},{"location":"guide/sandbox/#execution-result","title":"Execution Result","text":"<pre><code>@dataclass\nclass ExecutionResult:\n    stdout: str           # Standard output\n    stderr: str           # Standard error\n    exit_code: int        # Process exit code\n    timed_out: bool       # True if execution timed out\n    oom_killed: bool      # True if killed by OOM\n\n    @property\n    def success(self) -&gt; bool:\n        return self.exit_code == 0 and not self.timed_out and not self.oom_killed\n</code></pre>"},{"location":"guide/sandbox/#with-context-mount","title":"With Context Mount","text":"<p>Mount a file as read-only for the code to access:</p> <pre><code>result = sandbox.execute(\n    code=\"\"\"\nwith open('/mnt/context', 'r') as f:\n    print(f.read()[:100])\n\"\"\",\n    context_mount=\"/path/to/data.txt\"\n)\n</code></pre>"},{"location":"guide/sandbox/#security-validation","title":"Security Validation","text":"<pre><code>security = sandbox.validate_security()\nprint(security)\n# {\n#     'docker_available': True,\n#     'gvisor_available': True,\n#     'network_disabled': True,\n#     'memory_limited': True,\n#     'pids_limited': True\n# }\n</code></pre>"},{"location":"guide/sandbox/#blocked-modules","title":"Blocked Modules","text":"<p>The sandbox automatically blocks dangerous modules:</p> <ul> <li><code>subprocess</code></li> <li><code>multiprocessing</code></li> <li><code>ctypes</code></li> <li><code>cffi</code></li> </ul> <pre><code># This will raise ImportError in the sandbox:\nimport subprocess  # Blocked!\n</code></pre>"},{"location":"security/architecture/","title":"Security Architecture","text":"<p>RLM-Python implements Defense in Depth with 5 security layers.</p>"},{"location":"security/architecture/#the-onion-model","title":"The Onion Model","text":"<pre><code>graph TB\n    subgraph L5[\" L5: Application \"]\n        EgressFilter[\"Egress Filtering&lt;br/&gt;\u2022 Entropy detection&lt;br/&gt;\u2022 Secret patterns&lt;br/&gt;\u2022 Magic bytes\"]\n    end\n\n    subgraph L4[\" L4: Filesystem \"]\n        TempDir[\"Ephemeral Storage&lt;br/&gt;\u2022 TemporaryDirectory&lt;br/&gt;\u2022 Read-only mounts&lt;br/&gt;\u2022 No persistence\"]\n    end\n\n    subgraph L3[\" L3: Network \"]\n        Network[\"Air-Gapped&lt;br/&gt;\u2022 network_mode=none&lt;br/&gt;\u2022 Zero egress\"]\n    end\n\n    subgraph L2[\" L2: Kernel \"]\n        gVisor[\"gVisor Sentry&lt;br/&gt;\u2022 Syscall interception&lt;br/&gt;\u2022 Sandboxed kernel\"]\n    end\n\n    subgraph L1[\" L1: Container \"]\n        Docker[\"Docker Namespace&lt;br/&gt;\u2022 Memory limits&lt;br/&gt;\u2022 CPU limits&lt;br/&gt;\u2022 PID limits\"]\n    end\n\n    L5 --&gt; L4 --&gt; L3 --&gt; L2 --&gt; L1</code></pre>"},{"location":"security/architecture/#layer-details","title":"Layer Details","text":""},{"location":"security/architecture/#l1-container-isolation-docker","title":"L1: Container Isolation (Docker)","text":"<p>Every code execution runs in an ephemeral Docker container:</p> Resource Default Purpose Memory 256MB Prevent OOM attacks CPU 0.5 cores Prevent crypto mining PIDs 50 Prevent fork bombs Swap Disabled No memory overflow <pre><code># Container automatically configured with:\n# - --memory=256m\n# - --memory-swap=256m  (no swap)\n# - --cpus=0.5\n# - --pids-limit=50\n# - --security-opt=no-new-privileges\n# - --ipc=none\n</code></pre>"},{"location":"security/architecture/#l2-kernel-interception-gvisor","title":"L2: Kernel Interception (gVisor)","text":"<p>gVisor provides a user-space kernel that intercepts syscalls:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         User Application            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         gVisor Sentry               \u2502  \u2190 Intercepts syscalls\n\u2502     (User-space Kernel)             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      Host Linux Kernel              \u2502  \u2190 Only safe syscalls reach here\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Blocked syscalls include:</p> <ul> <li><code>ptrace</code> (debugging/injection)</li> <li><code>mount</code> (filesystem manipulation)</li> <li>Low-level networking operations</li> <li>Kernel module loading</li> </ul>"},{"location":"security/architecture/#l3-network-isolation","title":"L3: Network Isolation","text":"<p>Network is disabled by default:</p> <pre><code>network_mode = \"none\"  # No network interface in container\n</code></pre> <p>Enabling Network</p> <p>Network can be enabled via <code>RLM_NETWORK_ENABLED=1</code>, but this:</p> <ul> <li>Allows data exfiltration</li> <li>Enables C2 communication</li> <li>Should NEVER be used in production</li> </ul>"},{"location":"security/architecture/#l4-filesystem-isolation","title":"L4: Filesystem Isolation","text":"<p>RLM v3.0 uses TemporaryDirectory for crash-safe cleanup:</p> <pre><code>with tempfile.TemporaryDirectory(prefix=\"rlm_exec_\") as tmpdir:\n    script_path = Path(tmpdir) / \"user_code.py\"\n    # Execute...\n# Directory auto-deleted even on crash\n</code></pre> <p>Volume mounting:</p> Mount Mode Purpose <code>/opt/rlm_agent_lib</code> <code>ro</code> Agent utilities <code>/tmp/user_code.py</code> <code>ro</code> User script <code>/mnt/context</code> <code>ro</code> Context file (optional) <p>No writable mounts. No persistence.</p>"},{"location":"security/architecture/#l5-egress-filtering","title":"L5: Egress Filtering","text":"<p>The final layer catches data leaks in output:</p>"},{"location":"security/architecture/#shannon-entropy-detection","title":"Shannon Entropy Detection","text":"<p>High entropy strings indicate secrets:</p> <pre><code># Normal text: entropy ~3.5\n\"Hello, this is a normal message\"\n\n# API key: entropy ~5.5 (BLOCKED)\n\"sk-a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\"\n</code></pre>"},{"location":"security/architecture/#pattern-matching","title":"Pattern Matching","text":"<p>Known secret formats are detected:</p> <ul> <li>AWS Access Keys (<code>AKIA...</code>)</li> <li>Private Keys (<code>-----BEGIN RSA...</code>)</li> <li>JWTs (<code>eyJ...</code>)</li> <li>Bearer Tokens</li> </ul>"},{"location":"security/architecture/#binary-detection","title":"Binary Detection","text":"<p>v3.0 blocks binary output via magic bytes:</p> Magic File Type <code>\\x89PNG</code> PNG Image <code>PK\\x03\\x04</code> ZIP Archive <code>%PDF</code> PDF Document <code>\\x7fELF</code> Executable"},{"location":"security/architecture/#fail-closed-security","title":"Fail-Closed Security","text":"<p>RLM refuses to run without gVisor:</p> <pre><code>from rlm import Orchestrator\n\nagent = Orchestrator()\nresult = agent.run(\"print('hello')\")\n# \u274c SecurityViolationError: gVisor (runsc) not found!\n</code></pre> <p>To explicitly allow reduced security:</p> <pre><code>from rlm.core.repl import SandboxConfig\n\nconfig = SandboxConfig(allow_unsafe_runtime=True)\n# \u26a0\ufe0f Warning logged, execution proceeds with runc\n</code></pre>"},{"location":"security/architecture/#threat-model","title":"Threat Model","text":""},{"location":"security/architecture/#what-rlm-protects-against","title":"What RLM Protects Against","text":"Threat Protection Host filesystem access L1: Container isolation Network exfiltration L3: <code>network_mode=none</code> Container escape L2: gVisor syscall filtering Resource exhaustion L1: Memory/CPU/PID limits Secret leakage L5: Egress filtering Binary smuggling L5: Magic byte detection"},{"location":"security/architecture/#what-rlm-does-not-protect-against","title":"What RLM Does NOT Protect Against","text":"Threat Reason SIGKILL from kernel Not catchable Docker daemon compromise Trust boundary Host network if enabled User's choice LLM prompt injection Out of scope"},{"location":"security/architecture/#security-checklist","title":"Security Checklist","text":"<p>Before deploying to production:</p> <ul> <li>[ ] gVisor installed and verified</li> <li>[ ] <code>RLM_ALLOW_UNSAFE_RUNTIME=0</code></li> <li>[ ] <code>RLM_NETWORK_ENABLED=0</code></li> <li>[ ] Memory/CPU limits appropriate for workload</li> <li>[ ] Docker daemon secured</li> <li>[ ] Regular image updates</li> </ul>"},{"location":"security/best-practices/","title":"Security Best Practices","text":"<p>Deploy RLM securely in production.</p>"},{"location":"security/best-practices/#the-golden-rules","title":"The Golden Rules","text":"<p>Always</p> <ul> <li>\u2705 Use gVisor in production</li> <li>\u2705 Keep <code>RLM_NETWORK_ENABLED=0</code></li> <li>\u2705 Set appropriate resource limits</li> <li>\u2705 Update Docker images regularly</li> <li>\u2705 Monitor execution logs</li> </ul> <p>Never</p> <ul> <li>\u274c Use <code>allow_unsafe_runtime=True</code> in production</li> <li>\u274c Enable network without strong justification</li> <li>\u274c Run without resource limits</li> <li>\u274c Ignore egress filter warnings</li> </ul>"},{"location":"security/best-practices/#production-configuration","title":"Production Configuration","text":"<pre><code># Strict security settings\nexport RLM_DOCKER_RUNTIME=runsc        # Force gVisor\nexport RLM_ALLOW_UNSAFE_RUNTIME=0      # Fail if no gVisor\nexport RLM_NETWORK_ENABLED=0           # No network\nexport RLM_MEMORY_LIMIT=128m           # Tight limits\nexport RLM_CPU_LIMIT=0.25              # Minimal CPU\nexport RLM_EXECUTION_TIMEOUT=15        # Short timeout\nexport RLM_PIDS_LIMIT=30               # Limit processes\n</code></pre>"},{"location":"security/best-practices/#docker-hardening","title":"Docker Hardening","text":""},{"location":"security/best-practices/#use-minimal-images","title":"Use Minimal Images","text":"<pre><code># \u2705 Good: Minimal base\nFROM python:3.11-slim\n\n# \u274c Bad: Full base with unnecessary tools\nFROM python:3.11\n</code></pre>"},{"location":"security/best-practices/#regular-updates","title":"Regular Updates","text":"<pre><code># Update weekly\ndocker pull python:3.11-slim\n</code></pre>"},{"location":"security/best-practices/#read-only-root","title":"Read-Only Root","text":"<p>RLM already mounts volumes as read-only, but you can add:</p> <pre><code># In SandboxConfig\nread_only=True  # Make container filesystem read-only\n</code></pre>"},{"location":"security/best-practices/#egress-filtering-tuning","title":"Egress Filtering Tuning","text":""},{"location":"security/best-practices/#raise-on-leak","title":"Raise on Leak","text":"<p>In production, you may want to fail instead of redact:</p> <pre><code>from rlm.core import OrchestratorConfig\n\nconfig = OrchestratorConfig(\n    raise_on_leak=True  # Raises DataLeakageError instead of redacting\n)\n</code></pre>"},{"location":"security/best-practices/#custom-entropy-threshold","title":"Custom Entropy Threshold","text":"<p>Lower threshold = more sensitive (more false positives):</p> <pre><code># Default: 4.5\nexport RLM_ENTROPY_THRESHOLD=4.0  # More sensitive\n</code></pre>"},{"location":"security/best-practices/#monitoring","title":"Monitoring","text":""},{"location":"security/best-practices/#log-analysis","title":"Log Analysis","text":"<pre><code>import logging\n\n# Enable RLM logging\nlogging.getLogger(\"rlm\").setLevel(logging.INFO)\n\n# Watch for these patterns:\n# - \"Security runtime 'runsc' detected\" \u2705\n# - \"Using standard 'runc'\" \u26a0\ufe0f\n# - \"High entropy detected\" \ud83d\udd0d\n# - \"Secret pattern matched\" \ud83d\udd0d\n</code></pre>"},{"location":"security/best-practices/#metrics-to-track","title":"Metrics to Track","text":"Metric Why Execution time Detect anomalies Memory usage Prevent abuse Iteration count Catch infinite loops Egress filter triggers Security incidents"},{"location":"security/best-practices/#network-risks","title":"Network Risks","text":"<p>If you must enable network:</p> <p>High Risk</p> <p>Enabling network allows:</p> <ul> <li>Data exfiltration to external servers</li> <li>Downloading malicious payloads</li> <li>C2 (Command &amp; Control) communication</li> <li>Crypto mining pool connections</li> </ul> <p>Mitigations (if network required):</p> <ol> <li>Use network policies to whitelist specific domains</li> <li>Monitor egress traffic</li> <li>Rate limit requests</li> <li>Use a proxy with logging</li> </ol>"},{"location":"security/best-practices/#incident-response","title":"Incident Response","text":""},{"location":"security/best-practices/#if-you-detect-a-leak","title":"If You Detect a Leak","text":"<ol> <li>Stop the orchestrator immediately</li> <li>Review execution logs</li> <li>Identify what was leaked</li> <li>Rotate any exposed credentials</li> <li>Analyze how the leak occurred</li> </ol>"},{"location":"security/best-practices/#if-container-escapes","title":"If Container Escapes","text":"<ol> <li>Isolate the host</li> <li>Capture forensic data</li> <li>Review gVisor logs (<code>runsc logs</code>)</li> <li>Report to gVisor maintainers if it's a new escape</li> </ol>"},{"location":"security/best-practices/#security-checklist","title":"Security Checklist","text":"<p>Before going live:</p> <ul> <li>[ ] gVisor installed and tested</li> <li>[ ] <code>docker run --runtime=runsc hello-world</code> works</li> <li>[ ] <code>RLM_ALLOW_UNSAFE_RUNTIME=0</code></li> <li>[ ] <code>RLM_NETWORK_ENABLED=0</code></li> <li>[ ] Resource limits set appropriately</li> <li>[ ] Logging enabled</li> <li>[ ] Monitoring in place</li> <li>[ ] Incident response plan documented</li> </ul>"},{"location":"security/egress/","title":"Egress Filtering","text":"<p>Prevent data leaks from sandbox output.</p>"},{"location":"security/egress/#overview","title":"Overview","text":"<p>RLM's egress filter inspects all output from code execution and blocks potential secret leakage.</p> <pre><code>graph LR\n    A[Code Output] --&gt; B{Egress Filter}\n    B --&gt;|Clean| C[Return to LLM]\n    B --&gt;|Secret Detected| D[Redact/Block]</code></pre>"},{"location":"security/egress/#detection-methods","title":"Detection Methods","text":""},{"location":"security/egress/#1-shannon-entropy","title":"1. Shannon Entropy","text":"<p>High-entropy strings indicate secrets:</p> <pre><code>\"Hello world\"     \u2192 Entropy: 3.2  \u2705 OK\n\"sk-a1b2c3d4...\"  \u2192 Entropy: 5.1  \ud83d\udeab BLOCKED\n</code></pre> <p>How it works:</p> <ul> <li>Calculate information entropy of string segments</li> <li>Threshold default: 4.5 bits</li> <li>Higher entropy \u2192 more random \u2192 likely a secret</li> </ul>"},{"location":"security/egress/#2-pattern-matching","title":"2. Pattern Matching","text":"<p>Known secret formats are detected:</p> Pattern Example AWS Access Key <code>AKIAIOSFODNN7EXAMPLE</code> AWS Secret Key <code>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code> Private Key <code>-----BEGIN RSA PRIVATE KEY-----</code> JWT Token <code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...</code> Bearer Token <code>Bearer sk-...</code> Generic API Key <code>api_key=...</code>, <code>apikey: ...</code>"},{"location":"security/egress/#3-context-echo-prevention","title":"3. Context Echo Prevention","text":"<p>If you provided a context file, the filter prevents echoing sensitive portions:</p> <pre><code># If context contains: database_password=secret123\n# And output contains: \"The password is secret123\"\n# \u2192 BLOCKED: Context echo detected\n</code></pre>"},{"location":"security/egress/#4-binary-magic-bytes-v30","title":"4. Binary Magic Bytes (v3.0)","text":"<p>Binary data is detected and blocked:</p> Magic Bytes File Type <code>\\x89PNG</code> PNG Image <code>PK\\x03\\x04</code> ZIP Archive <code>%PDF</code> PDF Document <code>\\x7fELF</code> Executable <code>GIF89</code> GIF Image <code>MZ</code> Windows EXE"},{"location":"security/egress/#configuration","title":"Configuration","text":"<pre><code># Entropy threshold (lower = more sensitive)\nexport RLM_ENTROPY_THRESHOLD=4.5\n\n# Context similarity threshold\nexport RLM_SIMILARITY_THRESHOLD=0.8\n\n# Max output size (bytes)\nexport RLM_MAX_STDOUT_BYTES=4000\n</code></pre>"},{"location":"security/egress/#behavior-options","title":"Behavior Options","text":""},{"location":"security/egress/#redact-default","title":"Redact (Default)","text":"<p>Secrets are replaced with <code>[REDACTED]</code>:</p> <pre><code># Output: \"API_KEY=sk-abc123xyz\"\n# Filtered: \"API_KEY=[REDACTED: Secret Pattern]\"\n</code></pre>"},{"location":"security/egress/#raise-exception","title":"Raise Exception","text":"<p>Fail instead of redacting:</p> <pre><code>from rlm.core import OrchestratorConfig\n\nconfig = OrchestratorConfig(raise_on_leak=True)\nagent = Orchestrator(config=config)\n\ntry:\n    result = agent.run(\"print(os.environ.get('API_KEY'))\")\nexcept DataLeakageError as e:\n    print(f\"Leak blocked: {e.leak_type}\")\n</code></pre>"},{"location":"security/egress/#direct-filter-usage","title":"Direct Filter Usage","text":"<pre><code>from rlm.security.egress import EgressFilter\n\n# Create filter\nfilter = EgressFilter(context=\"my secret password here\")\n\n# Filter output\noutput = \"The password is my secret password\"\nfiltered = filter.filter(output, raise_on_leak=False)\n# \u2192 \"The password is [REDACTED: Context Echo]\"\n</code></pre>"},{"location":"security/egress/#detection-functions","title":"Detection Functions","text":""},{"location":"security/egress/#check-for-secrets","title":"Check for Secrets","text":"<pre><code>from rlm.security.egress import detect_secrets\n\ntext = \"My AWS key is AKIAIOSFODNN7EXAMPLE\"\nfindings = detect_secrets(text)\n\nfor finding in findings:\n    print(f\"Found: {finding.pattern_name} at position {finding.start}\")\n</code></pre>"},{"location":"security/egress/#calculate-entropy","title":"Calculate Entropy","text":"<pre><code>from rlm.security.egress import calculate_shannon_entropy\n\nentropy = calculate_shannon_entropy(\"sk-a1b2c3d4e5f6\")\nprint(f\"Entropy: {entropy:.2f}\")  # ~5.0 - suspicious!\n</code></pre>"},{"location":"security/egress/#allowlist","title":"Allowlist","text":"<p>Some high-entropy strings are safe (hashes, UUIDs):</p> <pre><code># These are NOT flagged:\n# - Git commit hashes\n# - MD5/SHA hashes\n# - UUIDs\n# - Base64 encoded safe content\n\nentropy_allowlist = [\n    r'^[a-f0-9]{32}$',  # MD5\n    r'^[a-f0-9]{40}$',  # SHA1\n    r'^[a-f0-9]{64}$',  # SHA256\n]\n</code></pre>"},{"location":"security/egress/#false-positives","title":"False Positives","text":"<p>If you're getting false positives:</p> <ol> <li> <p>Raise the threshold:    <pre><code>export RLM_ENTROPY_THRESHOLD=5.0\n</code></pre></p> </li> <li> <p>Review what's being blocked:    <pre><code>from rlm.security.egress import calculate_shannon_entropy\n\ntext = \"your_output_here\"\nprint(f\"Entropy: {calculate_shannon_entropy(text)}\")\n</code></pre></p> </li> <li> <p>Consider if it's actually sensitive - high entropy might be legitimate data.</p> </li> </ol>"}]}